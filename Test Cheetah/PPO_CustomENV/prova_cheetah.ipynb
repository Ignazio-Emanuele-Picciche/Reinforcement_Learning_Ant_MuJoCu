{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_HalfCheetah_tensorboard/PPO_8\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -174     |\n",
      "| time/              |          |\n",
      "|    fps             | 9377     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -177          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5310          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00094108755 |\n",
      "|    clip_fraction        | 1.22e-05      |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -8.53         |\n",
      "|    explained_variance   | 0.000323      |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.67e+05      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00235      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.32e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=0.33 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 0.332         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056430907 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -8.55         |\n",
      "|    explained_variance   | 0.0433        |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.53e+05      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00146      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 9.33e+05      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    fps             | 4189     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -180         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4172         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011634312 |\n",
      "|    clip_fraction        | 1.22e-05     |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.58        |\n",
      "|    explained_variance   | 0.0582       |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.23e+05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 7.49e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.64 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -0.643       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014516319 |\n",
      "|    clip_fraction        | 0.000122     |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.62        |\n",
      "|    explained_variance   | 0.0728       |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 2.72e+05     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 5.98e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -178     |\n",
      "| time/              |          |\n",
      "|    fps             | 3830     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -186         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3860         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006535859 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.65        |\n",
      "|    explained_variance   | 0.0894       |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.78e+05     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 9.31e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -189         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3918         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009431966 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.67        |\n",
      "|    explained_variance   | 0.106        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.15e+05     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 7.24e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=0.61 +/- 2.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 0.607         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064267113 |\n",
      "|    clip_fraction        | 1.22e-05      |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -8.7          |\n",
      "|    explained_variance   | 0.118         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.36e+05      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00171      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.51e+05      |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -192     |\n",
      "| time/              |          |\n",
      "|    fps             | 3748     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -193         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3783         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007989415 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.73        |\n",
      "|    explained_variance   | 0.133        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.71e+05     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 9.64e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-1.34 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -1.34         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092359364 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -8.75         |\n",
      "|    explained_variance   | 0.147         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.34e+05      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00221      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.94e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -194     |\n",
      "| time/              |          |\n",
      "|    fps             | 3674     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -195         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3708         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006388796 |\n",
      "|    clip_fraction        | 1.22e-05     |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.77        |\n",
      "|    explained_variance   | 0.161        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.95e+05     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 8.94e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -198         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3729         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007785994 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.81        |\n",
      "|    explained_variance   | 0.177        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.62e+05     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 6.74e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-0.51 +/- 1.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -0.506       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006349145 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.84        |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.44e+05     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 7.99e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    fps             | 3658     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3687         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008252482 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.86        |\n",
      "|    explained_variance   | 0.206        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 2.55e+05     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 4.86e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-1.18 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -1.18        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007996505 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.89        |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.61e+05     |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 6.83e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -208     |\n",
      "| time/              |          |\n",
      "|    fps             | 3630     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -215         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3642         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005786494 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.91        |\n",
      "|    explained_variance   | 0.221        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.17e+05     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 6.97e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -214          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3657          |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065377366 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -8.94         |\n",
      "|    explained_variance   | 0.234         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.4e+05       |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.00178      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 7.02e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-0.81 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -0.806       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003061187 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -8.96        |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 5.59e+05     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000912    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 1.04e+06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    fps             | 3600     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -212          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3624          |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056141213 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -8.98         |\n",
      "|    explained_variance   | 0.262         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.56e+05      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.00177      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 7.41e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-0.98 +/- 2.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -0.979       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005233804 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9           |\n",
      "|    explained_variance   | 0.274        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.21e+05     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    std                  | 1.09         |\n",
      "|    value_loss           | 6.91e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    fps             | 3591     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -215          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3605          |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 47            |\n",
      "|    total_timesteps      | 172032        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034384112 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.02         |\n",
      "|    explained_variance   | 0.173         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 8.17e+05      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00124      |\n",
      "|    std                  | 1.09          |\n",
      "|    value_loss           | 1.74e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-1.04 +/- 1.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -1.04         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 180000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057598896 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.04         |\n",
      "|    explained_variance   | 0.295         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.01e+05      |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.00172      |\n",
      "|    std                  | 1.09          |\n",
      "|    value_loss           | 7.36e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -212     |\n",
      "| time/              |          |\n",
      "|    fps             | 3565     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -209          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3583          |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 188416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00050976564 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.07         |\n",
      "|    explained_variance   | 0.3           |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.91e+05      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.00153      |\n",
      "|    std                  | 1.1           |\n",
      "|    value_loss           | 8.06e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -211          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3597          |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032629684 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.09         |\n",
      "|    explained_variance   | 0.298         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.38e+05      |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -0.00111      |\n",
      "|    std                  | 1.1           |\n",
      "|    value_loss           | 8.87e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-0.96 +/- 2.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -0.959       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 200000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004395795 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.11        |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.32e+05     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 8.54e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    fps             | 3551     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -210         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3573         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003184133 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.14        |\n",
      "|    explained_variance   | 0.326        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.59e+05     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 8.85e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-2.15 +/- 1.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -2.15         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 220000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035281555 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.16         |\n",
      "|    explained_variance   | 0.346         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.78e+05      |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.00109      |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 8.01e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -208     |\n",
      "| time/              |          |\n",
      "|    fps             | 3542     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -204          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3562          |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 64            |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032105274 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.17         |\n",
      "|    explained_variance   | 0.357         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.84e+05      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.0011       |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 8.01e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -205          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3572          |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 66            |\n",
      "|    total_timesteps      | 237568        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036788732 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.19         |\n",
      "|    explained_variance   | 0.354         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.59e+05      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00125      |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 8.65e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-1.50 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -1.5          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 240000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025939324 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.21         |\n",
      "|    explained_variance   | 0.362         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.6e+05       |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 9.21e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -215     |\n",
      "| time/              |          |\n",
      "|    fps             | 3552     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -216          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3557          |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 71            |\n",
      "|    total_timesteps      | 253952        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035586147 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.23         |\n",
      "|    explained_variance   | 0.374         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.08e+05      |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.00133      |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 8.45e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-3.56 +/- 1.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.56         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 260000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034096162 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.25         |\n",
      "|    explained_variance   | 0.387         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.26e+05      |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 8.98e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -218     |\n",
      "| time/              |          |\n",
      "|    fps             | 3534     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -218          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3536          |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 76            |\n",
      "|    total_timesteps      | 270336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026231766 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.27         |\n",
      "|    explained_variance   | 0.389         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.07e+05      |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.000977     |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 7.84e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -219          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3554          |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 78            |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018255178 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.29         |\n",
      "|    explained_variance   | 0.402         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.98e+05      |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.0008       |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 7.74e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-2.01 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -2.01         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 280000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033194182 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.31         |\n",
      "|    explained_variance   | 0.406         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.36e+05      |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.0012       |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 8.47e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    fps             | 3528     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -230          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3545          |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 83            |\n",
      "|    total_timesteps      | 294912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017088333 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.33         |\n",
      "|    explained_variance   | 0.422         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 2.87e+05      |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.000638     |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 5.55e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-2.35 +/- 1.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -2.35         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 300000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021496718 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.35         |\n",
      "|    explained_variance   | 0.414         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.1e+05       |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.000853     |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 9.93e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -231     |\n",
      "| time/              |          |\n",
      "|    fps             | 3528     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -231          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3543          |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 87            |\n",
      "|    total_timesteps      | 311296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017343074 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.37         |\n",
      "|    explained_variance   | 0.423         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.41e+05      |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | -0.000827     |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 8.55e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -227          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3553          |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 89            |\n",
      "|    total_timesteps      | 319488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026907783 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.38         |\n",
      "|    explained_variance   | 0.426         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.16e+05      |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00105      |\n",
      "|    std                  | 1.16          |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-1.21 +/- 1.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -1.21         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 320000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018952723 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.4          |\n",
      "|    explained_variance   | 0.438         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.72e+05      |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -0.000819     |\n",
      "|    std                  | 1.16          |\n",
      "|    value_loss           | 9.64e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    fps             | 3532     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -225          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3543          |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 94            |\n",
      "|    total_timesteps      | 335872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021196814 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.42         |\n",
      "|    explained_variance   | 0.454         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.56e+05      |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.000959     |\n",
      "|    std                  | 1.16          |\n",
      "|    value_loss           | 8.58e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-3.75 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.75         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 340000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025306956 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.44         |\n",
      "|    explained_variance   | 0.452         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.82e+05      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -0.000988     |\n",
      "|    std                  | 1.17          |\n",
      "|    value_loss           | 9.51e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -219     |\n",
      "| time/              |          |\n",
      "|    fps             | 3527     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -217          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3534          |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 99            |\n",
      "|    total_timesteps      | 352256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018425008 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.46         |\n",
      "|    explained_variance   | 0.466         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.11e+05      |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.000895     |\n",
      "|    std                  | 1.17          |\n",
      "|    value_loss           | 8.53e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-2.86 +/- 1.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -2.86         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 360000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019412831 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.47         |\n",
      "|    explained_variance   | 0.464         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.15e+05      |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.000885     |\n",
      "|    std                  | 1.17          |\n",
      "|    value_loss           | 1.02e+06      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -215     |\n",
      "| time/              |          |\n",
      "|    fps             | 3510     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -212          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3515          |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 104           |\n",
      "|    total_timesteps      | 368640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016748391 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.49         |\n",
      "|    explained_variance   | 0.458         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.42e+05      |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.000795     |\n",
      "|    std                  | 1.18          |\n",
      "|    value_loss           | 9.04e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -214          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3523          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 106           |\n",
      "|    total_timesteps      | 376832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016588515 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.5          |\n",
      "|    explained_variance   | 0.463         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.23e+05      |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000887     |\n",
      "|    std                  | 1.18          |\n",
      "|    value_loss           | 8.32e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-4.06 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -4.06         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 380000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024586794 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.52         |\n",
      "|    explained_variance   | 0.482         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.9e+05       |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    std                  | 1.18          |\n",
      "|    value_loss           | 9.77e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -210     |\n",
      "| time/              |          |\n",
      "|    fps             | 3500     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -207          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3504          |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 112           |\n",
      "|    total_timesteps      | 393216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017178697 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.54         |\n",
      "|    explained_variance   | 0.485         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.23e+05      |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.00083      |\n",
      "|    std                  | 1.19          |\n",
      "|    value_loss           | 8.69e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-2.35 +/- 1.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -2.35         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 400000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023356435 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.56         |\n",
      "|    explained_variance   | 0.504         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.21e+05      |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.00106      |\n",
      "|    std                  | 1.19          |\n",
      "|    value_loss           | 9.25e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    fps             | 3483     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3489         |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 409600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002081662 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.57        |\n",
      "|    explained_variance   | 0.507        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.25e+05     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.000927    |\n",
      "|    std                  | 1.2          |\n",
      "|    value_loss           | 9.21e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -202          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3502          |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 119           |\n",
      "|    total_timesteps      | 417792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013676568 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.59         |\n",
      "|    explained_variance   | 0.498         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.64e+05      |\n",
      "|    n_updates            | 500           |\n",
      "|    policy_gradient_loss | -0.000735     |\n",
      "|    std                  | 1.2           |\n",
      "|    value_loss           | 8.8e+05       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-3.07 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.07         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 420000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014268642 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.6          |\n",
      "|    explained_variance   | 0.498         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.86e+05      |\n",
      "|    n_updates            | 510           |\n",
      "|    policy_gradient_loss | -0.000841     |\n",
      "|    std                  | 1.2           |\n",
      "|    value_loss           | 9.39e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -205     |\n",
      "| time/              |          |\n",
      "|    fps             | 3490     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -204          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3499          |\n",
      "|    iterations           | 53            |\n",
      "|    time_elapsed         | 124           |\n",
      "|    total_timesteps      | 434176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017846038 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.62         |\n",
      "|    explained_variance   | 0.502         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.31e+05      |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | -0.000909     |\n",
      "|    std                  | 1.2           |\n",
      "|    value_loss           | 9.97e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-3.47 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.47         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 440000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029822305 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.64         |\n",
      "|    explained_variance   | 0.52          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.73e+05      |\n",
      "|    n_updates            | 530           |\n",
      "|    policy_gradient_loss | -0.00115      |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 1.12e+06      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -206     |\n",
      "| time/              |          |\n",
      "|    fps             | 3490     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -206          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3499          |\n",
      "|    iterations           | 55            |\n",
      "|    time_elapsed         | 128           |\n",
      "|    total_timesteps      | 450560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025828695 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.67         |\n",
      "|    explained_variance   | 0.515         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.1e+05       |\n",
      "|    n_updates            | 540           |\n",
      "|    policy_gradient_loss | -0.00101      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.03e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -208         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3512         |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 130          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.558166e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.69        |\n",
      "|    explained_variance   | 0.51         |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.74e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000591    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 7.97e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-3.22 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -3.22        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.859422e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.7         |\n",
      "|    explained_variance   | 0.542        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.37e+05     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.000586    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 8.75e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -213     |\n",
      "| time/              |          |\n",
      "|    fps             | 3503     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -216          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3514          |\n",
      "|    iterations           | 58            |\n",
      "|    time_elapsed         | 135           |\n",
      "|    total_timesteps      | 475136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011346254 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.71         |\n",
      "|    explained_variance   | 0.554         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.8e+05       |\n",
      "|    n_updates            | 570           |\n",
      "|    policy_gradient_loss | -0.000636     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 8.05e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-2.58 +/- 1.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -2.58         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 480000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010963967 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.73         |\n",
      "|    explained_variance   | 0.546         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.7e+05       |\n",
      "|    n_updates            | 580           |\n",
      "|    policy_gradient_loss | -0.000743     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.51e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    fps             | 3504     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -220          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3513          |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 139           |\n",
      "|    total_timesteps      | 491520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020479302 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.75         |\n",
      "|    explained_variance   | 0.572         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.66e+05      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.000742     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.15e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -224         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3522         |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.483624e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.76        |\n",
      "|    explained_variance   | 0.563        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.03e+05     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000386    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 7.99e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-4.19 +/- 1.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -4.19         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 500000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014068873 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.78         |\n",
      "|    explained_variance   | 0.586         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.23e+05      |\n",
      "|    n_updates            | 610           |\n",
      "|    policy_gradient_loss | -0.000705     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 7.78e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    fps             | 3512     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -232          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3521          |\n",
      "|    iterations           | 63            |\n",
      "|    time_elapsed         | 146           |\n",
      "|    total_timesteps      | 516096        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014065015 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.79         |\n",
      "|    explained_variance   | 0.572         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.15e+05      |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -0.000731     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.07e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-3.22 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.22         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 520000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013445705 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.81         |\n",
      "|    explained_variance   | 0.556         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.89e+05      |\n",
      "|    n_updates            | 630           |\n",
      "|    policy_gradient_loss | -0.000831     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 9.23e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -236     |\n",
      "| time/              |          |\n",
      "|    fps             | 3510     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -236          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3516          |\n",
      "|    iterations           | 65            |\n",
      "|    time_elapsed         | 151           |\n",
      "|    total_timesteps      | 532480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0222177e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.82         |\n",
      "|    explained_variance   | 0.551         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.18e+05      |\n",
      "|    n_updates            | 640           |\n",
      "|    policy_gradient_loss | -0.000443     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 8.21e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-3.54 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.54         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 540000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011395809 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.83         |\n",
      "|    explained_variance   | 0.577         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.39e+05      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | -0.000702     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 9.29e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -239     |\n",
      "| time/              |          |\n",
      "|    fps             | 3503     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -240          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 67            |\n",
      "|    time_elapsed         | 156           |\n",
      "|    total_timesteps      | 548864        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012488093 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.84         |\n",
      "|    explained_variance   | 0.292         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 7.43e+05      |\n",
      "|    n_updates            | 660           |\n",
      "|    policy_gradient_loss | -0.00069      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.97e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -235          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3516          |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 158           |\n",
      "|    total_timesteps      | 557056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011915537 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.85         |\n",
      "|    explained_variance   | 0.59          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.78e+05      |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.000813     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1e+06         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-4.81 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -4.81        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 560000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.464944e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.86        |\n",
      "|    explained_variance   | 0.594        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.03e+05     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 8.57e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -236     |\n",
      "| time/              |          |\n",
      "|    fps             | 3504     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -233          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3508          |\n",
      "|    iterations           | 70            |\n",
      "|    time_elapsed         | 163           |\n",
      "|    total_timesteps      | 573440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7436977e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.87         |\n",
      "|    explained_variance   | 0.588         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 3.58e+05      |\n",
      "|    n_updates            | 690           |\n",
      "|    policy_gradient_loss | -0.000471     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 8.37e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-3.65 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.65         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 580000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010524065 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.88         |\n",
      "|    explained_variance   | 0.596         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.39e+05      |\n",
      "|    n_updates            | 700           |\n",
      "|    policy_gradient_loss | -0.000665     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 9.07e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    fps             | 3497     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -229         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3505         |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.327349e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.89        |\n",
      "|    explained_variance   | 0.6          |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.86e+05     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000676    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 9.17e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -227          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3508          |\n",
      "|    iterations           | 73            |\n",
      "|    time_elapsed         | 170           |\n",
      "|    total_timesteps      | 598016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.5472206e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.9          |\n",
      "|    explained_variance   | 0.613         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.37e+05      |\n",
      "|    n_updates            | 720           |\n",
      "|    policy_gradient_loss | -0.000507     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 8.83e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-3.27 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.27         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 600000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018443912 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.92         |\n",
      "|    explained_variance   | 0.631         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.85e+05      |\n",
      "|    n_updates            | 730           |\n",
      "|    policy_gradient_loss | -0.000969     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 9.35e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    fps             | 3493     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -223         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3500         |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.170722e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.93        |\n",
      "|    explained_variance   | 0.619        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.56e+05     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000427    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 9.38e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-3.73 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -3.73        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 620000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.900673e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.94        |\n",
      "|    explained_variance   | 0.622        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.98e+05     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000388    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 8.49e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    fps             | 3490     |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -220          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3493          |\n",
      "|    iterations           | 77            |\n",
      "|    time_elapsed         | 180           |\n",
      "|    total_timesteps      | 630784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4443546e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.96         |\n",
      "|    explained_variance   | 0.63          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.3e+05       |\n",
      "|    n_updates            | 760           |\n",
      "|    policy_gradient_loss | -0.000548     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 8.5e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -221          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3502          |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 182           |\n",
      "|    total_timesteps      | 638976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.0056885e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -9.97         |\n",
      "|    explained_variance   | 0.635         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.32e+05      |\n",
      "|    n_updates            | 770           |\n",
      "|    policy_gradient_loss | -0.000417     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 9.05e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-3.05 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -3.05        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 640000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.593434e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -9.98        |\n",
      "|    explained_variance   | 0.632        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.24e+05     |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.000567    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 8.97e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    fps             | 3495     |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -223          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3503          |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 187           |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.6053554e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10           |\n",
      "|    explained_variance   | 0.645         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.28e+05      |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -0.00063      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 1.05e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-4.91 +/- 1.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -4.91         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 660000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.2146955e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10           |\n",
      "|    explained_variance   | 0.623         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.92e+05      |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | -0.000549     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 9.73e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -222     |\n",
      "| time/              |          |\n",
      "|    fps             | 3496     |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -225         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3503         |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 671744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.419387e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10          |\n",
      "|    explained_variance   | 0.638        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.36e+05     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -0.000443    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 9.05e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -227          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 83            |\n",
      "|    time_elapsed         | 193           |\n",
      "|    total_timesteps      | 679936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9550355e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10           |\n",
      "|    explained_variance   | 0.651         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.12e+05      |\n",
      "|    n_updates            | 820           |\n",
      "|    policy_gradient_loss | -0.000514     |\n",
      "|    std                  | 1.29          |\n",
      "|    value_loss           | 8.81e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-3.39 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -3.39         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 680000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016196739 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10           |\n",
      "|    explained_variance   | 0.642         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.32e+05      |\n",
      "|    n_updates            | 830           |\n",
      "|    policy_gradient_loss | -0.0007       |\n",
      "|    std                  | 1.29          |\n",
      "|    value_loss           | 1.09e+06      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    fps             | 3502     |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -226         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3510         |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 696320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.749088e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.1        |\n",
      "|    explained_variance   | 0.64         |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.09e+05     |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    std                  | 1.3          |\n",
      "|    value_loss           | 8.78e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=-4.07 +/- 0.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -4.07        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 700000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.076365e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.1        |\n",
      "|    explained_variance   | 0.663        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.58e+05     |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.000459    |\n",
      "|    std                  | 1.3          |\n",
      "|    value_loss           | 8.03e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -228     |\n",
      "| time/              |          |\n",
      "|    fps             | 3504     |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -228          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 203           |\n",
      "|    total_timesteps      | 712704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012298836 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.1         |\n",
      "|    explained_variance   | 0.663         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.03e+05      |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.000834     |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 1.04e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-5.42 +/- 1.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.42         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 720000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012275527 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.1         |\n",
      "|    explained_variance   | 0.68          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.99e+05      |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | -0.000742     |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -231     |\n",
      "| time/              |          |\n",
      "|    fps             | 3502     |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -227          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 207           |\n",
      "|    total_timesteps      | 729088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6375714e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.1         |\n",
      "|    explained_variance   | 0.676         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.28e+05      |\n",
      "|    n_updates            | 880           |\n",
      "|    policy_gradient_loss | -0.000389     |\n",
      "|    std                  | 1.31          |\n",
      "|    value_loss           | 8.48e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -226          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3516          |\n",
      "|    iterations           | 90            |\n",
      "|    time_elapsed         | 209           |\n",
      "|    total_timesteps      | 737280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2529576e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.1         |\n",
      "|    explained_variance   | 0.68          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.13e+05      |\n",
      "|    n_updates            | 890           |\n",
      "|    policy_gradient_loss | -0.000438     |\n",
      "|    std                  | 1.31          |\n",
      "|    value_loss           | 8.59e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-6.13 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -6.13        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 740000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.295495e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.1        |\n",
      "|    explained_variance   | 0.672        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 5.09e+05     |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -0.000414    |\n",
      "|    std                  | 1.31         |\n",
      "|    value_loss           | 9.89e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    fps             | 3509     |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -222         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3516         |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 214          |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.238728e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.1        |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 7.03e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000527    |\n",
      "|    std                  | 1.31         |\n",
      "|    value_loss           | 1.91e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-5.19 +/- 2.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.19         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 760000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7107296e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.2         |\n",
      "|    explained_variance   | 0.689         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.76e+05      |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.000606     |\n",
      "|    std                  | 1.32          |\n",
      "|    value_loss           | 9.09e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    fps             | 3507     |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -230        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3507        |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.83517e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.299       |\n",
      "|    entropy_loss         | -10.2       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 8.27e-05    |\n",
      "|    loss                 | 4.41e+05    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.000428   |\n",
      "|    std                  | 1.32        |\n",
      "|    value_loss           | 9.06e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -228          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 221           |\n",
      "|    total_timesteps      | 778240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1005085e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.2         |\n",
      "|    explained_variance   | 0.677         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.36e+05      |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | -0.000369     |\n",
      "|    std                  | 1.32          |\n",
      "|    value_loss           | 9.35e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=-5.16 +/- 1.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -5.16         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 780000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00010473657 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.2         |\n",
      "|    explained_variance   | 0.702         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.69e+05      |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -0.000857     |\n",
      "|    std                  | 1.32          |\n",
      "|    value_loss           | 9.5e+05       |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    fps             | 3504     |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -229          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 97            |\n",
      "|    time_elapsed         | 226           |\n",
      "|    total_timesteps      | 794624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013677508 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.2         |\n",
      "|    explained_variance   | 0.696         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.99e+05      |\n",
      "|    n_updates            | 960           |\n",
      "|    policy_gradient_loss | -0.000697     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 1.05e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-5.63 +/- 1.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.63        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.059798e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.2        |\n",
      "|    explained_variance   | 0.681        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.93e+05     |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 9.83e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    fps             | 3503     |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -231          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3508          |\n",
      "|    iterations           | 99            |\n",
      "|    time_elapsed         | 231           |\n",
      "|    total_timesteps      | 811008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5806297e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.2         |\n",
      "|    explained_variance   | 0.683         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.74e+05      |\n",
      "|    n_updates            | 980           |\n",
      "|    policy_gradient_loss | -0.00037      |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 1.01e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -228         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3512         |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 233          |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.108327e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.2        |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 3.97e+05     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 8.58e+05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-5.04 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.04        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 820000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001793958 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.3        |\n",
      "|    explained_variance   | 0.712        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 5.17e+05     |\n",
      "|    n_updates            | 1000         |\n",
      "|    policy_gradient_loss | -0.000882    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -229     |\n",
      "| time/              |          |\n",
      "|    fps             | 3505     |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -230          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3509          |\n",
      "|    iterations           | 102           |\n",
      "|    time_elapsed         | 238           |\n",
      "|    total_timesteps      | 835584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9367984e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.3         |\n",
      "|    explained_variance   | 0.706         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.6e+05       |\n",
      "|    n_updates            | 1010          |\n",
      "|    policy_gradient_loss | -0.000441     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 9.27e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-5.65 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.65        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.074369e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.3        |\n",
      "|    explained_variance   | 0.708        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.87e+05     |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.000575    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 9.83e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -231     |\n",
      "| time/              |          |\n",
      "|    fps             | 3501     |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -232          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3507          |\n",
      "|    iterations           | 104           |\n",
      "|    time_elapsed         | 242           |\n",
      "|    total_timesteps      | 851968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4830736e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.3         |\n",
      "|    explained_variance   | 0.724         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.51e+05      |\n",
      "|    n_updates            | 1030          |\n",
      "|    policy_gradient_loss | -0.000444     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 9.08e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=-5.11 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.11        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 860000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.202899e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.3        |\n",
      "|    explained_variance   | 0.637        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 5.28e+05     |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.000569    |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 1.16e+06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -227     |\n",
      "| time/              |          |\n",
      "|    fps             | 3501     |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -228          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3504          |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 247           |\n",
      "|    total_timesteps      | 868352        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7336395e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.3         |\n",
      "|    explained_variance   | 0.43          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 9.54e+05      |\n",
      "|    n_updates            | 1050          |\n",
      "|    policy_gradient_loss | -0.00037      |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 2.07e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -223          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3510          |\n",
      "|    iterations           | 107           |\n",
      "|    time_elapsed         | 249           |\n",
      "|    total_timesteps      | 876544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4095145e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.3         |\n",
      "|    explained_variance   | 0.72          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.26e+05      |\n",
      "|    n_updates            | 1060          |\n",
      "|    policy_gradient_loss | -0.000354     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 8.64e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-6.74 +/- 0.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -6.74         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 880000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.8845755e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.3         |\n",
      "|    explained_variance   | 0.402         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -0.000591     |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 2.11e+06      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -225     |\n",
      "| time/              |          |\n",
      "|    fps             | 3504     |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -221         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3509         |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 254          |\n",
      "|    total_timesteps      | 892928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.194956e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.4        |\n",
      "|    explained_variance   | 0.724        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 5.01e+05     |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.000566    |\n",
      "|    std                  | 1.36         |\n",
      "|    value_loss           | 1.02e+06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-4.97 +/- 1.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -4.97         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 900000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0801005e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.4         |\n",
      "|    explained_variance   | 0.68          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.85e+05      |\n",
      "|    n_updates            | 1090          |\n",
      "|    policy_gradient_loss | -0.000425     |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 9.9e+05       |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    fps             | 3501     |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -220         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3504         |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 259          |\n",
      "|    total_timesteps      | 909312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.300461e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.4        |\n",
      "|    explained_variance   | 0.719        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 5.19e+05     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -0.000495    |\n",
      "|    std                  | 1.37         |\n",
      "|    value_loss           | 9.94e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -220          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3508          |\n",
      "|    iterations           | 112           |\n",
      "|    time_elapsed         | 261           |\n",
      "|    total_timesteps      | 917504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5746525e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.4         |\n",
      "|    explained_variance   | 0.31          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 1.14e+06      |\n",
      "|    n_updates            | 1110          |\n",
      "|    policy_gradient_loss | -0.000346     |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 2.49e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-7.71 +/- 1.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -7.71         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 920000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7800502e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.4         |\n",
      "|    explained_variance   | 0.728         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.67e+05      |\n",
      "|    n_updates            | 1120          |\n",
      "|    policy_gradient_loss | -0.00038      |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 9.7e+05       |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -220     |\n",
      "| time/              |          |\n",
      "|    fps             | 3502     |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -220          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3505          |\n",
      "|    iterations           | 114           |\n",
      "|    time_elapsed         | 266           |\n",
      "|    total_timesteps      | 933888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011689676 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.4         |\n",
      "|    explained_variance   | 0.738         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.08e+05      |\n",
      "|    n_updates            | 1130          |\n",
      "|    policy_gradient_loss | -0.00065      |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 9.81e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-6.94 +/- 1.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -6.94         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 940000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8016693e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.4         |\n",
      "|    explained_variance   | 0.709         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.9e+05       |\n",
      "|    n_updates            | 1140          |\n",
      "|    policy_gradient_loss | -0.000272     |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 9.83e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    fps             | 3499     |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 269      |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -223         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3502         |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 271          |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.741899e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.4        |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.66e+05     |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -0.00045     |\n",
      "|    std                  | 1.38         |\n",
      "|    value_loss           | 9.16e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -225          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3506          |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 273           |\n",
      "|    total_timesteps      | 958464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4838162e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.4         |\n",
      "|    explained_variance   | 0.669         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 5.35e+05      |\n",
      "|    n_updates            | 1160          |\n",
      "|    policy_gradient_loss | -0.000253     |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 1.04e+06      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-6.16 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -6.16         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 960000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9537656e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.5         |\n",
      "|    explained_variance   | 0.749         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.49e+05      |\n",
      "|    n_updates            | 1170          |\n",
      "|    policy_gradient_loss | -0.000483     |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 9.1e+05       |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -224     |\n",
      "| time/              |          |\n",
      "|    fps             | 3500     |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -226         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3504         |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 278          |\n",
      "|    total_timesteps      | 974848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.158276e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.5        |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.77e+05     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -0.000525    |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 9.8e+05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-5.71 +/- 0.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -5.71        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 980000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.933195e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.5        |\n",
      "|    explained_variance   | 0.762        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.39e+05     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.00056     |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 8.64e+05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -231     |\n",
      "| time/              |          |\n",
      "|    fps             | 3499     |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -233         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3504         |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 282          |\n",
      "|    total_timesteps      | 991232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.015256e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.299        |\n",
      "|    entropy_loss         | -10.5        |\n",
      "|    explained_variance   | 0.758        |\n",
      "|    learning_rate        | 8.27e-05     |\n",
      "|    loss                 | 4.82e+05     |\n",
      "|    n_updates            | 1200         |\n",
      "|    policy_gradient_loss | -0.000491    |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 9.61e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1e+03         |\n",
      "|    ep_rew_mean          | -235          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 3508          |\n",
      "|    iterations           | 122           |\n",
      "|    time_elapsed         | 284           |\n",
      "|    total_timesteps      | 999424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2359825e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.5         |\n",
      "|    explained_variance   | 0.753         |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.78e+05      |\n",
      "|    n_updates            | 1210          |\n",
      "|    policy_gradient_loss | -0.000439     |\n",
      "|    std                  | 1.39          |\n",
      "|    value_loss           | 9.66e+05      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-7.36 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | -7.36         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 1000000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9807015e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.299         |\n",
      "|    entropy_loss         | -10.5         |\n",
      "|    explained_variance   | 0.77          |\n",
      "|    learning_rate        | 8.27e-05      |\n",
      "|    loss                 | 4.98e+05      |\n",
      "|    n_updates            | 1220          |\n",
      "|    policy_gradient_loss | -0.000367     |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 9.89e+05      |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -235     |\n",
      "| time/              |          |\n",
      "|    fps             | 3502     |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 1007616  |\n",
      "---------------------------------\n",
      "Mean Reward: -15.01 ± 1.76\n"
     ]
    }
   ],
   "source": [
    "# Numero di ambienti paralleli per il training\n",
    "NUM_ENVS = 4\n",
    "\n",
    "# Wrapper personalizzato per applicare la ricompensa modificata\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Accesso ai dati della simulazione MuJoCo\n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]  # Angolo del torso\n",
    "        torso_height = self.env.unwrapped.data.qpos[1]  # Altezza del torso\n",
    "\n",
    "        # Penalità per ribaltamento meno severa (aggiunta invece di sostituzione)\n",
    "        if torso_angle < -0.7 and torso_height < 0.3:\n",
    "            reward += -200  # Sottrai la penalità invece di sostituire la reward\n",
    "            terminated = True  # Termina l'episodio\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Funzione per creare un ambiente monitorato con custom reward\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.18925327466415615,\n",
    "                        forward_reward_weight=1.158890288504633,\n",
    "                        ctrl_cost_weight=0.05108108521573771)\n",
    "        env = Monitor(env)\n",
    "        env = CustomRewardWrapper(env)  # Applica il custom reward\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Creazione degli ambienti per il training (con parallelizzazione)\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)  # Normalizza solo osservazioni\n",
    "\n",
    "# Selezione automatica del device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parametri del modello\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"learning_rate\": 8.272618650819588e-05,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 128,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.9808272185952741,\n",
    "    \"gae_lambda\": 0.9080997013001573,\n",
    "    \"clip_range\": 0.29879721247771235,\n",
    "    \"ent_coef\": 0.02650410588466885,\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": \"./ppo_HalfCheetah_tensorboard/\",\n",
    "    \"device\": device,\n",
    "    \"policy_kwargs\": dict(net_arch=[256, 256, 128])\n",
    "}\n",
    "\n",
    "# Creazione dell'ambiente di valutazione con il custom reward\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10., training=False)\n",
    "\n",
    "# Callback per valutazione e salvataggi\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"ppo_halfcheetah_checkpoint\")\n",
    "\n",
    "# Creazione e training del modello\n",
    "model = PPO(**model_params)\n",
    "model.learn(total_timesteps=1_000_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# Salvataggio del modello e normalizzazione\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n",
    "# Caricamento del modello e della normalizzazione per la valutazione\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\", device=device)\n",
    "eval_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", eval_env)\n",
    "eval_env.training = False\n",
    "eval_env.reset()\n",
    "\n",
    "# Funzione per la valutazione\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Valutazione del modello allenato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, eval_env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")    # salviamo anche i parametri di normalizzazione\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
