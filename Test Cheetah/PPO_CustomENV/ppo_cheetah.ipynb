{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_HalfCheetah_tensorboard/PPO_17\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -362     |\n",
      "| time/              |          |\n",
      "|    fps             | 9027     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -308         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 7214         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092215715 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.5         |\n",
      "|    explained_variance   | -0.0073      |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-2.00 +/- 0.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -2         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00879291 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.5       |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 13.9       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.999      |\n",
      "|    value_loss           | 32.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -301     |\n",
      "| time/              |          |\n",
      "|    fps             | 5556     |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -287         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5315         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074455566 |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.52        |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00896     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-9.26 +/- 0.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -9.26       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008352937 |\n",
      "|    clip_fraction        | 0.0978      |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.52       |\n",
      "|    explained_variance   | 0.421       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -296     |\n",
      "| time/              |          |\n",
      "|    fps             | 4852     |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -290        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4832        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005432807 |\n",
      "|    clip_fraction        | 0.0243      |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.52       |\n",
      "|    explained_variance   | -0.157      |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 523         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.36e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -284         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 4964         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075007505 |\n",
      "|    clip_fraction        | 0.0985       |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.53        |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 28           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-30.44 +/- 0.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -30.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095256185 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.51        |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 8.8          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 20.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -272     |\n",
      "| time/              |          |\n",
      "|    fps             | 4623     |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -263        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4648        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063782 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.5        |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 23.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=3.54 +/- 0.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.54        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007911019 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.49       |\n",
      "|    explained_variance   | 0.663       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -257     |\n",
      "| time/              |          |\n",
      "|    fps             | 4565     |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -250        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4689        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009140965 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.48       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 24.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -239        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4748        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010312675 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.46       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 9.23        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-27.25 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -27.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01477962 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.43      |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 7.01       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.985      |\n",
      "|    value_loss           | 15.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -223     |\n",
      "| time/              |          |\n",
      "|    fps             | 4594     |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -205        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4629        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013508621 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.41       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 9.42        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-40.68 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | -40.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138631845 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -8.37        |\n",
      "|    explained_variance   | 0.74         |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 8.1          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0285      |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -182     |\n",
      "| time/              |          |\n",
      "|    fps             | 4525     |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -159      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4612      |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 28        |\n",
      "|    total_timesteps      | 131072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0128852 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -8.36     |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 11.4      |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | -0.0275   |\n",
      "|    std                  | 0.973     |\n",
      "|    value_loss           | 23.1      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -137        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015620965 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.33       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-7.58 +/- 0.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -7.58      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 140000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046784 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.3       |\n",
      "|    explained_variance   | 0.763      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 8.2        |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.965      |\n",
      "|    value_loss           | 17.6       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -113     |\n",
      "| time/              |          |\n",
      "|    fps             | 4586     |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -84.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4653        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013597529 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 28.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-18.63 +/- 0.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -18.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014139539 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 28.4        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -58.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 4608     |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4660        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016765399 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-7.98 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | -7.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01967495 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -8.19      |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 10.2       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.946      |\n",
      "|    value_loss           | 23.5       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 21.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 4607     |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 59.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4667        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015064225 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.16       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 36.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 91.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4715        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019031124 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.13       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 33.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-13.90 +/- 0.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -13.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020023908 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.8        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.929       |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 124      |\n",
      "| time/              |          |\n",
      "|    fps             | 4657     |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 160         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018178556 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 32.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-27.19 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | -27.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023521064 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -8.01       |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.917       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    fps             | 4628     |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 222        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4672       |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996683 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.714      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 16.9       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    std                  | 0.913      |\n",
      "|    value_loss           | 37.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 263         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021359755 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.94       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.909       |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=367.87 +/- 19.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025562609 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.92       |\n",
      "|    explained_variance   | 0.734       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.903       |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 305      |\n",
      "| time/              |          |\n",
      "|    fps             | 4657     |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 341         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027122818 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.87       |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=662.65 +/- 32.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 663         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026284775 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    std                  | 0.893       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    fps             | 4663     |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 419         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4684        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024506863 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.81       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17          |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    std                  | 0.89        |\n",
      "|    value_loss           | 39          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 451         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4727        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026685031 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.78       |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.884       |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1172.07 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.17e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026618335 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.74       |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 16          |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 480      |\n",
      "| time/              |          |\n",
      "|    fps             | 4692     |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 516         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4704        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027815126 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.69       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.871       |\n",
      "|    value_loss           | 35.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1488.27 +/- 29.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.49e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025827926 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.65       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.865       |\n",
      "|    value_loss           | 36.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 558      |\n",
      "| time/              |          |\n",
      "|    fps             | 4672     |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 598         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025974017 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.62       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.862       |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 643         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4742        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027218826 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.58       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.856       |\n",
      "|    value_loss           | 42.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1826.57 +/- 42.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 1.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022601727 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.54       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.85        |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 688      |\n",
      "| time/              |          |\n",
      "|    fps             | 4719     |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 735         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4752        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030658022 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.5        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    std                  | 0.844       |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=2076.84 +/- 181.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030145455 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.45       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.838       |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 813      |\n",
      "| time/              |          |\n",
      "|    fps             | 4716     |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 865         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4749        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 74          |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024786923 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.41       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.833       |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=2387.33 +/- 48.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 2.39e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 360000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02410267 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.38      |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.829      |\n",
      "|    value_loss           | 55.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 897      |\n",
      "| time/              |          |\n",
      "|    fps             | 4702     |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 950         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4734        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024566464 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.34       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.824       |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 994         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4746        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027705131 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.29       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=2382.89 +/- 38.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.38e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025200596 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.26       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.813       |\n",
      "|    value_loss           | 46.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4721     |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.08e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4738        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020520564 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.23       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.808       |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=2333.78 +/- 75.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.33e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018624969 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.21       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 84.8        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00998    |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 235         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4698     |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4724        |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012127158 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.21       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 146         |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 439         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.2e+03    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4742       |\n",
      "|    iterations           | 51         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 417792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03106182 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.18      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 22.3       |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.802      |\n",
      "|    value_loss           | 54.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=2784.04 +/- 26.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 2.78e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030547522 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 55.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4722     |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.29e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4746        |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023398738 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.794       |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=3011.12 +/- 24.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.01e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027492318 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    std                  | 0.788       |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4727     |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4736        |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030227263 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -7.04       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.786       |\n",
      "|    value_loss           | 61.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.39e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4752       |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 96         |\n",
      "|    total_timesteps      | 458752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02258322 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7.02      |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.9       |\n",
      "|    n_updates            | 550        |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    std                  | 0.783      |\n",
      "|    value_loss           | 66.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=3028.47 +/- 184.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 3.03e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02553604 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -7         |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0405    |\n",
      "|    std                  | 0.781      |\n",
      "|    value_loss           | 81.7       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4725     |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4741        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012979197 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.99       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.781       |\n",
      "|    value_loss           | 507         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3180.88 +/- 103.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.18e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026242502 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.97       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.776       |\n",
      "|    value_loss           | 71.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4713     |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.57e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4721       |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 104        |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03031031 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.94      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.774      |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 1.61e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4738       |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 105        |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02983067 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.92      |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 33.4       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.771      |\n",
      "|    value_loss           | 87.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=3270.22 +/- 71.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.27e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027111497 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.9        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 35.4        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.768       |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4705     |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4719        |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632685 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.87       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.764       |\n",
      "|    value_loss           | 77.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=3429.28 +/- 118.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.43e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023504378 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.85       |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.762       |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4690     |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 1.8e+03   |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4708      |\n",
      "|    iterations           | 65        |\n",
      "|    time_elapsed         | 113       |\n",
      "|    total_timesteps      | 532480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0244323 |\n",
      "|    clip_fraction        | 0.24      |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -6.82     |\n",
      "|    explained_variance   | 0.952     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 37.3      |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0359   |\n",
      "|    std                  | 0.76      |\n",
      "|    value_loss           | 95.1      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3464.45 +/- 66.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.46e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022398997 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.8        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 87.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.89e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027348602 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.78       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    std                  | 0.755       |\n",
      "|    value_loss           | 74.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 1.94e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028053906 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.75       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 31.1        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.751       |\n",
      "|    value_loss           | 82.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3632.60 +/- 16.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.63e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025647089 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.71       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    std                  | 0.744       |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4686     |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 573440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022923429 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.67       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.4        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.741       |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=3580.45 +/- 86.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.58e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022341192 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.65       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.8        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4664     |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4676        |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024953296 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.64       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 99.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.2e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4678        |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 127         |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024716442 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.62       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.735       |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=3857.56 +/- 70.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 3.86e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 600000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01632211 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.6       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 53.4       |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    std                  | 0.733      |\n",
      "|    value_loss           | 117        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.26e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4672        |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023471389 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.58       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=3836.17 +/- 75.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 3.84e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024027724 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.57       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 42.6        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 96.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.35e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4667        |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023358444 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.56       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 89.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.39e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4669        |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030021716 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.53       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.726       |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=4014.73 +/- 102.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.01e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021705844 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.51       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4672        |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019063491 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 41.8        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.72        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=4312.77 +/- 77.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.31e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034482908 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.46       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.718       |\n",
      "|    value_loss           | 86          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4662     |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4677        |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024632948 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 45.6        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.716       |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 2.57e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4691       |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 144        |\n",
      "|    total_timesteps      | 679936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02628667 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.43      |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 38         |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    std                  | 0.715      |\n",
      "|    value_loss           | 95.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=4332.57 +/- 111.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.33e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022621242 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.41       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 45.3        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.712       |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4687        |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023681726 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    std                  | 0.711       |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=4423.11 +/- 43.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.42e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025823377 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.37       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.708       |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4678     |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.74e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4693        |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031666018 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.35       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.706       |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=4531.96 +/- 50.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.53e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030409198 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 93.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4681     |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 2.83e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4695       |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 155        |\n",
      "|    total_timesteps      | 729088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04509469 |\n",
      "|    clip_fraction        | 0.35       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.29      |\n",
      "|    explained_variance   | 0.722      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    std                  | 0.699      |\n",
      "|    value_loss           | 88.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.87e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4704        |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028049134 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.27       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 56.5        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=4737.21 +/- 33.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.74e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025323562 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.25       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 48.8        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    std                  | 0.695       |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 2.99e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4702        |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 160         |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037523694 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    std                  | 0.693       |\n",
      "|    value_loss           | 98.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=4878.25 +/- 100.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 4.88e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021054596 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.19       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 66.3        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.688       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.08e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4697       |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 163        |\n",
      "|    total_timesteps      | 770048     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04075737 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -6.15      |\n",
      "|    explained_variance   | 0.526      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 30.1       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    std                  | 0.683      |\n",
      "|    value_loss           | 98.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4701        |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032924328 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 34.4        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.682       |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=5025.34 +/- 60.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.03e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038908906 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.1        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0346     |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 93.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.24e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4699        |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019273046 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.07       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 58.9        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.677       |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=5167.47 +/- 62.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.17e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047206793 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 23.4        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    std                  | 0.676       |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4689     |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.33e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 811008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020390615 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.05       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.34e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4702        |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025332933 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.04       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.674       |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=5468.82 +/- 87.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.47e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017946536 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 59          |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.45e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4706        |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017947689 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 57.8        |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=5463.76 +/- 22.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.46e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043719485 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6          |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.672       |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4689     |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.54e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4696        |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043676365 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -6.01       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.671       |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=5574.91 +/- 80.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.57e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031222098 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.98       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 43.4        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.667       |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.62e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4697       |\n",
      "|    iterations           | 106        |\n",
      "|    time_elapsed         | 184        |\n",
      "|    total_timesteps      | 868352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02920485 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.93      |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.662      |\n",
      "|    value_loss           | 126        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.66e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4709        |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024043687 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.91       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 63.6        |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=5661.08 +/- 44.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.66e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019123148 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.9        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 71          |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.66        |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4701     |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.72e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4705       |\n",
      "|    iterations           | 109        |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02077879 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.89      |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 52.5       |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    std                  | 0.659      |\n",
      "|    value_loss           | 159        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=5831.48 +/- 67.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110804975 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.87       |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    std                  | 0.657       |\n",
      "|    value_loss           | 62.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 3.84e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4696       |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 193        |\n",
      "|    total_timesteps      | 909312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03566218 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.87      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.3       |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.657      |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 3.92e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4708        |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044474684 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.84       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.654       |\n",
      "|    value_loss           | 97.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=5829.18 +/- 121.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.83e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055641763 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 81.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4697     |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 4e+03     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4702      |\n",
      "|    iterations           | 114       |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 933888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0204705 |\n",
      "|    clip_fraction        | 0.314     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.8      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 53.4      |\n",
      "|    n_updates            | 1130      |\n",
      "|    policy_gradient_loss | -0.0183   |\n",
      "|    std                  | 0.652     |\n",
      "|    value_loss           | 175       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=5800.62 +/- 122.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.8e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035768107 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.79       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.651       |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.1e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 202         |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062293444 |\n",
      "|    clip_fraction        | 0.373       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.77       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.648       |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4700        |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026896564 |\n",
      "|    clip_fraction        | 0.381       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.76       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.647       |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=5750.13 +/- 411.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 5.75e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034481972 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.74       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 53.1        |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.645       |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4693     |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 4.31e+03  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 4699      |\n",
      "|    iterations           | 119       |\n",
      "|    time_elapsed         | 207       |\n",
      "|    total_timesteps      | 974848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0352898 |\n",
      "|    clip_fraction        | 0.291     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.72     |\n",
      "|    explained_variance   | 0.717     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 33.5      |\n",
      "|    n_updates            | 1180      |\n",
      "|    policy_gradient_loss | -0.0293   |\n",
      "|    std                  | 0.643     |\n",
      "|    value_loss           | 114       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=6207.42 +/- 71.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.21e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 980000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04967072 |\n",
      "|    clip_fraction        | 0.372      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.68      |\n",
      "|    explained_variance   | 0.697      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.6       |\n",
      "|    n_updates            | 1190       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.638      |\n",
      "|    value_loss           | 88.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4691     |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.46e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4701        |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 991232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041806318 |\n",
      "|    clip_fraction        | 0.393       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.4        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.635       |\n",
      "|    value_loss           | 94.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.49e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4712        |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036548767 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 56.3        |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.633       |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=6348.27 +/- 70.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.35e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041141156 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    std                  | 0.628       |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4698     |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 1007616  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 4.61e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4703       |\n",
      "|    iterations           | 124        |\n",
      "|    time_elapsed         | 215        |\n",
      "|    total_timesteps      | 1015808    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07041606 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.53      |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 1230       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.624      |\n",
      "|    value_loss           | 79.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=6492.27 +/- 31.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.49e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017999135 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    std                  | 0.623       |\n",
      "|    value_loss           | 184         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4688     |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 1024000  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.7e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4695        |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 219         |\n",
      "|    total_timesteps      | 1032192     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014363175 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.51       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 247         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=6479.99 +/- 75.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.48e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013484834 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.621       |\n",
      "|    value_loss           | 270         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4682     |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 1040384  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.77e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023496803 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 52.2        |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.78e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4696        |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012880348 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.5        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 73.1        |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    std                  | 0.622       |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=6431.86 +/- 63.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 6.43e+03  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1060000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0642121 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.48     |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 18.1      |\n",
      "|    n_updates            | 1290      |\n",
      "|    policy_gradient_loss | -0.0356   |\n",
      "|    std                  | 0.619     |\n",
      "|    value_loss           | 77.4      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4686     |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 1064960  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 4.85e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4696       |\n",
      "|    iterations           | 131        |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 1073152    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04488112 |\n",
      "|    clip_fraction        | 0.397      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.47      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.7       |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    std                  | 0.618      |\n",
      "|    value_loss           | 90.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=6497.19 +/- 77.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.5e+03    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06079851 |\n",
      "|    clip_fraction        | 0.39       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.43      |\n",
      "|    explained_variance   | 0.743      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 24.9       |\n",
      "|    n_updates            | 1310       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.612      |\n",
      "|    value_loss           | 72.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4685     |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.9e+03     |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4693        |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 1089536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057512563 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.743       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 78.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 4.96e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4692        |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013581677 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.38       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 85.6        |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.00918    |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 239         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=6587.75 +/- 79.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 6.59e+03     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1100000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0123555055 |\n",
      "|    clip_fraction        | 0.153        |\n",
      "|    clip_range           | 0.191        |\n",
      "|    entropy_loss         | -5.38        |\n",
      "|    explained_variance   | 0.84         |\n",
      "|    learning_rate        | 0.000637     |\n",
      "|    loss                 | 73.9         |\n",
      "|    n_updates            | 1340         |\n",
      "|    policy_gradient_loss | -0.0203      |\n",
      "|    std                  | 0.61         |\n",
      "|    value_loss           | 233          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4679     |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 1105920  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.01e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4683        |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013977047 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=6548.21 +/- 102.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.55e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025721103 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 38.9        |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4673     |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 1122304  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.06e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4681        |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 1130496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020474777 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 75.3        |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.07e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4679        |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 1138688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018614974 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=6631.86 +/- 133.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.63e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020846032 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 187         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.1e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4671     |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 1146880  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.15e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4673       |\n",
      "|    iterations           | 141        |\n",
      "|    time_elapsed         | 247        |\n",
      "|    total_timesteps      | 1155072    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02217726 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.34      |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 56.6       |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.607      |\n",
      "|    value_loss           | 178        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=6669.92 +/- 122.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.67e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012782255 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 90.3        |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.606       |\n",
      "|    value_loss           | 339         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4667     |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 1163264  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.15e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4675        |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 250         |\n",
      "|    total_timesteps      | 1171456     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049334474 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.17e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4674        |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 252         |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010114375 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=6582.44 +/- 45.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.58e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048216008 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 79.8        |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4665     |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 1187840  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.12e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4668        |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014888584 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.34       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 126         |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.609       |\n",
      "|    value_loss           | 244         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=6646.37 +/- 58.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.65e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011952142 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 73.8        |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 262         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4663     |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 1204224  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.12e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4667       |\n",
      "|    iterations           | 148        |\n",
      "|    time_elapsed         | 259        |\n",
      "|    total_timesteps      | 1212416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03322653 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.35      |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 73.9       |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.61       |\n",
      "|    value_loss           | 149        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=6623.44 +/- 37.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 6.62e+03  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 1220000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0154896 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.191     |\n",
      "|    entropy_loss         | -5.35     |\n",
      "|    explained_variance   | 0.805     |\n",
      "|    learning_rate        | 0.000637  |\n",
      "|    loss                 | 187       |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0163   |\n",
      "|    std                  | 0.61      |\n",
      "|    value_loss           | 332       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 1220608  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.16e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4657        |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 263         |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061620235 |\n",
      "|    clip_fraction        | 0.409       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.35       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 35.8        |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.61        |\n",
      "|    value_loss           | 90.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.21e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4657       |\n",
      "|    iterations           | 151        |\n",
      "|    time_elapsed         | 265        |\n",
      "|    total_timesteps      | 1236992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08853138 |\n",
      "|    clip_fraction        | 0.479      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.34      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26.7       |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.609      |\n",
      "|    value_loss           | 71.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=5910.27 +/- 1378.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 5.91e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1240000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07478796 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.32      |\n",
      "|    explained_variance   | 0.813      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 29.3       |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.606      |\n",
      "|    value_loss           | 72.4       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 1245184  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.21e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4653        |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031689998 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 61.5        |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 209         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=6698.82 +/- 119.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.7e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024322102 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.29       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 61.3        |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.603       |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4646     |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 1261568  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.21e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4654       |\n",
      "|    iterations           | 155        |\n",
      "|    time_elapsed         | 272        |\n",
      "|    total_timesteps      | 1269760    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06707505 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.27      |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26         |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.601      |\n",
      "|    value_loss           | 72.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.23e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 274         |\n",
      "|    total_timesteps      | 1277952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030624658 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.25       |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 57          |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.6         |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=6621.61 +/- 93.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.62e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062478513 |\n",
      "|    clip_fraction        | 0.411       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.22       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.597       |\n",
      "|    value_loss           | 66          |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4650     |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 1286144  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.33e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4658       |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 277        |\n",
      "|    total_timesteps      | 1294336    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07641291 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.19      |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.5       |\n",
      "|    n_updates            | 1570       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.595      |\n",
      "|    value_loss           | 70.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=6161.74 +/- 1368.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.16e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031505346 |\n",
      "|    clip_fraction        | 0.408       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.18       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 75.6        |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.594       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 1302528  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.36e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4656        |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018858576 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.16       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.592       |\n",
      "|    value_loss           | 248         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.43e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 1318912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019898362 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.15       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 54.7        |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 220         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=6880.44 +/- 70.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.88e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1320000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09026857 |\n",
      "|    clip_fraction        | 0.43       |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 23.2       |\n",
      "|    n_updates            | 1610       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 66.6       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 1327104  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.47e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4660       |\n",
      "|    iterations           | 163        |\n",
      "|    time_elapsed         | 286        |\n",
      "|    total_timesteps      | 1335296    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03930735 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.13      |\n",
      "|    explained_variance   | 0.741      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 62.1       |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.00293   |\n",
      "|    std                  | 0.591      |\n",
      "|    value_loss           | 123        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=6971.40 +/- 45.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 6.97e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1340000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06847571 |\n",
      "|    clip_fraction        | 0.423      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.11      |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 26.3       |\n",
      "|    n_updates            | 1630       |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 62.4       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 1343488  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.51e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4657       |\n",
      "|    iterations           | 165        |\n",
      "|    time_elapsed         | 290        |\n",
      "|    total_timesteps      | 1351680    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03700964 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.08      |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 45.7       |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    std                  | 0.584      |\n",
      "|    value_loss           | 102        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.53e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4664        |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 1359872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023589738 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.06       |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.583       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=7020.69 +/- 157.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.02e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1360000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05724726 |\n",
      "|    clip_fraction        | 0.415      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5.03      |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 31.8       |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.58       |\n",
      "|    value_loss           | 77.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.57e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4656     |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 1368064  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.52e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022797698 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.02       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 77.7        |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 174         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=7096.19 +/- 121.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.1e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019940924 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    std                  | 0.578       |\n",
      "|    value_loss           | 317         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.55e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4647     |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 1384448  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.59e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 170        |\n",
      "|    time_elapsed         | 299        |\n",
      "|    total_timesteps      | 1392640    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02276326 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -5         |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 128        |\n",
      "|    n_updates            | 1690       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.577      |\n",
      "|    value_loss           | 252        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=7000.83 +/- 42.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7e+03       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029939003 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.99       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 82.4        |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 178         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4649     |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 1400832  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.65e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 172        |\n",
      "|    time_elapsed         | 302        |\n",
      "|    total_timesteps      | 1409024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05296438 |\n",
      "|    clip_fraction        | 0.386      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.98      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 32.6       |\n",
      "|    n_updates            | 1710       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.577      |\n",
      "|    value_loss           | 80.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.67e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4661        |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 304         |\n",
      "|    total_timesteps      | 1417216     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022127194 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 49.6        |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.575       |\n",
      "|    value_loss           | 169         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=7078.77 +/- 61.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.08e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040515065 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.95       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 47.1        |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.576       |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.69e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4648     |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 306      |\n",
      "|    total_timesteps | 1425408  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.71e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4651       |\n",
      "|    iterations           | 175        |\n",
      "|    time_elapsed         | 308        |\n",
      "|    total_timesteps      | 1433600    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05234484 |\n",
      "|    clip_fraction        | 0.421      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.93      |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 27.4       |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.573      |\n",
      "|    value_loss           | 69         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=6904.91 +/- 109.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 6.9e+03     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031007597 |\n",
      "|    clip_fraction        | 0.377       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.571       |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.7e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 4646     |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 1441792  |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.77e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4652       |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 311        |\n",
      "|    total_timesteps      | 1449984    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05563052 |\n",
      "|    clip_fraction        | 0.366      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.89      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 41.4       |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.57       |\n",
      "|    value_loss           | 80.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 5.79e+03   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 4655       |\n",
      "|    iterations           | 178        |\n",
      "|    time_elapsed         | 313        |\n",
      "|    total_timesteps      | 1458176    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05911491 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.85      |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 27         |\n",
      "|    n_updates            | 1770       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.566      |\n",
      "|    value_loss           | 67.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=7149.62 +/- 82.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 7.15e+03    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029348727 |\n",
      "|    clip_fraction        | 0.376       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.83       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.565       |\n",
      "|    value_loss           | 160         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4652     |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 1466368  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.86e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060422387 |\n",
      "|    clip_fraction        | 0.426       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.8        |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.563       |\n",
      "|    value_loss           | 66.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=7110.42 +/- 54.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.11e+03   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1480000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04455567 |\n",
      "|    clip_fraction        | 0.467      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.78      |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 0.56       |\n",
      "|    value_loss           | 87.4       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.92e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4651     |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 1482752  |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.93e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4658        |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 320         |\n",
      "|    total_timesteps      | 1490944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020402173 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.76       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 39.1        |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    std                  | 0.559       |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 5.91e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4664        |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 1499136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030475464 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.191       |\n",
      "|    entropy_loss         | -4.75       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.000637    |\n",
      "|    loss                 | 52.6        |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.558       |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=7099.20 +/- 65.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 7.1e+03    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02354683 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.191      |\n",
      "|    entropy_loss         | -4.75      |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.000637   |\n",
      "|    loss                 | 74         |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.558      |\n",
      "|    value_loss           | 141        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.93e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 4653     |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 1507328  |\n",
      "---------------------------------\n",
      "Mean Reward: -260.46 ± 4.78\n"
     ]
    }
   ],
   "source": [
    "# Numero di ambienti paralleli per il training\n",
    "NUM_ENVS = 4\n",
    "\n",
    "# Wrapper personalizzato per applicare la ricompensa modificata\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]\n",
    "\n",
    "        if not hasattr(self, 'cappottato_start_time'):  # Inizializza al primo step\n",
    "            self.cappottato_start_time = None\n",
    "\n",
    "        if torso_angle < -0.7:  # Se è caduto\n",
    "            if self.cappottato_start_time is None:  # Se è la prima volta che cade\n",
    "                self.cappottato_start_time = time.time()  # Registra il tempo\n",
    "\n",
    "            tempo_cappottato = time.time() - self.cappottato_start_time # Calcola da quanto è cappottato\n",
    "\n",
    "            penalty = 50 * tempo_cappottato  # Penalità cumulativa (10 è un fattore di scala)\n",
    "            reward -= penalty\n",
    "\n",
    "        else:  # Se non è più caduto\n",
    "            self.cappottato_start_time = None  # Resetta il timer\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Funzione per creare un ambiente monitorato con custom reward\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.013459312664159742,\n",
    "                        forward_reward_weight=1.4435374113892951,\n",
    "                        ctrl_cost_weight=0.09129087622076545)\n",
    "        env = Monitor(env)\n",
    "        env = CustomRewardWrapper(env)  # Applica il custom reward\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Creazione degli ambienti per il training (con parallelizzazione)\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)  # Normalizza solo osservazioni\n",
    "\n",
    "# Selezione automatica del device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parametri del modello\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"learning_rate\": 0.0006365820963392328,\n",
    "    \"n_steps\": 2048,\n",
    "    \"batch_size\": 1024,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.9932509667338772,\n",
    "    \"gae_lambda\": 0.9196254842611007,\n",
    "    \"clip_range\": 0.19119739932498195,\n",
    "    \"ent_coef\": 0.007152371678457134,\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": \"./ppo_HalfCheetah_tensorboard/\",\n",
    "    \"device\": device,\n",
    "    \"policy_kwargs\": dict(net_arch=[256, 256, 128])\n",
    "}\n",
    "\n",
    "# Creazione dell'ambiente di valutazione con il custom reward\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10., training=False)\n",
    "\n",
    "# Callback per valutazione e salvataggi\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"ppo_halfcheetah_checkpoint\")\n",
    "\n",
    "# Creazione e training del modello\n",
    "model = PPO(**model_params)\n",
    "model.learn(total_timesteps=1_500_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# Salvataggio del modello e normalizzazione\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n",
    "# Caricamento del modello e della normalizzazione per la valutazione\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\", device=device)\n",
    "eval_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", eval_env)\n",
    "eval_env.training = False\n",
    "eval_env.reset()\n",
    "\n",
    "# Funzione per la valutazione\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Valutazione del modello allenato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, eval_env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")    # salviamo anche i parametri di normalizzazione\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
