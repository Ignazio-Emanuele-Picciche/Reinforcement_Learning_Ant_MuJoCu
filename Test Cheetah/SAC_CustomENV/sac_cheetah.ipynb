{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import optuna\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di ambienti paralleli per il training\n",
    "NUM_ENVS = 4\n",
    "\n",
    "# Wrapper personalizzato per la ricompensa modificata\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.cappottato_start_time = None\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        torso_angle = self.env.unwrapped.data.qpos[2]\n",
    "        \n",
    "        if torso_angle < -0.7:\n",
    "            if self.cappottato_start_time is None:\n",
    "                self.cappottato_start_time = time.time()\n",
    "            tempo_cappottato = time.time() - self.cappottato_start_time\n",
    "            penalty = 50 * tempo_cappottato\n",
    "            reward -= penalty\n",
    "        else:\n",
    "            self.cappottato_start_time = None\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "# Funzione per creare l'ambiente\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.013459312664159742,\n",
    "                        forward_reward_weight=1.4435374113892951,\n",
    "                        ctrl_cost_weight=0.09129087622076545)\n",
    "        env = Monitor(env)\n",
    "        env = CustomRewardWrapper(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# Creazione degli ambienti per il training\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
    "\n",
    "# Selezione automatica del device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Parametri del modello SAC\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"buffer_size\": 500000,\n",
    "    \"batch_size\": 256,\n",
    "    \"tau\": 0.005,\n",
    "    \"gamma\": 0.99,\n",
    "    \"train_freq\": 64,\n",
    "    \"gradient_steps\": 32,\n",
    "    \"ent_coef\": \"auto\",\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": \"./sac_HalfCheetah_tensorboard/\",\n",
    "    \"device\": device,\n",
    "    \"policy_kwargs\": dict(net_arch=[256, 256, 128])\n",
    "}\n",
    "\n",
    "# Creazione dell'ambiente di valutazione\n",
    "eval_env = DummyVecEnv([make_env()])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False, clip_obs=10., training=False)\n",
    "\n",
    "# Callback per valutazione e salvataggi\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"sac_halfcheetah_checkpoint\")\n",
    "\n",
    "# Creazione e training del modello\n",
    "model = SAC(**model_params)\n",
    "model.learn(total_timesteps=1_500_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# Salvataggio del modello e normalizzazione\n",
    "model.save(\"sac_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")\n",
    "\n",
    "# Caricamento del modello e della normalizzazione per la valutazione\n",
    "model = SAC.load(\"sac_HalfCheetah_model\", device=device)\n",
    "eval_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", eval_env)\n",
    "eval_env.training = False\n",
    "eval_env.reset()\n",
    "\n",
    "# Funzione per la valutazione\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# Valutazione del modello allenato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, eval_env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"sac_HalfCheetah_model\")\n",
    "env.save(\"vecnormalize_HalfCheetah.pkl\")    # salviamo anche i parametri di normalizzazione\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
