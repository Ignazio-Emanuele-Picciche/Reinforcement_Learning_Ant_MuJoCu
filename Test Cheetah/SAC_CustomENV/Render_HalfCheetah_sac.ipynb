{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Caricamento del modello\n",
    "model = PPO.load(\"ppo_HalfCheetah_model\")  # Assicurati che il percorso sia corretto\n",
    "\n",
    "# Creazione dell'ambiente *con* DummyVecEnv per VecNormalize\n",
    "render_env = DummyVecEnv([lambda: gym.make(\"HalfCheetah-v5\",\n",
    "                        reset_noise_scale=0.013459312664159742,\n",
    "                        forward_reward_weight=1.4435374113892951,\n",
    "                        ctrl_cost_weight=0.09129087622076545,\n",
    "                        render_mode='human')])  # Puoi cambiare 'human' con 'rgb_array'\n",
    "\n",
    "# Caricamento della normalizzazione\n",
    "render_env = VecNormalize.load(\"vecnormalize_HalfCheetah.pkl\", render_env)  # Assicurati che il percorso sia corretto\n",
    "\n",
    "render_env.training = False\n",
    "render_env.norm_reward = True  # Normalizza anche le reward se vuoi\n",
    "\n",
    "# Reset dell'ambiente\n",
    "obs = render_env.reset()  # Ottieni le osservazioni (è un array numpy)\n",
    "\n",
    "done = False\n",
    "\n",
    "# Loop per eseguire il rendering\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, rewards, dones, infos = render_env.step(action)  # Corretto: 4 valori\n",
    "    render_env.render()  # Mostra l'ambiente a schermo (se render_mode='human')\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    # Gestione del 'done' (dones può essere un array anche con 1 solo env)\n",
    "    done = dones[0]  # Prendo il primo elemento per il mio singolo ambiente\n",
    "\n",
    "render_env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
