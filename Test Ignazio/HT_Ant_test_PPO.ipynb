{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "# Install tqdm if not already installed\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(reset_noise_scale, forward_reward_weight, ctrl_cost_weight, healthy_reward, contact_cost_weight, healthy_z_range, contact_force_range):\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium con i parametri specificati.\n",
    "    \"\"\"\n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=reset_noise_scale, \n",
    "                    forward_reward_weight=forward_reward_weight, \n",
    "                    ctrl_cost_weight=ctrl_cost_weight, \n",
    "                    healthy_reward=healthy_reward, \n",
    "                    contact_cost_weight = contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z_range,\n",
    "                    contact_force_range=contact_force_range)\n",
    "                   # render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 max 30\n",
    "\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "    # forward_reward_weight = trial.suggest_float('forward_reward_weight', 0.5, 1.5)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "    # ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.1, 1.0)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "    # healthy_reward = trial.suggest_float('healthy_reward', 0.5, 1.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "    \n",
    "    # # Parametri aggiuntivi per Ant-v5\n",
    "    # contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "    # healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.3)             # Per definire l'intervallo di altezze \"sane\"\n",
    "    # healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.8, 1.2)\n",
    "    # contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "    # contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "\n",
    "    # learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    # n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])  \n",
    "    # # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "    # gamma = trial.suggest_float('gamma', 0.99, 0.999)\n",
    "    # gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)\n",
    "    # clip_range = trial.suggest_float('clip_range', 0.1, 0.3) \n",
    "    # ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "# V2 max 1600\n",
    " # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 0.5, 1.5)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.1, 1.0)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 0.5, 1.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.3)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.8, 1.2)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "# n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048, 4096])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.965, 0.98)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.9, 1.0)\n",
    "# clip_range = trial.suggest_float('clip_range', 0.3, 0.5) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "# V3 max 2130\n",
    " # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 1, 1.6)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.5, 1.2)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 1.4, 1.9)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0, 0.2)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.9, 1.1)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "# n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048, 4096])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.96, 0.98)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.88, 0.99)\n",
    "# clip_range = trial.suggest_float('clip_range', 0.1, 0.3) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.2)\n",
    "\n",
    "\n",
    "# V4 max 2681 (BEST)\n",
    "# # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0, 0.8)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 1.4, 1.8)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 1.1, 1.5)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 2, 2.4)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-5, 1e-4)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.25, 0.5)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 1, 1.3)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.2, -0.9)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.9, 1.2)\n",
    "\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "# n_steps = trial.suggest_int('n_steps', 4096, 12288, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.93, 0.96)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.95, 0.98)\n",
    "# clip_range = trial.suggest_float('clip_range', 0, 0.2) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-15 17:52:31,922] A new study created in memory with name: no-name-954bd81b-66a9-4638-93c2-0f7f60c47d70\n",
      "[I 2025-02-15 17:53:48,780] Trial 0 finished with value: 2470.070210707386 and parameters: {'reset_noise_scale': 0.03027402117249338, 'forward_reward_weight': 1.7176746302481225, 'ctrl_cost_weight': 1.515081195861414, 'healthy_reward': 2.4967386120929826, 'contact_cost_weight': 9.662230456918625e-05, 'healthy_z_lower': 0.14341017806806028, 'healthy_z_upper': 1.2225635655853404, 'contact_force_min': -1.1946074934959345, 'contact_force_max': 1.0321224276711545, 'learning_rate': 0.00012887072243519075, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9368442081560295, 'gae_lambda': 0.9506318675607892, 'clip_range': 0.042031228729394736, 'ent_coef': 0.09723276651749996}. Best is trial 0 with value: 2470.070210707386.\n",
      "[I 2025-02-15 17:54:57,446] Trial 1 finished with value: 2345.177075664718 and parameters: {'reset_noise_scale': 0.24263786887494249, 'forward_reward_weight': 1.8774906672453573, 'ctrl_cost_weight': 1.5925862577176002, 'healthy_reward': 2.445957872713247, 'contact_cost_weight': 7.61017300815981e-06, 'healthy_z_lower': 0.3404525563335024, 'healthy_z_upper': 1.104121724710549, 'contact_force_min': -1.0419295506797288, 'contact_force_max': 0.8956486896271697, 'learning_rate': 6.7218270321281e-05, 'n_steps': 6144, 'batch_size': 2048, 'gamma': 0.9333297710984416, 'gae_lambda': 0.9551522730959763, 'clip_range': 0.08650297396389482, 'ent_coef': 0.060126401267089694}. Best is trial 0 with value: 2470.070210707386.\n",
      "[I 2025-02-15 17:56:07,242] Trial 2 finished with value: 2559.8533242199446 and parameters: {'reset_noise_scale': 0.16979061290367028, 'forward_reward_weight': 1.7595233775419197, 'ctrl_cost_weight': 1.2709187737034495, 'healthy_reward': 2.4888495043667955, 'contact_cost_weight': 7.296086205230949e-06, 'healthy_z_lower': 0.33572102253218794, 'healthy_z_upper': 1.2307192256854242, 'contact_force_min': -1.0556118407526631, 'contact_force_max': 1.0886636336246394, 'learning_rate': 0.00012189134367742307, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.930685594993316, 'gae_lambda': 0.9520794501997544, 'clip_range': 0.1273231156810095, 'ent_coef': 0.019845548358773824}. Best is trial 2 with value: 2559.8533242199446.\n",
      "[I 2025-02-15 17:56:55,360] Trial 3 finished with value: 44.47856744924832 and parameters: {'reset_noise_scale': 0.07148877834906574, 'forward_reward_weight': 1.8284852538303717, 'ctrl_cost_weight': 1.351850736519042, 'healthy_reward': 2.2045886430768213, 'contact_cost_weight': 2.8018258875608055e-05, 'healthy_z_lower': 0.3971500399041196, 'healthy_z_upper': 1.265641757331536, 'contact_force_min': -1.2343722489663136, 'contact_force_max': 0.9417399724427933, 'learning_rate': 5.15328341487733e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.956955151173885, 'gae_lambda': 0.9560531208536656, 'clip_range': 0.14521249887629736, 'ent_coef': 0.005143965378039706}. Best is trial 2 with value: 2559.8533242199446.\n",
      "[I 2025-02-15 17:58:05,537] Trial 4 finished with value: 2427.065901887274 and parameters: {'reset_noise_scale': 0.17872034992473684, 'forward_reward_weight': 1.8120189673946043, 'ctrl_cost_weight': 1.5896541027435902, 'healthy_reward': 2.42895282715878, 'contact_cost_weight': 4.5849258271763535e-05, 'healthy_z_lower': 0.1492966603221466, 'healthy_z_upper': 1.1296042951423089, 'contact_force_min': -1.288171448207639, 'contact_force_max': 1.0695999523227213, 'learning_rate': 2.7596303440658435e-05, 'n_steps': 8192, 'batch_size': 1024, 'gamma': 0.9443786326262458, 'gae_lambda': 0.96603010757125, 'clip_range': 0.16397167819716948, 'ent_coef': 0.020583033109936356}. Best is trial 2 with value: 2559.8533242199446.\n",
      "[I 2025-02-15 17:59:22,411] Trial 5 finished with value: 2258.228710447932 and parameters: {'reset_noise_scale': 0.24008573306555558, 'forward_reward_weight': 1.827607746514397, 'ctrl_cost_weight': 1.5061673782491858, 'healthy_reward': 2.328449829325145, 'contact_cost_weight': 3.0133015869709787e-05, 'healthy_z_lower': 0.30766610706581365, 'healthy_z_upper': 1.1469197830813462, 'contact_force_min': -1.0999270450895398, 'contact_force_max': 0.8549463266783331, 'learning_rate': 0.0006042429612517749, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9499089591606984, 'gae_lambda': 0.9703808903790755, 'clip_range': 0.03849289504212883, 'ent_coef': 0.017001297917299495}. Best is trial 2 with value: 2559.8533242199446.\n",
      "[I 2025-02-15 18:00:31,994] Trial 6 finished with value: 2261.19800673221 and parameters: {'reset_noise_scale': 0.2786426778913154, 'forward_reward_weight': 1.6668611278976107, 'ctrl_cost_weight': 1.5257274042902127, 'healthy_reward': 2.3713736634736646, 'contact_cost_weight': 1.2928008227467987e-05, 'healthy_z_lower': 0.24953243250900545, 'healthy_z_upper': 1.3009616973829288, 'contact_force_min': -1.1070346966312623, 'contact_force_max': 0.9462949154833771, 'learning_rate': 0.00031509109653153785, 'n_steps': 4096, 'batch_size': 1024, 'gamma': 0.9533641366461321, 'gae_lambda': 0.962639209844647, 'clip_range': 0.014848357626526365, 'ent_coef': 0.01896368772603645}. Best is trial 2 with value: 2559.8533242199446.\n",
      "[I 2025-02-15 18:01:45,599] Trial 7 finished with value: 2580.8726184684433 and parameters: {'reset_noise_scale': 0.0908236156981845, 'forward_reward_weight': 1.7446624421086572, 'ctrl_cost_weight': 1.4755168923926927, 'healthy_reward': 2.4386178702979096, 'contact_cost_weight': 7.261740384095406e-05, 'healthy_z_lower': 0.2542339247207864, 'healthy_z_upper': 1.2467618136333154, 'contact_force_min': -1.278465354481589, 'contact_force_max': 0.8013537362739197, 'learning_rate': 0.00039794529762511506, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9560889282918068, 'gae_lambda': 0.9742381353092985, 'clip_range': 0.14965219507704916, 'ent_coef': 0.019997049679790737}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:02:54,147] Trial 8 finished with value: 2251.9745778310466 and parameters: {'reset_noise_scale': 0.018492753077030043, 'forward_reward_weight': 1.6899586862486904, 'ctrl_cost_weight': 1.318409287579555, 'healthy_reward': 2.237342526062182, 'contact_cost_weight': 1.8893847618557743e-05, 'healthy_z_lower': 0.3383162768650826, 'healthy_z_upper': 1.2708774387528528, 'contact_force_min': -1.0098559804011027, 'contact_force_max': 0.9855789964517875, 'learning_rate': 6.707628253022418e-05, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9550223242441411, 'gae_lambda': 0.950333322203255, 'clip_range': 0.1776577064564285, 'ent_coef': 0.012177329516915393}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:04:08,478] Trial 9 finished with value: 2249.778262210234 and parameters: {'reset_noise_scale': 0.28612049161211406, 'forward_reward_weight': 1.8030328122773436, 'ctrl_cost_weight': 1.5885567924449815, 'healthy_reward': 2.370745761887257, 'contact_cost_weight': 2.805768414044498e-05, 'healthy_z_lower': 0.19091214600279494, 'healthy_z_upper': 1.1671931413199785, 'contact_force_min': -1.2961351823228329, 'contact_force_max': 1.0095738348268635, 'learning_rate': 0.0005750676585925526, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9589949849528838, 'gae_lambda': 0.9610086086305252, 'clip_range': 0.11598121472610257, 'ent_coef': 0.02832202428679195}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:05:23,446] Trial 10 finished with value: 1965.2745603831943 and parameters: {'reset_noise_scale': 0.1046010225795872, 'forward_reward_weight': 1.625202958484625, 'ctrl_cost_weight': 1.420712103132825, 'healthy_reward': 2.1470319943949407, 'contact_cost_weight': 8.075346974825514e-05, 'healthy_z_lower': 0.22220690182667382, 'healthy_z_upper': 1.3971813442103065, 'contact_force_min': -1.1845458848890296, 'contact_force_max': 0.8011556214334402, 'learning_rate': 0.00026072186723185696, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9449452597838011, 'gae_lambda': 0.9797822914956584, 'clip_range': 0.1986210084813782, 'ent_coef': 0.04428818083561957}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:06:31,977] Trial 11 finished with value: 2500.592021626035 and parameters: {'reset_noise_scale': 0.14605941075050896, 'forward_reward_weight': 1.7561822378229754, 'ctrl_cost_weight': 1.2030565379094287, 'healthy_reward': 2.498657903795224, 'contact_cost_weight': 6.704364776646506e-05, 'healthy_z_lower': 0.2891375375501151, 'healthy_z_upper': 1.2077962836822234, 'contact_force_min': -1.106440446908739, 'contact_force_max': 1.0951810438608487, 'learning_rate': 0.0001766230248025054, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9301019018096113, 'gae_lambda': 0.9758112751803034, 'clip_range': 0.11550605198562422, 'ent_coef': 0.04218010225259259}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:07:43,461] Trial 12 finished with value: 2421.8666052708777 and parameters: {'reset_noise_scale': 0.157355920774037, 'forward_reward_weight': 1.75230869133793, 'ctrl_cost_weight': 1.2336102308841386, 'healthy_reward': 2.4136636294640046, 'contact_cost_weight': 5.897150468596486e-05, 'healthy_z_lower': 0.38077667925313985, 'healthy_z_upper': 1.3249998933330118, 'contact_force_min': -1.0622215834863038, 'contact_force_max': 0.8122163358915049, 'learning_rate': 1.0123482043040788e-05, 'n_steps': 10240, 'batch_size': 512, 'gamma': 0.9400055582490161, 'gae_lambda': 0.9708575502506399, 'clip_range': 0.08659934986331112, 'ent_coef': 0.0683007126399749}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:08:56,788] Trial 13 finished with value: 2362.368040692434 and parameters: {'reset_noise_scale': 0.11065004134532597, 'forward_reward_weight': 1.770699293086747, 'ctrl_cost_weight': 1.4287039849406942, 'healthy_reward': 2.296410817744323, 'contact_cost_weight': 7.654713264915612e-05, 'healthy_z_lower': 0.28343070455722646, 'healthy_z_upper': 1.2059398175519096, 'contact_force_min': -1.154771074157433, 'contact_force_max': 0.8961837600099108, 'learning_rate': 0.0009574288571842948, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9495305935752391, 'gae_lambda': 0.9694616249594274, 'clip_range': 0.13967635988107377, 'ent_coef': 0.034710636592617077}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:10:07,108] Trial 14 finished with value: 2431.5556254416206 and parameters: {'reset_noise_scale': 0.20018189694673905, 'forward_reward_weight': 1.7090111817475926, 'ctrl_cost_weight': 1.291415602391659, 'healthy_reward': 2.4492542951009337, 'contact_cost_weight': 5.098027872047857e-05, 'healthy_z_lower': 0.20586233352512998, 'healthy_z_upper': 1.3461561750370095, 'contact_force_min': -1.2519176955146554, 'contact_force_max': 1.0475054996676014, 'learning_rate': 0.0003139983982795603, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9393634059887128, 'gae_lambda': 0.9750891314722342, 'clip_range': 0.135992046121339, 'ent_coef': 0.0035923285631623177}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:11:22,409] Trial 15 finished with value: 2382.400662389795 and parameters: {'reset_noise_scale': 0.06783167958013997, 'forward_reward_weight': 1.8998372696463244, 'ctrl_cost_weight': 1.457083380455997, 'healthy_reward': 2.3839297452370958, 'contact_cost_weight': 9.890127833269628e-05, 'healthy_z_lower': 0.3458926457159206, 'healthy_z_upper': 1.2490749061899433, 'contact_force_min': -1.1482086819268524, 'contact_force_max': 0.9824922218940441, 'learning_rate': 2.6283669103824375e-05, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.94951824473771, 'gae_lambda': 0.9557208934436966, 'clip_range': 0.06625843233720867, 'ent_coef': 0.05761776869346166}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:12:31,189] Trial 16 finished with value: 2267.1228336845834 and parameters: {'reset_noise_scale': 0.12091145621167883, 'forward_reward_weight': 1.6508421541707714, 'ctrl_cost_weight': 1.3665387427988926, 'healthy_reward': 2.2954106611114016, 'contact_cost_weight': 4.4261228753561505e-05, 'healthy_z_lower': 0.27349315851928735, 'healthy_z_upper': 1.1876437253505798, 'contact_force_min': -1.0024801334912403, 'contact_force_max': 0.8901037786303921, 'learning_rate': 0.0001900333454131344, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9417642219977079, 'gae_lambda': 0.959732516853074, 'clip_range': 0.1673881087623353, 'ent_coef': 0.07778280714689675}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:13:46,147] Trial 17 finished with value: 2459.551660099079 and parameters: {'reset_noise_scale': 0.06188520435656679, 'forward_reward_weight': 1.782005084901194, 'ctrl_cost_weight': 1.2596355544749405, 'healthy_reward': 2.4775663449260517, 'contact_cost_weight': 7.979822756075897e-05, 'healthy_z_lower': 0.23926323989439718, 'healthy_z_upper': 1.2358864463971089, 'contact_force_min': -1.0635504793367996, 'contact_force_max': 0.843998827505206, 'learning_rate': 0.00010432763306787989, 'n_steps': 6144, 'batch_size': 1024, 'gamma': 0.9347205495609683, 'gae_lambda': 0.966534203735431, 'clip_range': 0.10906824985612054, 'ent_coef': 0.03139256163762509}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:15:06,157] Trial 18 finished with value: 2049.934389899946 and parameters: {'reset_noise_scale': 0.20871319274682187, 'forward_reward_weight': 1.726376361480746, 'ctrl_cost_weight': 1.469834492033804, 'healthy_reward': 2.101648421959031, 'contact_cost_weight': 2.467057814394516e-06, 'healthy_z_lower': 0.1747027843085061, 'healthy_z_upper': 1.2929096129547837, 'contact_force_min': -1.2323433308777934, 'contact_force_max': 1.0960050170720956, 'learning_rate': 0.0004468770964375174, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9302130491837479, 'gae_lambda': 0.9746787542975165, 'clip_range': 0.18477517581984687, 'ent_coef': 0.0002118573810222027}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:16:39,395] Trial 19 finished with value: 2326.2183900318123 and parameters: {'reset_noise_scale': 0.14050878795752092, 'forward_reward_weight': 1.8470418655343455, 'ctrl_cost_weight': 1.3802954629758857, 'healthy_reward': 2.3317556277920395, 'contact_cost_weight': 6.644204642175382e-05, 'healthy_z_lower': 0.3154896678381189, 'healthy_z_upper': 1.3560225440392208, 'contact_force_min': -1.1941504036900303, 'contact_force_max': 0.9239147883271357, 'learning_rate': 3.686693864634461e-05, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9597893362161645, 'gae_lambda': 0.9796459895754474, 'clip_range': 0.14625338915050637, 'ent_coef': 0.028669724442126444}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:18:10,940] Trial 20 finished with value: 2303.156908989055 and parameters: {'reset_noise_scale': 0.002300625337380763, 'forward_reward_weight': 1.606102745314614, 'ctrl_cost_weight': 1.32630527540762, 'healthy_reward': 2.4073494060437897, 'contact_cost_weight': 3.520708029993142e-05, 'healthy_z_lower': 0.10151247855053772, 'healthy_z_upper': 1.1782264632337585, 'contact_force_min': -1.1414495406032972, 'contact_force_max': 0.9843835587240428, 'learning_rate': 0.0008224253837400371, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9523031574735529, 'gae_lambda': 0.9580184782947296, 'clip_range': 0.1278577241702365, 'ent_coef': 0.04904143506667884}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:19:38,677] Trial 21 finished with value: 2488.2431980425117 and parameters: {'reset_noise_scale': 0.15829300503090443, 'forward_reward_weight': 1.7481710261889891, 'ctrl_cost_weight': 1.219270813980778, 'healthy_reward': 2.4938016556325824, 'contact_cost_weight': 6.620662313693637e-05, 'healthy_z_lower': 0.285185574769996, 'healthy_z_upper': 1.2098350710260455, 'contact_force_min': -1.1039357648169674, 'contact_force_max': 1.0836428218684515, 'learning_rate': 0.0001635339875076796, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9310178498449989, 'gae_lambda': 0.9756718300135755, 'clip_range': 0.09397932050722166, 'ent_coef': 0.04022213660884334}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:21:07,718] Trial 22 finished with value: 2410.0985189832622 and parameters: {'reset_noise_scale': 0.13433701850858182, 'forward_reward_weight': 1.7534543166179477, 'ctrl_cost_weight': 1.2026061287499317, 'healthy_reward': 2.456860009023011, 'contact_cost_weight': 6.495044789843894e-05, 'healthy_z_lower': 0.2653765964522328, 'healthy_z_upper': 1.2392911845905625, 'contact_force_min': -1.0783400603052424, 'contact_force_max': 1.0635734255374336, 'learning_rate': 0.00022193548835673443, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.9355367940102434, 'gae_lambda': 0.9727045067291642, 'clip_range': 0.11867064943560497, 'ent_coef': 0.025034373598092588}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:22:33,818] Trial 23 finished with value: 2436.638234981865 and parameters: {'reset_noise_scale': 0.0997307542378245, 'forward_reward_weight': 1.784443080871363, 'ctrl_cost_weight': 1.2626457176204928, 'healthy_reward': 2.4979353476226187, 'contact_cost_weight': 8.42145317566133e-05, 'healthy_z_lower': 0.3083756588708125, 'healthy_z_upper': 1.2815522474169423, 'contact_force_min': -1.0262313792454656, 'contact_force_max': 1.0285562360253149, 'learning_rate': 0.00039118050183848584, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9328173908381854, 'gae_lambda': 0.9777298649229441, 'clip_range': 0.15519697097969198, 'ent_coef': 0.012924669647519732}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:24:00,406] Trial 24 finished with value: 2458.9815850766004 and parameters: {'reset_noise_scale': 0.17722776726717204, 'forward_reward_weight': 1.6906775699753365, 'ctrl_cost_weight': 1.2544396116265277, 'healthy_reward': 2.464712062254833, 'contact_cost_weight': 5.5891470886158e-05, 'healthy_z_lower': 0.3615391536281638, 'healthy_z_upper': 1.1999380277118152, 'contact_force_min': -1.1104640976693394, 'contact_force_max': 1.0978033415395754, 'learning_rate': 0.00014326770709068143, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.9375917132491588, 'gae_lambda': 0.9676563195555895, 'clip_range': 0.07443383139349175, 'ent_coef': 0.041981667564226395}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:25:32,181] Trial 25 finished with value: 2377.8589357206224 and parameters: {'reset_noise_scale': 0.08224691137082592, 'forward_reward_weight': 1.743014596412164, 'ctrl_cost_weight': 1.300083620674182, 'healthy_reward': 2.4043239997583656, 'contact_cost_weight': 7.190219198466787e-05, 'healthy_z_lower': 0.317886814734459, 'healthy_z_upper': 1.2575205124424238, 'contact_force_min': -1.1334437185309463, 'contact_force_max': 1.006428157355195, 'learning_rate': 8.661409933369164e-05, 'n_steps': 10240, 'batch_size': 512, 'gamma': 0.94694018509573, 'gae_lambda': 0.9634397396463804, 'clip_range': 0.10621762255947687, 'ent_coef': 0.03644091179693292}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:27:09,876] Trial 26 finished with value: 2165.1665394167107 and parameters: {'reset_noise_scale': 0.04539577407883326, 'forward_reward_weight': 1.7678459662850563, 'ctrl_cost_weight': 1.5511880285867026, 'healthy_reward': 2.4385572838403395, 'contact_cost_weight': 8.888814081607871e-05, 'healthy_z_lower': 0.23138535067095028, 'healthy_z_upper': 1.226285769742378, 'contact_force_min': -1.0729170445894523, 'contact_force_max': 1.0707372727838196, 'learning_rate': 0.00020952371742424564, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.932409153763775, 'gae_lambda': 0.9733277112217192, 'clip_range': 0.12619583601189485, 'ent_coef': 0.05278335739266135}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:28:38,930] Trial 27 finished with value: 2330.592178041805 and parameters: {'reset_noise_scale': 0.1874650052056268, 'forward_reward_weight': 1.7981261791761727, 'ctrl_cost_weight': 1.201527610108459, 'healthy_reward': 2.2451049563685412, 'contact_cost_weight': 4.172469392376509e-05, 'healthy_z_lower': 0.29194086983651296, 'healthy_z_upper': 1.1677855519695959, 'contact_force_min': -1.0862882236082818, 'contact_force_max': 0.8311574505232909, 'learning_rate': 0.00011846815188730528, 'n_steps': 10240, 'batch_size': 1024, 'gamma': 0.9434574386131761, 'gae_lambda': 0.9528776016701872, 'clip_range': 0.06402873754126898, 'ent_coef': 0.010551908241649054}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:30:05,937] Trial 28 finished with value: 2406.658248455605 and parameters: {'reset_noise_scale': 0.22573353244794625, 'forward_reward_weight': 1.7300002810748933, 'ctrl_cost_weight': 1.4646440834742946, 'healthy_reward': 2.469405885191757, 'contact_cost_weight': 8.945227697000564e-05, 'healthy_z_lower': 0.2654375907679016, 'healthy_z_upper': 1.3100741986033684, 'contact_force_min': -1.037040642239159, 'contact_force_max': 1.0507528655787381, 'learning_rate': 0.0004251951963546472, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.930024526914624, 'gae_lambda': 0.9771831270752015, 'clip_range': 0.15458295712889925, 'ent_coef': 0.07670850925552329}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:31:45,712] Trial 29 finished with value: 2458.7482062575536 and parameters: {'reset_noise_scale': 0.0898597280034793, 'forward_reward_weight': 1.7134978602822484, 'ctrl_cost_weight': 1.4121327873099876, 'healthy_reward': 2.4982225878501128, 'contact_cost_weight': 7.207672124378908e-05, 'healthy_z_lower': 0.3676080675239903, 'healthy_z_upper': 1.2091965404817013, 'contact_force_min': -1.2654294463381976, 'contact_force_max': 1.0338011379979017, 'learning_rate': 0.00014731712301259515, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.937448615959877, 'gae_lambda': 0.9724424252783763, 'clip_range': 0.12748953610715835, 'ent_coef': 0.09963081140003255}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:33:17,721] Trial 30 finished with value: 2305.5645559601794 and parameters: {'reset_noise_scale': 0.12709424353172905, 'forward_reward_weight': 1.671527126074786, 'ctrl_cost_weight': 1.2832951357560134, 'healthy_reward': 2.347945056408672, 'contact_cost_weight': 5.796863932861094e-05, 'healthy_z_lower': 0.21369841097312015, 'healthy_z_upper': 1.2257635272406857, 'contact_force_min': -1.1620922677203094, 'contact_force_max': 0.8638702270137817, 'learning_rate': 7.981264411843764e-05, 'n_steps': 6144, 'batch_size': 512, 'gamma': 0.9473409438068405, 'gae_lambda': 0.9684675417400657, 'clip_range': 0.0995753699903715, 'ent_coef': 0.08754980483817949}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:34:44,529] Trial 31 finished with value: 2469.462349421494 and parameters: {'reset_noise_scale': 0.16023641150817564, 'forward_reward_weight': 1.7408798775651741, 'ctrl_cost_weight': 1.2319501539736564, 'healthy_reward': 2.4835606212121286, 'contact_cost_weight': 6.589489958389762e-05, 'healthy_z_lower': 0.29932709769273813, 'healthy_z_upper': 1.2128508169385863, 'contact_force_min': -1.1127009903769864, 'contact_force_max': 1.08292656280337, 'learning_rate': 0.00015478234699062095, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9315435753305595, 'gae_lambda': 0.9763003866141158, 'clip_range': 0.09478389717959382, 'ent_coef': 0.04028444205893442}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[I 2025-02-15 18:36:17,195] Trial 32 finished with value: 2493.2085814477905 and parameters: {'reset_noise_scale': 0.15145324139041336, 'forward_reward_weight': 1.761745041076038, 'ctrl_cost_weight': 1.2290339051031043, 'healthy_reward': 2.4919889445043397, 'contact_cost_weight': 7.211640427600575e-05, 'healthy_z_lower': 0.32989892353145284, 'healthy_z_upper': 1.1897264570977348, 'contact_force_min': -1.1287979160455863, 'contact_force_max': 1.0814982633462262, 'learning_rate': 0.00018073114208037424, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9351963268160866, 'gae_lambda': 0.9746946408500139, 'clip_range': 0.0812482629897561, 'ent_coef': 0.02335670209066231}. Best is trial 7 with value: 2580.8726184684433.\n",
      "[W 2025-02-15 18:37:42,941] Trial 33 failed with parameters: {'reset_noise_scale': 0.14159355659767972, 'forward_reward_weight': 1.774949868290568, 'ctrl_cost_weight': 1.2338067611316246, 'healthy_reward': 2.441694163412547, 'contact_cost_weight': 7.365768674817672e-05, 'healthy_z_lower': 0.33124932298827836, 'healthy_z_upper': 1.1480071975202757, 'contact_force_min': -1.047134919851657, 'contact_force_max': 1.0992148087571945, 'learning_rate': 0.00030777735848501903, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9344002067725724, 'gae_lambda': 0.9715581854164922, 'clip_range': 0.08214119057767522, 'ent_coef': 0.025760795415723826} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/5w/qb_kxxjs5lscg9_8tpttrzs40000gn/T/ipykernel_32281/935930604.py\", line 102, in objective\n",
      "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py\", line 88, in evaluate_policy\n",
      "    actions, states = model.predict(\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\", line 557, in predict\n",
      "    return self.policy.predict(observation, state, episode_start, deterministic)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py\", line 352, in predict\n",
      "    self.set_training_mode(False)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py\", line 211, in set_training_mode\n",
      "    self.train(mode)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2841, in train\n",
      "    self.training = mode\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1942, in __setattr__\n",
      "    if isinstance(value, Parameter):\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/parameter.py\", line 10, in __instancecheck__\n",
      "    def __instancecheck__(self, instance):\n",
      "KeyboardInterrupt\n",
      "[W 2025-02-15 18:37:42,949] Trial 33 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Crea uno studio Optuna e ottimizza l'obiettivo\u001b[39;00m\n\u001b[1;32m    112\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Stampa i migliori iperparametri trovati\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 102\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     78\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# episode_rewards = []\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# for episode in range(episodes):\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m#     obs = env.reset()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print(f'Mean is: {mean_reward}, Std is: {reward_std}\\n')\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m mean_reward, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Chiudi e rilascia le risorse\u001b[39;00m\n\u001b[1;32m    105\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:352\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03mGet the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03mIncludes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_training_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:211\u001b[0m, in \u001b[0;36mBaseModel.set_training_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_training_mode\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Put the policy in either training or evaluation mode.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    :param mode: if true, set to training mode, else set to evaluation mode\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2841\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mode, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m   2840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2841\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m   2843\u001b[0m     module\u001b[38;5;241m.\u001b[39mtrain(mode)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1942\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 d\u001b[38;5;241m.\u001b[39mdiscard(name)\n\u001b[1;32m   1941\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParameter\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1945\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign parameters before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1946\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/parameter.py:10\u001b[0m, in \u001b[0;36m_ParameterMeta.__instancecheck__\u001b[0;34m(self, instance)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_ParameterMeta\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TensorMeta):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Parameter:\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(instance, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m     13\u001b[0m                 instance, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_param\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m             ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning con Optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Parametri dell'environment\n",
    "    reset_noise_scale = trial.suggest_float('reset_noise_scale', 0, 0.3)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "    forward_reward_weight = trial.suggest_float('forward_reward_weight', 1.6, 1.9)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "    ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 1.2, 1.6)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "    healthy_reward = trial.suggest_float('healthy_reward', 2.1, 2.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "    # Parametri aggiuntivi per Ant-v5\n",
    "    contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-6, 1e-4)  # Es. range intorno a 5e-4 come default\n",
    "    healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.4)             # Per definire l'intervallo di altezze \"sane\"\n",
    "    healthy_z_upper = trial.suggest_float('healthy_z_upper', 1.1, 1.4)\n",
    "    contact_force_min = trial.suggest_float('contact_force_min', -1.3, -1)         # Modificabile se usi forze di contatto\n",
    "    contact_force_max = trial.suggest_float('contact_force_max', 0.8, 1.1)\n",
    "\n",
    "    # Crea l'ambiente passando tutti i parametri\n",
    "    # env = make_env(\n",
    "    #     reset_noise_scale,\n",
    "    #     forward_reward_weight,\n",
    "    #     ctrl_cost_weight,\n",
    "    #     healthy_reward,\n",
    "    #     contact_cost_weight=contact_cost_weight,\n",
    "    #     healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "    #     contact_force_range=(contact_force_min, contact_force_max)\n",
    "    # )\n",
    "    #env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    # MULTIPROCESSING (MULTIENVIRONMENTS) \n",
    "    NUM_ENVS=4\n",
    "    env = SubprocVecEnv([\n",
    "        lambda: make_env(\n",
    "            reset_noise_scale,\n",
    "            forward_reward_weight,\n",
    "            ctrl_cost_weight,\n",
    "            healthy_reward,\n",
    "            contact_cost_weight=contact_cost_weight,\n",
    "            healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "            contact_force_range=(contact_force_min, contact_force_max)\n",
    "        ) for _ in range(NUM_ENVS)\n",
    "    ])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "    \n",
    "\n",
    "    env.training = False # Setta l'environment in modalità di valutazione\n",
    "    env.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello.\n",
    "    \n",
    "\n",
    "    # Iperparametri per il modello PPO\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    n_steps = trial.suggest_int('n_steps', 4096, 12288, step=2048)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048])  \n",
    "    # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "    gamma = trial.suggest_float('gamma', 0.93, 0.96)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.95, 0.98)\n",
    "    clip_range = trial.suggest_float('clip_range', 0, 0.2) \n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "    \n",
    "    # Nuovo iperparametro per la penalizzazione della varianza\n",
    "    # std_penalty_weight = trial.suggest_float('std_penalty_weight', 0.0, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Crea ed allena il modello PPO\n",
    "    model = PPO(\"MlpPolicy\", env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=gae_lambda,\n",
    "                clip_range=clip_range,\n",
    "                ent_coef=ent_coef,\n",
    "                seed=42,\n",
    "                verbose=0)\n",
    "    model.learn(total_timesteps=200000)\n",
    "\n",
    "    # Valuta il modello su 200 episodi (200 è ottimale)\n",
    "    episodes = 300\n",
    "\n",
    "    # episode_rewards = []\n",
    "    # for episode in range(episodes):\n",
    "    #     obs = env.reset()\n",
    "    #     done = False\n",
    "    #     episode_reward = 0\n",
    "    #     while not done:\n",
    "    #         action, _states = model.predict(obs)\n",
    "    #         obs, reward, done, info = env.step(action)\n",
    "    #         episode_reward += reward\n",
    "    #     episode_rewards.append(episode_reward)\n",
    "\n",
    "    # # Calcola reward media e varianza\n",
    "    # mean_reward = np.mean(episode_rewards)\n",
    "    # reward_std = np.std(episode_rewards)\n",
    "\n",
    "    # # Definisce l'obiettivo: massimizzare la reward media penalizzando la varianza\n",
    "    # score = mean_reward - std_penalty_weight * reward_std\n",
    "\n",
    "    # print(f'Mean is: {mean_reward}, Std is: {reward_std}\\n')\n",
    "\n",
    "\n",
    "\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
    "\n",
    "    # Chiudi e rilascia le risorse\n",
    "    env.close()\n",
    "    del model, env\n",
    "    gc.collect()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# Crea uno studio Optuna e ottimizza l'obiettivo\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "# Stampa i migliori iperparametri trovati\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
