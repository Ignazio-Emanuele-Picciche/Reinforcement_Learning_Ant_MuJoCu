{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "import optuna\n",
    "\n",
    "# Install tqdm if not already installed\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(reset_noise_scale, forward_reward_weight, ctrl_cost_weight, healthy_reward, contact_cost_weight, healthy_z_range, contact_force_range):\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium con i parametri specificati.\n",
    "    \"\"\"\n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=reset_noise_scale, \n",
    "                    forward_reward_weight=forward_reward_weight, \n",
    "                    ctrl_cost_weight=ctrl_cost_weight, \n",
    "                    healthy_reward=healthy_reward, \n",
    "                    contact_cost_weight = contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z_range,\n",
    "                    contact_force_range=contact_force_range)\n",
    "                   # render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "    # forward_reward_weight = trial.suggest_float('forward_reward_weight', 0.5, 1.5)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "    # ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.1, 1.0)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "    # healthy_reward = trial.suggest_float('healthy_reward', 0.5, 1.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "    \n",
    "    # # Parametri aggiuntivi per Ant-v5\n",
    "    # contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "    # healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.3)             # Per definire l'intervallo di altezze \"sane\"\n",
    "    # healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.8, 1.2)\n",
    "    # contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "    # contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "\n",
    "    # learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    # n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])  \n",
    "    # # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "    # gamma = trial.suggest_float('gamma', 0.99, 0.999)\n",
    "    # gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)\n",
    "    # clip_range = trial.suggest_float('clip_range', 0.1, 0.3) \n",
    "    # ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-12 23:15:45,142] A new study created in memory with name: no-name-527676ed-5155-4069-be5b-4d25cf773395\n",
      "/var/folders/5w/qb_kxxjs5lscg9_8tpttrzs40000gn/T/ipykernel_2129/946846396.py:31: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6144`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 2048\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6144 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "[I 2025-02-12 23:17:26,663] Trial 0 finished with value: 905.0469077882271 and parameters: {'reset_noise_scale': 0.19063551485460006, 'forward_reward_weight': 1.1874610350303911, 'ctrl_cost_weight': 0.7635871233517055, 'healthy_reward': 0.9167550523607197, 'contact_cost_weight': 0.00031549876314330944, 'healthy_z_lower': 0.20679790820057092, 'healthy_z_upper': 0.9706826850531042, 'contact_force_min': -0.5166260803523359, 'contact_force_max': 0.8952287392588767, 'learning_rate': 2.9390505260778442e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.9776577251734287, 'gae_lambda': 0.9466444608224389, 'clip_range': 0.42321693449630365, 'ent_coef': 0.08075447939975819}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:18:31,889] Trial 1 finished with value: -30.04559154788653 and parameters: {'reset_noise_scale': 0.19746414428291612, 'forward_reward_weight': 1.0051652446385122, 'ctrl_cost_weight': 0.7942619300493272, 'healthy_reward': 1.2601335699374352, 'contact_cost_weight': 0.0008850805751623407, 'healthy_z_lower': 0.17079590118393465, 'healthy_z_upper': 0.940710781092089, 'contact_force_min': -0.760318374172898, 'contact_force_max': 0.7439747097159344, 'learning_rate': 0.0004459552365417473, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.9772310778225349, 'gae_lambda': 0.9219732513765615, 'clip_range': 0.3709767030090552, 'ent_coef': 0.08643946562173802}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:20:14,605] Trial 2 finished with value: 502.8931781611403 and parameters: {'reset_noise_scale': 0.18697132796427973, 'forward_reward_weight': 1.1087955427245237, 'ctrl_cost_weight': 0.19295183746314876, 'healthy_reward': 0.6080748590007244, 'contact_cost_weight': 0.0007686603344608902, 'healthy_z_lower': 0.13457501076321474, 'healthy_z_upper': 1.1003456920542123, 'contact_force_min': -0.9951891808027209, 'contact_force_max': 0.7799985535271492, 'learning_rate': 0.00014310144788192875, 'n_steps': 2048, 'batch_size': 1024, 'gamma': 0.96682853460526, 'gae_lambda': 0.9107380939561832, 'clip_range': 0.4861298844360087, 'ent_coef': 0.03696793957439067}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:21:22,941] Trial 3 finished with value: -37.7846863302605 and parameters: {'reset_noise_scale': 0.1358627071378025, 'forward_reward_weight': 1.4040950074194316, 'ctrl_cost_weight': 0.43682548864445137, 'healthy_reward': 0.6476762778087657, 'contact_cost_weight': 0.0003812010752031448, 'healthy_z_lower': 0.23530852400456057, 'healthy_z_upper': 1.0733160140430358, 'contact_force_min': -0.797288339692755, 'contact_force_max': 0.5121470152062678, 'learning_rate': 0.00045669343628987623, 'n_steps': 6144, 'batch_size': 512, 'gamma': 0.9751210133321926, 'gae_lambda': 0.9706121499987946, 'clip_range': 0.3130959944132557, 'ent_coef': 0.05872430890648259}. Best is trial 0 with value: 905.0469077882271.\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2048`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2048\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2048 and n_envs=1)\n",
      "  warnings.warn(\n",
      "[I 2025-02-12 23:22:27,847] Trial 4 finished with value: -24.5731386643361 and parameters: {'reset_noise_scale': 0.0850304462928781, 'forward_reward_weight': 1.0137425647976155, 'ctrl_cost_weight': 0.5196430681597589, 'healthy_reward': 0.7473484209373884, 'contact_cost_weight': 0.0002726326058246408, 'healthy_z_lower': 0.28715907904694, 'healthy_z_upper': 1.1973324374620677, 'contact_force_min': -0.7528680167896921, 'contact_force_max': 0.6180133754977883, 'learning_rate': 0.0008621842187944453, 'n_steps': 2048, 'batch_size': 4096, 'gamma': 0.9797697196568037, 'gae_lambda': 0.9967441842087371, 'clip_range': 0.37490960195974493, 'ent_coef': 0.08035810872034804}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:23:52,088] Trial 5 finished with value: 246.3791933170685 and parameters: {'reset_noise_scale': 0.12311261339128832, 'forward_reward_weight': 1.4304584405437466, 'ctrl_cost_weight': 0.7734601013958023, 'healthy_reward': 0.7775234764889951, 'contact_cost_weight': 0.0008914957379287437, 'healthy_z_lower': 0.10324988204265168, 'healthy_z_upper': 0.9297499565279783, 'contact_force_min': -0.8286661758230341, 'contact_force_max': 0.5177062358823721, 'learning_rate': 0.00013040493192430122, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.965463684170193, 'gae_lambda': 0.9252249687362116, 'clip_range': 0.4604943891390768, 'ent_coef': 0.0842759692692531}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:25:01,503] Trial 6 finished with value: -17.380321648146104 and parameters: {'reset_noise_scale': 0.05378919245745084, 'forward_reward_weight': 0.8426925335116012, 'ctrl_cost_weight': 0.7934716921437411, 'healthy_reward': 0.6509406356070634, 'contact_cost_weight': 0.00037649602808024167, 'healthy_z_lower': 0.26857908407155145, 'healthy_z_upper': 0.8597271216354868, 'contact_force_min': -0.9884050189741196, 'contact_force_max': 0.5024852583156283, 'learning_rate': 0.0001489088826443827, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.971256270766513, 'gae_lambda': 0.905219972495219, 'clip_range': 0.32765964816583254, 'ent_coef': 0.08852932284050086}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:26:13,183] Trial 7 finished with value: 67.33085728847111 and parameters: {'reset_noise_scale': 0.19680001165312763, 'forward_reward_weight': 1.4460568946066505, 'ctrl_cost_weight': 0.44703824324352376, 'healthy_reward': 1.3480387355807624, 'contact_cost_weight': 0.00015072074095551937, 'healthy_z_lower': 0.26414429612752094, 'healthy_z_upper': 0.9131755693711565, 'contact_force_min': -0.7009022499870823, 'contact_force_max': 0.993787877970316, 'learning_rate': 0.00023289632159780482, 'n_steps': 8192, 'batch_size': 1024, 'gamma': 0.9756115336493072, 'gae_lambda': 0.9200513182741032, 'clip_range': 0.47532920484701713, 'ent_coef': 0.01601899792733691}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:27:23,939] Trial 8 finished with value: -54.507105212290774 and parameters: {'reset_noise_scale': 0.1482839491385761, 'forward_reward_weight': 1.2599354878731464, 'ctrl_cost_weight': 0.7812247625920281, 'healthy_reward': 0.7004607953191053, 'contact_cost_weight': 0.00058588440154457, 'healthy_z_lower': 0.16282209624648472, 'healthy_z_upper': 1.0953843638284497, 'contact_force_min': -0.5626535243031805, 'contact_force_max': 0.5839383468696586, 'learning_rate': 0.0004829908281605717, 'n_steps': 4096, 'batch_size': 512, 'gamma': 0.9736031577515282, 'gae_lambda': 0.9386466884727088, 'clip_range': 0.41300203101476574, 'ent_coef': 0.06225038992765639}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:28:43,244] Trial 9 finished with value: 218.86472694253666 and parameters: {'reset_noise_scale': 0.12047929203968885, 'forward_reward_weight': 1.084477217047389, 'ctrl_cost_weight': 0.27261083039015266, 'healthy_reward': 0.9335741426430408, 'contact_cost_weight': 0.0005887775456326669, 'healthy_z_lower': 0.2847405009290855, 'healthy_z_upper': 0.94886315017124, 'contact_force_min': -0.8963579946796381, 'contact_force_max': 0.6262810741403285, 'learning_rate': 7.4498696571229e-05, 'n_steps': 6144, 'batch_size': 512, 'gamma': 0.9706398563920832, 'gae_lambda': 0.9426179846701819, 'clip_range': 0.3070481148868809, 'ent_coef': 0.0580860028098824}. Best is trial 0 with value: 905.0469077882271.\n",
      "[I 2025-02-12 23:30:31,498] Trial 10 finished with value: 1081.2364110857247 and parameters: {'reset_noise_scale': 0.16602760934799443, 'forward_reward_weight': 0.5214372034077012, 'ctrl_cost_weight': 0.9974817062870505, 'healthy_reward': 1.0828988674688365, 'contact_cost_weight': 0.00013350724607231866, 'healthy_z_lower': 0.21538822768356114, 'healthy_z_upper': 1.0424427408257477, 'contact_force_min': -0.5121817742476594, 'contact_force_max': 0.9591784463995079, 'learning_rate': 1.7155702283216875e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.9797811251325927, 'gae_lambda': 0.9635569567309412, 'clip_range': 0.4276399656297298, 'ent_coef': 0.0018024420873916855}. Best is trial 10 with value: 1081.2364110857247.\n",
      "[I 2025-02-12 23:32:20,744] Trial 11 finished with value: 1111.7199385565518 and parameters: {'reset_noise_scale': 0.1605260412178317, 'forward_reward_weight': 0.52770403669601, 'ctrl_cost_weight': 0.947856743905373, 'healthy_reward': 1.1138670342902466, 'contact_cost_weight': 0.00011250466449713564, 'healthy_z_lower': 0.2185858791995682, 'healthy_z_upper': 1.020257223905608, 'contact_force_min': -0.5504777706839631, 'contact_force_max': 0.987841594175063, 'learning_rate': 1.7149366763818275e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.9799855234382995, 'gae_lambda': 0.96559122611204, 'clip_range': 0.42925010165035016, 'ent_coef': 0.0007937967566549204}. Best is trial 11 with value: 1111.7199385565518.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning con Optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Parametri dell'environment\n",
    "    reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "    forward_reward_weight = trial.suggest_float('forward_reward_weight', 0.5, 1.5)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "    ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.1, 1.0)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "    healthy_reward = trial.suggest_float('healthy_reward', 0.5, 1.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "    \n",
    "    # Parametri aggiuntivi per Ant-v5\n",
    "    contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "    healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.3)             # Per definire l'intervallo di altezze \"sane\"\n",
    "    healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.8, 1.2)\n",
    "    contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "    contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "    # Crea l'ambiente passando tutti i parametri\n",
    "    env = make_env(\n",
    "        reset_noise_scale,\n",
    "        forward_reward_weight,\n",
    "        ctrl_cost_weight,\n",
    "        healthy_reward,\n",
    "        contact_cost_weight=contact_cost_weight,\n",
    "        healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "        contact_force_range=(contact_force_min, contact_force_max)\n",
    "    )\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "    # Iperparametri per il modello PPO\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048, 4096])  \n",
    "    # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "    gamma = trial.suggest_float('gamma', 0.965, 0.98)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.9, 1.0)\n",
    "    clip_range = trial.suggest_float('clip_range', 0.3, 0.5) \n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "    \n",
    "    # Nuovo iperparametro per la penalizzazione della varianza\n",
    "    # std_penalty_weight = trial.suggest_float('std_penalty_weight', 0.0, 0.5)\n",
    "\n",
    "    # Crea ed allena il modello PPO\n",
    "    model = PPO(\"MlpPolicy\", env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=gae_lambda,\n",
    "                clip_range=clip_range,\n",
    "                ent_coef=ent_coef,\n",
    "                seed=42,\n",
    "                verbose=0)\n",
    "    model.learn(total_timesteps=200000)\n",
    "\n",
    "    # Valuta il modello su 200 episodi (200 è ottimale)\n",
    "    episodes = 150\n",
    "\n",
    "    # episode_rewards = []\n",
    "    # for episode in range(episodes):\n",
    "    #     obs = env.reset()\n",
    "    #     done = False\n",
    "    #     episode_reward = 0\n",
    "    #     while not done:\n",
    "    #         action, _states = model.predict(obs)\n",
    "    #         obs, reward, done, info = env.step(action)\n",
    "    #         episode_reward += reward\n",
    "    #     episode_rewards.append(episode_reward)\n",
    "\n",
    "    # # Calcola reward media e varianza\n",
    "    # mean_reward = np.mean(episode_rewards)\n",
    "    # reward_std = np.std(episode_rewards)\n",
    "\n",
    "    # # Definisce l'obiettivo: massimizzare la reward media penalizzando la varianza\n",
    "    # score = mean_reward - std_penalty_weight * reward_std\n",
    "\n",
    "    # print(f'Mean is: {mean_reward}, Std is: {reward_std}\\n')\n",
    "\n",
    "    env.training = False # Setta l'environment in modalità di valutazione\n",
    "    env.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello.\n",
    "\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# Crea uno studio Optuna e ottimizza l'obiettivo\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "# Stampa i migliori iperparametri trovati\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
