{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag\n",
    "        # Tensorbaord will find & display metrics from the `SCALARS` tab\n",
    "        metric_dict = {\n",
    "            #\"rollout/ep_len_mean\": 0,\n",
    "            #\"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_1 (PPO_4) -> {'reset_noise_scale': 0.16872520546404454, 'forward_reward_weight': 0.569165596187308, 'ctrl_cost_weight': 0.15369909636721105, 'healthy_reward': 1.1651483169773327, 'learning_rate': 0.00025118614395972893, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9900195327210904, 'gae_lambda': 0.8063306496367846, 'clip_range': 0.1411162146550987, 'ent_coef': 0.006226601057899701, 'variance_penalty_weight': 0.0007310600475679448}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale=0.10405074414945424 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight=0.5940601384640877 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight=0.14771040407991193 # peso del reward per il controllo\n",
    "hp_healthy_reward =1.4039427670916238 # reward per la salute\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=0.00014010166026390974           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.9974446213345484      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.8025419607496327              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.16218657788555388               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.00017603718662988996                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "env = DummyVecEnv([make_env])  \n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=hp_learning_rate,           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "    n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    verbose=1,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = DummyVecEnv([make_env])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=10000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_6\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 430  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 323         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008173848 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | -0.736      |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=34.97 +/- 4.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 35          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008655772 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.00362    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.0348      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 257   |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 259         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008446093 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.0239      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=38.52 +/- 3.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008516131 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.017       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 241   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007966857 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0286     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.0194      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009215651 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.00917    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=39.62 +/- 3.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740136 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.1       |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0394     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    std                  | 0.973       |\n",
      "|    value_loss           | 0.0202      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 238   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 137   |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008501561 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11.1       |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0376     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 0.0204      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=40.77 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 40.8         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097571295 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -11.1        |\n",
      "|    explained_variance   | 0.288        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.0377      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0284      |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 0.0212       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 235   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 174   |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009912364 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0341     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.0228      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526085 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -11         |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0279     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 0.0196      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=40.89 +/- 2.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 40.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00973811 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -10.9      |\n",
      "|    explained_variance   | 0.554      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0102    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.944      |\n",
      "|    value_loss           | 0.0255     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 235   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 226   |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 242         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009897024 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.9       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 0.0206      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=41.19 +/- 1.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009208256 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0481     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 0.0246      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 233   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 263   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 278         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009037226 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.8       |\n",
      "|    explained_variance   | 0.726       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.93        |\n",
      "|    value_loss           | 0.0246      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010219857 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.924       |\n",
      "|    value_loss           | 0.0289      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=41.90 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 41.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00905649 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -10.7      |\n",
      "|    explained_variance   | 0.766      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.918      |\n",
      "|    value_loss           | 0.0193     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 233   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 315   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 234        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 331        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00931019 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -10.6      |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.914      |\n",
      "|    value_loss           | 0.0207     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=40.97 +/- 1.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009630229 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.6       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.91        |\n",
      "|    value_loss           | 0.0348      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 232   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 352   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 370         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009463245 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.6       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.906       |\n",
      "|    value_loss           | 0.0294      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=41.67 +/- 1.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009491344 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.5       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0292     |\n",
      "|    std                  | 0.901       |\n",
      "|    value_loss           | 0.0294      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 230   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 391   |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010546457 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.5       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0301     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 0.0447      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 421         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588696 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.5       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.00484    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.892       |\n",
      "|    value_loss           | 0.05        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=42.30 +/- 1.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010670677 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.4       |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    std                  | 0.886       |\n",
      "|    value_loss           | 0.0304      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 231    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 442    |\n",
      "|    total_timesteps | 102400 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 458        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00997037 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -10.4      |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.881      |\n",
      "|    value_loss           | 0.0343     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=42.58 +/- 1.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009239532 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.3       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.878       |\n",
      "|    value_loss           | 0.0488      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 479    |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 494         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009970591 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.3       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.874       |\n",
      "|    value_loss           | 0.0519      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 233          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 509          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110601075 |\n",
      "|    clip_fraction        | 0.174        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -10.2        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.028       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0317      |\n",
      "|    std                  | 0.868        |\n",
      "|    value_loss           | 0.0407       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=42.51 +/- 0.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010958411 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.2       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.863       |\n",
      "|    value_loss           | 0.0368      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 231    |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 530    |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011172535 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10.1       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.041      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.855       |\n",
      "|    value_loss           | 0.0391      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=42.79 +/- 0.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 42.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 130000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01055856 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -10.1      |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.85       |\n",
      "|    value_loss           | 0.0294     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 231    |\n",
      "|    iterations      | 32     |\n",
      "|    time_elapsed    | 566    |\n",
      "|    total_timesteps | 131072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 581         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012201745 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -10         |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.845       |\n",
      "|    value_loss           | 0.0324      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 596         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010238389 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.98       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    std                  | 0.84        |\n",
      "|    value_loss           | 0.0431      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=42.86 +/- 0.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009784136 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.94       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.836       |\n",
      "|    value_loss           | 0.043       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 232    |\n",
      "|    iterations      | 35     |\n",
      "|    time_elapsed    | 617    |\n",
      "|    total_timesteps | 143360 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 633         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011485006 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.89       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.831       |\n",
      "|    value_loss           | 0.0198      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=41.87 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011617979 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.82       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.823       |\n",
      "|    value_loss           | 0.0391      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 231    |\n",
      "|    iterations      | 37     |\n",
      "|    time_elapsed    | 653    |\n",
      "|    total_timesteps | 151552 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 669        |\n",
      "|    total_timesteps      | 155648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01120866 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -9.76      |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.818      |\n",
      "|    value_loss           | 0.0292     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 683         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011329461 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.72       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 0.0446      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=42.15 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 42.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01237489 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -9.67      |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.0323    |\n",
      "|    std                  | 0.808      |\n",
      "|    value_loss           | 0.0283     |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 231    |\n",
      "|    iterations      | 40     |\n",
      "|    time_elapsed    | 706    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 232          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 722          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0105992025 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -9.61        |\n",
      "|    explained_variance   | 0.902        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.0202      |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0307      |\n",
      "|    std                  | 0.802        |\n",
      "|    value_loss           | 0.0596       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=41.08 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011085908 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.55       |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 0.0422      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 231    |\n",
      "|    iterations      | 42     |\n",
      "|    time_elapsed    | 744    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 759         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010844012 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.49       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.79        |\n",
      "|    value_loss           | 0.0524      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=40.63 +/- 1.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011914847 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.43       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0381     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.784       |\n",
      "|    value_loss           | 0.0517      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 44     |\n",
      "|    time_elapsed    | 781    |\n",
      "|    total_timesteps | 180224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 797         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012318226 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.36       |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0337     |\n",
      "|    std                  | 0.778       |\n",
      "|    value_loss           | 0.0403      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 812         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012795677 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.32       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0323     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.774       |\n",
      "|    value_loss           | 0.0393      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=41.09 +/- 0.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012192676 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.28       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0337     |\n",
      "|    std                  | 0.77        |\n",
      "|    value_loss           | 0.0451      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 47     |\n",
      "|    time_elapsed    | 833    |\n",
      "|    total_timesteps | 192512 |\n",
      "-------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 231       |\n",
      "|    iterations           | 48        |\n",
      "|    time_elapsed         | 850       |\n",
      "|    total_timesteps      | 196608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0107035 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.162     |\n",
      "|    entropy_loss         | -9.23     |\n",
      "|    explained_variance   | 0.917     |\n",
      "|    learning_rate        | 0.00014   |\n",
      "|    loss                 | -0.0234   |\n",
      "|    n_updates            | 470       |\n",
      "|    policy_gradient_loss | -0.0298   |\n",
      "|    std                  | 0.764     |\n",
      "|    value_loss           | 0.0698    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=40.09 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012299389 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.18       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    std                  | 0.76        |\n",
      "|    value_loss           | 0.0324      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 872    |\n",
      "|    total_timesteps | 200704 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 889         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012034436 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.14       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0231      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.757       |\n",
      "|    value_loss           | 0.0473      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 905         |\n",
      "|    total_timesteps      | 208896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012114857 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.09       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0359     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.752       |\n",
      "|    value_loss           | 0.0595      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=40.58 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 210000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011781469 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -9.04       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0341     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.747       |\n",
      "|    value_loss           | 0.0453      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 52     |\n",
      "|    time_elapsed    | 927    |\n",
      "|    total_timesteps | 212992 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 943         |\n",
      "|    total_timesteps      | 217088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011252971 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.99       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.743       |\n",
      "|    value_loss           | 0.0919      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=40.11 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012425537 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.93       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0418     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0331     |\n",
      "|    std                  | 0.738       |\n",
      "|    value_loss           | 0.0255      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 54     |\n",
      "|    time_elapsed    | 964    |\n",
      "|    total_timesteps | 221184 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 979         |\n",
      "|    total_timesteps      | 225280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012703283 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.88       |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.732       |\n",
      "|    value_loss           | 0.0489      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 994         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012873977 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.82       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    std                  | 0.728       |\n",
      "|    value_loss           | 0.0426      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=40.65 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 230000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012823898 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.76       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    std                  | 0.721       |\n",
      "|    value_loss           | 0.0446      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 57     |\n",
      "|    time_elapsed    | 1014   |\n",
      "|    total_timesteps | 233472 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1029        |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012038168 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.71       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.027      |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    std                  | 0.717       |\n",
      "|    value_loss           | 0.0666      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=39.75 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012134616 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.67       |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.033      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.715       |\n",
      "|    value_loss           | 0.0928      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 59     |\n",
      "|    time_elapsed    | 1051   |\n",
      "|    total_timesteps | 241664 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 1068       |\n",
      "|    total_timesteps      | 245760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01332273 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -8.64      |\n",
      "|    explained_variance   | 0.934      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0434    |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.712      |\n",
      "|    value_loss           | 0.0591     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 1083       |\n",
      "|    total_timesteps      | 249856     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01275575 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -8.59      |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.00326   |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    std                  | 0.706      |\n",
      "|    value_loss           | 0.0714     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=38.88 +/- 0.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012742903 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.54       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 0.0798      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 62     |\n",
      "|    time_elapsed    | 1105   |\n",
      "|    total_timesteps | 253952 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1120        |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012836605 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.51       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0458     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 0.0897      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=38.35 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012036356 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.47       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.697       |\n",
      "|    value_loss           | 0.0768      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 64     |\n",
      "|    time_elapsed    | 1141   |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 1156        |\n",
      "|    total_timesteps      | 266240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012794631 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.44       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.042      |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 0.0418      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=37.07 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013128603 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.41       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 0.0845      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 66     |\n",
      "|    time_elapsed    | 1177   |\n",
      "|    total_timesteps | 270336 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 1192         |\n",
      "|    total_timesteps      | 274432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128978165 |\n",
      "|    clip_fraction        | 0.215        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -8.36        |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.0254      |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.0342      |\n",
      "|    std                  | 0.687        |\n",
      "|    value_loss           | 0.0848       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 1207         |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0129393125 |\n",
      "|    clip_fraction        | 0.189        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -8.32        |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.0571      |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.0338      |\n",
      "|    std                  | 0.683        |\n",
      "|    value_loss           | 0.0446       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=35.27 +/- 1.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 35.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012652556 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.68        |\n",
      "|    value_loss           | 0.093       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 69     |\n",
      "|    time_elapsed    | 1229   |\n",
      "|    total_timesteps | 282624 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1244        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012844603 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.25       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    std                  | 0.678       |\n",
      "|    value_loss           | 0.0615      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=35.50 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 35.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 290000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014192628 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0553     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    std                  | 0.675       |\n",
      "|    value_loss           | 0.0308      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 71     |\n",
      "|    time_elapsed    | 1265   |\n",
      "|    total_timesteps | 290816 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 1280         |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127965445 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -8.18        |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.0386      |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.0346      |\n",
      "|    std                  | 0.672        |\n",
      "|    value_loss           | 0.0419       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 1294         |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0126686115 |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -8.14        |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | 0.012        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.033       |\n",
      "|    std                  | 0.668        |\n",
      "|    value_loss           | 0.0892       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=37.74 +/- 1.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012900255 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.09       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.029      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    std                  | 0.663       |\n",
      "|    value_loss           | 0.0988      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 74     |\n",
      "|    time_elapsed    | 1315   |\n",
      "|    total_timesteps | 303104 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 1330        |\n",
      "|    total_timesteps      | 307200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011720238 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -8.05       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.662       |\n",
      "|    value_loss           | 0.0872      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=38.00 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 38           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 310000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0148496665 |\n",
      "|    clip_fraction        | 0.231        |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -8.01        |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.0281      |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0351      |\n",
      "|    std                  | 0.658        |\n",
      "|    value_loss           | 0.0229       |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 76     |\n",
      "|    time_elapsed    | 1353   |\n",
      "|    total_timesteps | 311296 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 1368       |\n",
      "|    total_timesteps      | 315392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01390372 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -7.97      |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.653      |\n",
      "|    value_loss           | 0.0844     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 1385         |\n",
      "|    total_timesteps      | 319488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125651285 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.162        |\n",
      "|    entropy_loss         | -7.9         |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.00014      |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.0308      |\n",
      "|    std                  | 0.648        |\n",
      "|    value_loss           | 0.112        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=38.05 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013633541 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.84       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.031      |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    std                  | 0.643       |\n",
      "|    value_loss           | 0.0823      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 79     |\n",
      "|    time_elapsed    | 1407   |\n",
      "|    total_timesteps | 323584 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1422        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013783952 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.8        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 0.0942      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=37.11 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013807617 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.76       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.036      |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    std                  | 0.638       |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 81     |\n",
      "|    time_elapsed    | 1443   |\n",
      "|    total_timesteps | 331776 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 1458       |\n",
      "|    total_timesteps      | 335872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01547374 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -7.71      |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0465    |\n",
      "|    n_updates            | 810        |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    std                  | 0.633      |\n",
      "|    value_loss           | 0.0313     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 1473       |\n",
      "|    total_timesteps      | 339968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01297063 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -7.66      |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.63       |\n",
      "|    value_loss           | 0.0933     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=37.39 +/- 2.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014330653 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.6        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0177      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    std                  | 0.624       |\n",
      "|    value_loss           | 0.0711      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 84     |\n",
      "|    time_elapsed    | 1496   |\n",
      "|    total_timesteps | 344064 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1512        |\n",
      "|    total_timesteps      | 348160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014788117 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.55       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    std                  | 0.621       |\n",
      "|    value_loss           | 0.049       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=38.68 +/- 3.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156381 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.49       |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0598     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    std                  | 0.616       |\n",
      "|    value_loss           | 0.0172      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 86     |\n",
      "|    time_elapsed    | 1534   |\n",
      "|    total_timesteps | 352256 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1549        |\n",
      "|    total_timesteps      | 356352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013698109 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.44       |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.00186     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.613       |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=37.72 +/- 1.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014493143 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.38       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    std                  | 0.607       |\n",
      "|    value_loss           | 0.0378      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 88     |\n",
      "|    time_elapsed    | 1570   |\n",
      "|    total_timesteps | 360448 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1585        |\n",
      "|    total_timesteps      | 364544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015628459 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.33       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.046      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.604       |\n",
      "|    value_loss           | 0.0269      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1602        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014845549 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.29       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.6         |\n",
      "|    value_loss           | 0.034       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=37.10 +/- 1.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 370000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013236586 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.24       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0261     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.597       |\n",
      "|    value_loss           | 0.0896      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 91     |\n",
      "|    time_elapsed    | 1623   |\n",
      "|    total_timesteps | 372736 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 92         |\n",
      "|    time_elapsed         | 1638       |\n",
      "|    total_timesteps      | 376832     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01380388 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -7.19      |\n",
      "|    explained_variance   | 0.927      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0362    |\n",
      "|    n_updates            | 910        |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    std                  | 0.594      |\n",
      "|    value_loss           | 0.0548     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=37.39 +/- 2.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 37.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013942832 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.14       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0469      |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    std                  | 0.589       |\n",
      "|    value_loss           | 0.0933      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 93     |\n",
      "|    time_elapsed    | 1659   |\n",
      "|    total_timesteps | 380928 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 1674       |\n",
      "|    total_timesteps      | 385024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01485606 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -7.09      |\n",
      "|    explained_variance   | 0.947      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0555    |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.587      |\n",
      "|    value_loss           | 0.0744     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1689        |\n",
      "|    total_timesteps      | 389120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014335341 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -7.05       |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.583       |\n",
      "|    value_loss           | 0.0609      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=37.00 +/- 0.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 37        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 390000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0153699 |\n",
      "|    clip_fraction        | 0.238     |\n",
      "|    clip_range           | 0.162     |\n",
      "|    entropy_loss         | -7.01     |\n",
      "|    explained_variance   | 0.945     |\n",
      "|    learning_rate        | 0.00014   |\n",
      "|    loss                 | -0.0393   |\n",
      "|    n_updates            | 950       |\n",
      "|    policy_gradient_loss | -0.0355   |\n",
      "|    std                  | 0.581     |\n",
      "|    value_loss           | 0.0786    |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 96     |\n",
      "|    time_elapsed    | 1710   |\n",
      "|    total_timesteps | 393216 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1725        |\n",
      "|    total_timesteps      | 397312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015656415 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.96       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.576       |\n",
      "|    value_loss           | 0.0786      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=36.29 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 36.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017432043 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.91       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0347     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.574       |\n",
      "|    value_loss           | 0.0214      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 98     |\n",
      "|    time_elapsed    | 1747   |\n",
      "|    total_timesteps | 401408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1762        |\n",
      "|    total_timesteps      | 405504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012850569 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.88       |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    std                  | 0.572       |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1777        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015224183 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.86       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.00938    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.57        |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=35.59 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 35.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 410000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016266182 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.82       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    std                  | 0.567       |\n",
      "|    value_loss           | 0.0387      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 101    |\n",
      "|    time_elapsed    | 1798   |\n",
      "|    total_timesteps | 413696 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 1813        |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015520832 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.77       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0387     |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.564       |\n",
      "|    value_loss           | 0.0687      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=35.53 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 35.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015281947 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.73       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    std                  | 0.56        |\n",
      "|    value_loss           | 0.0531      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 103    |\n",
      "|    time_elapsed    | 1834   |\n",
      "|    total_timesteps | 421888 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 104        |\n",
      "|    time_elapsed         | 1849       |\n",
      "|    total_timesteps      | 425984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01456519 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -6.68      |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | 0.124      |\n",
      "|    n_updates            | 1030       |\n",
      "|    policy_gradient_loss | -0.0318    |\n",
      "|    std                  | 0.557      |\n",
      "|    value_loss           | 0.0798     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=34.96 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 35          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 430000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014721357 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.63       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.554       |\n",
      "|    value_loss           | 0.0287      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 105    |\n",
      "|    time_elapsed    | 1870   |\n",
      "|    total_timesteps | 430080 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 1885        |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015878253 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.6        |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    std                  | 0.552       |\n",
      "|    value_loss           | 0.0321      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 1900        |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015004493 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.55       |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0627     |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    std                  | 0.548       |\n",
      "|    value_loss           | 0.065       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=36.19 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 440000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015060179 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.52       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0544     |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    std                  | 0.546       |\n",
      "|    value_loss           | 0.061       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 108    |\n",
      "|    time_elapsed    | 1922   |\n",
      "|    total_timesteps | 442368 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 1937        |\n",
      "|    total_timesteps      | 446464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015479943 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.49       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    std                  | 0.544       |\n",
      "|    value_loss           | 0.0464      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=36.61 +/- 0.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 36.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 450000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017630933 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.45       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.542       |\n",
      "|    value_loss           | 0.0294      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 110    |\n",
      "|    time_elapsed    | 1958   |\n",
      "|    total_timesteps | 450560 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 1973        |\n",
      "|    total_timesteps      | 454656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015566348 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.42       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.539       |\n",
      "|    value_loss           | 0.0664      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 1988        |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015315067 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.39       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    std                  | 0.538       |\n",
      "|    value_loss           | 0.0857      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=36.41 +/- 0.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 36.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01713831 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -6.36      |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.536      |\n",
      "|    value_loss           | 0.051      |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 113    |\n",
      "|    time_elapsed    | 2010   |\n",
      "|    total_timesteps | 462848 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 2025        |\n",
      "|    total_timesteps      | 466944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015919663 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.32       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    std                  | 0.533       |\n",
      "|    value_loss           | 0.0737      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=37.93 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 37.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 470000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01641282 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -6.27      |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.529      |\n",
      "|    value_loss           | 0.0899     |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 115    |\n",
      "|    time_elapsed    | 2046   |\n",
      "|    total_timesteps | 471040 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 2062        |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017774474 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.22       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0442     |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    std                  | 0.526       |\n",
      "|    value_loss           | 0.028       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 2077        |\n",
      "|    total_timesteps      | 479232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015232812 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.18       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    std                  | 0.523       |\n",
      "|    value_loss           | 0.0406      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=38.57 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015319874 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.13       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0568     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    std                  | 0.52        |\n",
      "|    value_loss           | 0.0205      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 118    |\n",
      "|    time_elapsed    | 2099   |\n",
      "|    total_timesteps | 483328 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 2115        |\n",
      "|    total_timesteps      | 487424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017514104 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.09       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0489     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.518       |\n",
      "|    value_loss           | 0.0113      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=38.21 +/- 0.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 490000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016474899 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.06       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.054      |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    std                  | 0.516       |\n",
      "|    value_loss           | 0.0215      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 120    |\n",
      "|    time_elapsed    | 2137   |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 2153        |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015636794 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -6.03       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.0998      |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 122        |\n",
      "|    time_elapsed         | 2168       |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01598286 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -6.01      |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0243    |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    std                  | 0.513      |\n",
      "|    value_loss           | 0.0567     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=38.86 +/- 0.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015692065 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 0.0628      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 123    |\n",
      "|    time_elapsed    | 2189   |\n",
      "|    total_timesteps | 503808 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 124        |\n",
      "|    time_elapsed         | 2204       |\n",
      "|    total_timesteps      | 507904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01795742 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -5.92      |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 1230       |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    std                  | 0.507      |\n",
      "|    value_loss           | 0.0447     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=39.44 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 510000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015870139 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.88       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0346     |\n",
      "|    std                  | 0.505       |\n",
      "|    value_loss           | 0.0683      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 125    |\n",
      "|    time_elapsed    | 2225   |\n",
      "|    total_timesteps | 512000 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 2240        |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015337916 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.85       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0211     |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.502       |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=39.70 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017788688 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.81       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    std                  | 0.5         |\n",
      "|    value_loss           | 0.0786      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 230    |\n",
      "|    iterations      | 127    |\n",
      "|    time_elapsed    | 2261   |\n",
      "|    total_timesteps | 520192 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 128        |\n",
      "|    time_elapsed         | 2277       |\n",
      "|    total_timesteps      | 524288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01765946 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -5.78      |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 1270       |\n",
      "|    policy_gradient_loss | -0.0409    |\n",
      "|    std                  | 0.498      |\n",
      "|    value_loss           | 0.0497     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 2293        |\n",
      "|    total_timesteps      | 528384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017769638 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.74       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.495       |\n",
      "|    value_loss           | 0.0903      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=40.32 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 530000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014899833 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.69       |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.177       |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.492       |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 130    |\n",
      "|    time_elapsed    | 2316   |\n",
      "|    total_timesteps | 532480 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 131        |\n",
      "|    time_elapsed         | 2331       |\n",
      "|    total_timesteps      | 536576     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01670625 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -5.64      |\n",
      "|    explained_variance   | 0.887      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | 0.0101     |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.49       |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=39.86 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017883578 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.61       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0207     |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    std                  | 0.488       |\n",
      "|    value_loss           | 0.0825      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 132    |\n",
      "|    time_elapsed    | 2352   |\n",
      "|    total_timesteps | 540672 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 2368        |\n",
      "|    total_timesteps      | 544768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018532425 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.58       |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0637     |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.486       |\n",
      "|    value_loss           | 0.0341      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 2383        |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017602963 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.54       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.027      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    std                  | 0.484       |\n",
      "|    value_loss           | 0.0223      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=39.69 +/- 0.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 550000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019364446 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.49       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0676     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.48        |\n",
      "|    value_loss           | 0.0206      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 135    |\n",
      "|    time_elapsed    | 2405   |\n",
      "|    total_timesteps | 552960 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 2421        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017114988 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.44       |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    std                  | 0.477       |\n",
      "|    value_loss           | 0.0694      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=40.63 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019310163 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.4        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0424     |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.476       |\n",
      "|    value_loss           | 0.0126      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 137    |\n",
      "|    time_elapsed    | 2442   |\n",
      "|    total_timesteps | 561152 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 2457        |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019006725 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.37       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0679     |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    std                  | 0.473       |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 2473        |\n",
      "|    total_timesteps      | 569344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017467514 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.33       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0482     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0396     |\n",
      "|    std                  | 0.471       |\n",
      "|    value_loss           | 0.0181      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=39.55 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 570000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018338887 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.3        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.469       |\n",
      "|    value_loss           | 0.0386      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 140    |\n",
      "|    time_elapsed    | 2495   |\n",
      "|    total_timesteps | 573440 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 141         |\n",
      "|    time_elapsed         | 2511        |\n",
      "|    total_timesteps      | 577536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018432437 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.26       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0627     |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.467       |\n",
      "|    value_loss           | 0.0126      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=39.91 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 39.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 580000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01636048 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -5.21      |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | -0.0352    |\n",
      "|    std                  | 0.464      |\n",
      "|    value_loss           | 0.0687     |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 142    |\n",
      "|    time_elapsed    | 2532   |\n",
      "|    total_timesteps | 581632 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 2547        |\n",
      "|    total_timesteps      | 585728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018411793 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.17       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.462       |\n",
      "|    value_loss           | 0.0698      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 144         |\n",
      "|    time_elapsed         | 2563        |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019482967 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.12       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    std                  | 0.459       |\n",
      "|    value_loss           | 0.052       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=40.20 +/- 1.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 590000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020414684 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.06       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    std                  | 0.455       |\n",
      "|    value_loss           | 0.0249      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 145    |\n",
      "|    time_elapsed    | 2584   |\n",
      "|    total_timesteps | 593920 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 2599        |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019976884 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -5.01       |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.041      |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.453       |\n",
      "|    value_loss           | 0.0143      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=39.35 +/- 0.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021204695 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.96       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0619     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    std                  | 0.45        |\n",
      "|    value_loss           | 0.0215      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 147    |\n",
      "|    time_elapsed    | 2621   |\n",
      "|    total_timesteps | 602112 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 2636        |\n",
      "|    total_timesteps      | 606208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019577162 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.91       |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0677     |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.447       |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=39.87 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 1e+03     |\n",
      "|    mean_reward          | 39.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 610000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0193627 |\n",
      "|    clip_fraction        | 0.282     |\n",
      "|    clip_range           | 0.162     |\n",
      "|    entropy_loss         | -4.88     |\n",
      "|    explained_variance   | 0.924     |\n",
      "|    learning_rate        | 0.00014   |\n",
      "|    loss                 | -0.00727  |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0356   |\n",
      "|    std                  | 0.447     |\n",
      "|    value_loss           | 0.0427    |\n",
      "---------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 149    |\n",
      "|    time_elapsed    | 2658   |\n",
      "|    total_timesteps | 610304 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 2674        |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018208811 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.88       |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0582     |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.445       |\n",
      "|    value_loss           | 0.0298      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 151        |\n",
      "|    time_elapsed         | 2689       |\n",
      "|    total_timesteps      | 618496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01868207 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -4.85      |\n",
      "|    explained_variance   | 0.923      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    std                  | 0.444      |\n",
      "|    value_loss           | 0.0481     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=39.51 +/- 1.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018292006 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.82       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0425     |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    std                  | 0.442       |\n",
      "|    value_loss           | 0.0652      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 152    |\n",
      "|    time_elapsed    | 2713   |\n",
      "|    total_timesteps | 622592 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 2729        |\n",
      "|    total_timesteps      | 626688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022179116 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.78       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0682     |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    std                  | 0.44        |\n",
      "|    value_loss           | 0.0119      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=39.34 +/- 1.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 39.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 630000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01750183 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -4.73      |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 1530       |\n",
      "|    policy_gradient_loss | -0.035     |\n",
      "|    std                  | 0.437      |\n",
      "|    value_loss           | 0.0311     |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 154    |\n",
      "|    time_elapsed    | 2751   |\n",
      "|    total_timesteps | 630784 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 2767        |\n",
      "|    total_timesteps      | 634880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019704478 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.69       |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    std                  | 0.435       |\n",
      "|    value_loss           | 0.0654      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 2782        |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019545965 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.66       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0406     |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.036      |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 0.0583      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=41.77 +/- 2.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018170796 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.62       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    std                  | 0.431       |\n",
      "|    value_loss           | 0.0743      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 157    |\n",
      "|    time_elapsed    | 2803   |\n",
      "|    total_timesteps | 643072 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 158         |\n",
      "|    time_elapsed         | 2818        |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021568358 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.57       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.073      |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    std                  | 0.429       |\n",
      "|    value_loss           | 0.0122      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=39.89 +/- 2.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 39.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015720502 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.52       |\n",
      "|    explained_variance   | 0.862       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.426       |\n",
      "|    value_loss           | 0.0805      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 159    |\n",
      "|    time_elapsed    | 2840   |\n",
      "|    total_timesteps | 651264 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 2855        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021336421 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.47       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0624     |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.423       |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 2870        |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020097602 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.44       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.06       |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.422       |\n",
      "|    value_loss           | 0.0138      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=41.88 +/- 1.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020890042 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.39       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    std                  | 0.419       |\n",
      "|    value_loss           | 0.0281      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 162    |\n",
      "|    time_elapsed    | 2892   |\n",
      "|    total_timesteps | 663552 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 2908        |\n",
      "|    total_timesteps      | 667648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018367449 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.36       |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0561     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 0.418       |\n",
      "|    value_loss           | 0.0361      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=41.83 +/- 1.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 41.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020651422 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.33       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    std                  | 0.417       |\n",
      "|    value_loss           | 0.0329      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 164    |\n",
      "|    time_elapsed    | 2929   |\n",
      "|    total_timesteps | 671744 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 165        |\n",
      "|    time_elapsed         | 2945       |\n",
      "|    total_timesteps      | 675840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01823989 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -4.31      |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0375    |\n",
      "|    std                  | 0.415      |\n",
      "|    value_loss           | 0.0636     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 166        |\n",
      "|    time_elapsed         | 2960       |\n",
      "|    total_timesteps      | 679936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02040691 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -4.27      |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0553    |\n",
      "|    n_updates            | 1650       |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.413      |\n",
      "|    value_loss           | 0.0479     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=42.95 +/- 1.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 43         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 680000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01887312 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -4.21      |\n",
      "|    explained_variance   | 0.885      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0561    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0368    |\n",
      "|    std                  | 0.409      |\n",
      "|    value_loss           | 0.0352     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 167    |\n",
      "|    time_elapsed    | 2982   |\n",
      "|    total_timesteps | 684032 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 2997        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018303223 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.18       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.0337     |\n",
      "|    std                  | 0.409       |\n",
      "|    value_loss           | 0.073       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=42.74 +/- 1.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 690000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022818074 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.14       |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0444     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.406       |\n",
      "|    value_loss           | 0.0103      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 169    |\n",
      "|    time_elapsed    | 3019   |\n",
      "|    total_timesteps | 692224 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 3034        |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020516694 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.09       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0595     |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.404       |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=42.93 +/- 2.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019931294 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -4.04       |\n",
      "|    explained_variance   | 0.902       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0577     |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    std                  | 0.401       |\n",
      "|    value_loss           | 0.052       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 171    |\n",
      "|    time_elapsed    | 3056   |\n",
      "|    total_timesteps | 700416 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 172        |\n",
      "|    time_elapsed         | 3072       |\n",
      "|    total_timesteps      | 704512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02133397 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -4         |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0562    |\n",
      "|    n_updates            | 1710       |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    std                  | 0.399      |\n",
      "|    value_loss           | 0.0097     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 173        |\n",
      "|    time_elapsed         | 3087       |\n",
      "|    total_timesteps      | 708608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02240598 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -3.96      |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.397      |\n",
      "|    value_loss           | 0.00788    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=42.13 +/- 1.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 710000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023973424 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.92       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.07       |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    std                  | 0.395       |\n",
      "|    value_loss           | 0.00824     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 174    |\n",
      "|    time_elapsed    | 3109   |\n",
      "|    total_timesteps | 712704 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 3124        |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019547608 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.87       |\n",
      "|    explained_variance   | 0.879       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0379     |\n",
      "|    std                  | 0.392       |\n",
      "|    value_loss           | 0.036       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=42.68 +/- 1.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022672117 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.8        |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0628     |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    std                  | 0.389       |\n",
      "|    value_loss           | 0.00833     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 176    |\n",
      "|    time_elapsed    | 3146   |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 177         |\n",
      "|    time_elapsed         | 3162        |\n",
      "|    total_timesteps      | 724992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021204757 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.75       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    std                  | 0.387       |\n",
      "|    value_loss           | 0.0344      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 3177        |\n",
      "|    total_timesteps      | 729088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022051021 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.72       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0399     |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    std                  | 0.385       |\n",
      "|    value_loss           | 0.0644      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=42.57 +/- 3.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 42.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 730000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019280067 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.68       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    std                  | 0.384       |\n",
      "|    value_loss           | 0.0149      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 179    |\n",
      "|    time_elapsed    | 3199   |\n",
      "|    total_timesteps | 733184 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 3215        |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023596963 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.63       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    std                  | 0.381       |\n",
      "|    value_loss           | 0.00632     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=43.84 +/- 2.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 43.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02516978 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -3.58      |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0749    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.379      |\n",
      "|    value_loss           | 0.00578    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 181    |\n",
      "|    time_elapsed    | 3237   |\n",
      "|    total_timesteps | 741376 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 3252        |\n",
      "|    total_timesteps      | 745472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022264548 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.55       |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.378       |\n",
      "|    value_loss           | 0.00563     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 3267        |\n",
      "|    total_timesteps      | 749568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020507254 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.53       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    std                  | 0.377       |\n",
      "|    value_loss           | 0.0185      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=37.87 +/- 10.46\n",
      "Episode length: 886.00 +/- 228.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 886         |\n",
      "|    mean_reward          | 37.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 750000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024768334 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.49       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.042      |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    std                  | 0.374       |\n",
      "|    value_loss           | 0.00353     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 184    |\n",
      "|    time_elapsed    | 3288   |\n",
      "|    total_timesteps | 753664 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 185         |\n",
      "|    time_elapsed         | 3303        |\n",
      "|    total_timesteps      | 757760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022046817 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.44       |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0483     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.372       |\n",
      "|    value_loss           | 0.00746     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=45.99 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 46          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022425812 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.4        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.059      |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 0.00511     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 186    |\n",
      "|    time_elapsed    | 3325   |\n",
      "|    total_timesteps | 761856 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 3340        |\n",
      "|    total_timesteps      | 765952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020272303 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.36       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0458     |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.368       |\n",
      "|    value_loss           | 0.0111      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=44.31 +/- 1.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 770000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025791343 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.3        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0602     |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.365       |\n",
      "|    value_loss           | 0.00333     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 188    |\n",
      "|    time_elapsed    | 3361   |\n",
      "|    total_timesteps | 770048 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 3376        |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024384903 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.25       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0824     |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 190        |\n",
      "|    time_elapsed         | 3392       |\n",
      "|    total_timesteps      | 778240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02139103 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -3.19      |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 1890       |\n",
      "|    policy_gradient_loss | -0.0425    |\n",
      "|    std                  | 0.361      |\n",
      "|    value_loss           | 0.0039     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=43.86 +/- 3.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 43.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023569232 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.14       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.358       |\n",
      "|    value_loss           | 0.00412     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 191    |\n",
      "|    time_elapsed    | 3413   |\n",
      "|    total_timesteps | 782336 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 192         |\n",
      "|    time_elapsed         | 3428        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024508042 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3.08       |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0667     |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.355       |\n",
      "|    value_loss           | 0.00387     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=44.48 +/- 1.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 790000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022712369 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -3          |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.352       |\n",
      "|    value_loss           | 0.0283      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 193    |\n",
      "|    time_elapsed    | 3450   |\n",
      "|    total_timesteps | 790528 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 3465        |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021887243 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0629     |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.348       |\n",
      "|    value_loss           | 0.0193      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 3480        |\n",
      "|    total_timesteps      | 798720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023832878 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0645     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    std                  | 0.345       |\n",
      "|    value_loss           | 0.00738     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=44.61 +/- 2.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022502042 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.81       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0478     |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 0.0118      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 196    |\n",
      "|    time_elapsed    | 3502   |\n",
      "|    total_timesteps | 802816 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 197         |\n",
      "|    time_elapsed         | 3517        |\n",
      "|    total_timesteps      | 806912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027831983 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0555     |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.341       |\n",
      "|    value_loss           | 0.00367     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=44.66 +/- 2.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 810000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025207954 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0637     |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.338       |\n",
      "|    value_loss           | 0.00498     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 229    |\n",
      "|    iterations      | 198    |\n",
      "|    time_elapsed    | 3541   |\n",
      "|    total_timesteps | 811008 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 3556        |\n",
      "|    total_timesteps      | 815104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023986124 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.61       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0408     |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    std                  | 0.335       |\n",
      "|    value_loss           | 0.0256      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 3572        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024285398 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0735     |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    std                  | 0.332       |\n",
      "|    value_loss           | 0.00451     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=44.81 +/- 1.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026544202 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.062      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0423     |\n",
      "|    std                  | 0.33        |\n",
      "|    value_loss           | 0.00216     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 201    |\n",
      "|    time_elapsed    | 3595   |\n",
      "|    total_timesteps | 823296 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 3611        |\n",
      "|    total_timesteps      | 827392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023397852 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.056      |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    std                  | 0.329       |\n",
      "|    value_loss           | 0.00714     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=45.54 +/- 0.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022490246 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.39       |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0664     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.326       |\n",
      "|    value_loss           | 0.00314     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 203    |\n",
      "|    time_elapsed    | 3633   |\n",
      "|    total_timesteps | 831488 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 204         |\n",
      "|    time_elapsed         | 3648        |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026036493 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    std                  | 0.324       |\n",
      "|    value_loss           | 0.00627     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 3663        |\n",
      "|    total_timesteps      | 839680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023395821 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0475     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    std                  | 0.322       |\n",
      "|    value_loss           | 0.00425     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=44.89 +/- 0.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028365066 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.23       |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.068      |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.319       |\n",
      "|    value_loss           | 0.00271     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 206    |\n",
      "|    time_elapsed    | 3685   |\n",
      "|    total_timesteps | 843776 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 3700        |\n",
      "|    total_timesteps      | 847872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026718415 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0636     |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.318       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=45.49 +/- 0.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 850000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026374698 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0766     |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.315       |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 208    |\n",
      "|    time_elapsed    | 3721   |\n",
      "|    total_timesteps | 851968 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 209         |\n",
      "|    time_elapsed         | 3738        |\n",
      "|    total_timesteps      | 856064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028589202 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.312       |\n",
      "|    value_loss           | 0.0024      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=43.10 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 43.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 860000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02362968 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -2.01      |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 2090       |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    std                  | 0.311      |\n",
      "|    value_loss           | 0.0048     |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 210    |\n",
      "|    time_elapsed    | 3761   |\n",
      "|    total_timesteps | 860160 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 3777        |\n",
      "|    total_timesteps      | 864256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026199607 |\n",
      "|    clip_fraction        | 0.329       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0539     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    std                  | 0.31        |\n",
      "|    value_loss           | 0.00289     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 3793        |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027346916 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.308       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=45.43 +/- 1.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 870000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026005495 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0781     |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    std                  | 0.306       |\n",
      "|    value_loss           | 0.0027      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 213    |\n",
      "|    time_elapsed    | 3815   |\n",
      "|    total_timesteps | 872448 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 3830        |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027439632 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    std                  | 0.304       |\n",
      "|    value_loss           | 0.0016      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=45.80 +/- 0.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025946278 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0547     |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.301       |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 215    |\n",
      "|    time_elapsed    | 3852   |\n",
      "|    total_timesteps | 880640 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 3867        |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029785104 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0491     |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.298       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 3882        |\n",
      "|    total_timesteps      | 888832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029248782 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0722     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.296       |\n",
      "|    value_loss           | 0.00186     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=46.21 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 46.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 890000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025790077 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0569     |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.294       |\n",
      "|    value_loss           | 0.00171     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 218    |\n",
      "|    time_elapsed    | 3904   |\n",
      "|    total_timesteps | 892928 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 219         |\n",
      "|    time_elapsed         | 3920        |\n",
      "|    total_timesteps      | 897024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025666617 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.291       |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=46.00 +/- 0.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 46          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025362749 |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.042      |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.29        |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 220    |\n",
      "|    time_elapsed    | 3942   |\n",
      "|    total_timesteps | 901120 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 221        |\n",
      "|    time_elapsed         | 3957       |\n",
      "|    total_timesteps      | 905216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02723874 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.952      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0559    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.287      |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 3972        |\n",
      "|    total_timesteps      | 909312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029547444 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0597     |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.284       |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=44.81 +/- 2.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 910000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027642049 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.281       |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 223    |\n",
      "|    time_elapsed    | 3994   |\n",
      "|    total_timesteps | 913408 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 4009        |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026129719 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    std                  | 0.279       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=45.52 +/- 0.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031942908 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0422     |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    std                  | 0.277       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 225    |\n",
      "|    time_elapsed    | 4031   |\n",
      "|    total_timesteps | 921600 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 4046        |\n",
      "|    total_timesteps      | 925696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027770637 |\n",
      "|    clip_fraction        | 0.337       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.275       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 4061        |\n",
      "|    total_timesteps      | 929792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028229062 |\n",
      "|    clip_fraction        | 0.343       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    std                  | 0.273       |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=45.44 +/- 0.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 45.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 930000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02912202 |\n",
      "|    clip_fraction        | 0.338      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -0.906     |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0615    |\n",
      "|    n_updates            | 2270       |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.27       |\n",
      "|    value_loss           | 0.000906   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 228    |\n",
      "|    time_elapsed    | 4082   |\n",
      "|    total_timesteps | 933888 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 229        |\n",
      "|    time_elapsed         | 4100       |\n",
      "|    total_timesteps      | 937984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02948255 |\n",
      "|    clip_fraction        | 0.344      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -0.842     |\n",
      "|    explained_variance   | 0.88       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    std                  | 0.268      |\n",
      "|    value_loss           | 0.00192    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=44.99 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 45         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 940000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02867493 |\n",
      "|    clip_fraction        | 0.342      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -0.785     |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0477    |\n",
      "|    n_updates            | 2290       |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    std                  | 0.267      |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 230    |\n",
      "|    time_elapsed    | 4122   |\n",
      "|    total_timesteps | 942080 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 4138        |\n",
      "|    total_timesteps      | 946176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029799433 |\n",
      "|    clip_fraction        | 0.33        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0622     |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.264       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=45.00 +/- 0.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 45         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 950000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03048656 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -0.648     |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 2310       |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.262      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 232    |\n",
      "|    time_elapsed    | 4160   |\n",
      "|    total_timesteps | 950272 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 4177        |\n",
      "|    total_timesteps      | 954368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030216403 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.259       |\n",
      "|    value_loss           | 0.0008      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 234         |\n",
      "|    time_elapsed         | 4193        |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029784564 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.047      |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.257       |\n",
      "|    value_loss           | 0.000757    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=44.98 +/- 0.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027094986 |\n",
      "|    clip_fraction        | 0.318       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.0549      |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0608     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.255       |\n",
      "|    value_loss           | 0.0293      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 235    |\n",
      "|    time_elapsed    | 4215   |\n",
      "|    total_timesteps | 962560 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 236        |\n",
      "|    time_elapsed         | 4231       |\n",
      "|    total_timesteps      | 966656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02787447 |\n",
      "|    clip_fraction        | 0.349      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | -0.397     |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 2350       |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    std                  | 0.254      |\n",
      "|    value_loss           | 0.00695    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=45.13 +/- 1.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 45.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 970000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032876812 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.056      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.253       |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 237    |\n",
      "|    time_elapsed    | 4252   |\n",
      "|    total_timesteps | 970752 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 4267        |\n",
      "|    total_timesteps      | 974848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029555324 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0717     |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.251       |\n",
      "|    value_loss           | 0.002       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 4283        |\n",
      "|    total_timesteps      | 978944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031991787 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0539     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.249       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=44.31 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029481914 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.081       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.247       |\n",
      "|    value_loss           | 0.00693     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 228    |\n",
      "|    iterations      | 240    |\n",
      "|    time_elapsed    | 4307   |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 4324        |\n",
      "|    total_timesteps      | 987136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030106235 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.156      |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0542     |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.247       |\n",
      "|    value_loss           | 0.0111      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=44.62 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 44.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 990000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028144568 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.127      |\n",
      "|    explained_variance   | 0.00246     |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0618     |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.246       |\n",
      "|    value_loss           | 0.0592      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 227    |\n",
      "|    iterations      | 242    |\n",
      "|    time_elapsed    | 4348   |\n",
      "|    total_timesteps | 991232 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 4364        |\n",
      "|    total_timesteps      | 995328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030652054 |\n",
      "|    clip_fraction        | 0.334       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.082      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0339     |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.244       |\n",
      "|    value_loss           | 0.00413     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 244         |\n",
      "|    time_elapsed         | 4379        |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032389466 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.162       |\n",
      "|    entropy_loss         | -0.00902    |\n",
      "|    explained_variance   | -0.066      |\n",
      "|    learning_rate        | 0.00014     |\n",
      "|    loss                 | -0.0547     |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.242       |\n",
      "|    value_loss           | 0.00582     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=44.88 +/- 0.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1e+03      |\n",
      "|    mean_reward          | 44.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03045904 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.162      |\n",
      "|    entropy_loss         | 0.0393     |\n",
      "|    explained_variance   | 0.826      |\n",
      "|    learning_rate        | 0.00014    |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    std                  | 0.24       |\n",
      "|    value_loss           | 0.00402    |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 228     |\n",
      "|    iterations      | 245     |\n",
      "|    time_elapsed    | 4401    |\n",
      "|    total_timesteps | 1003520 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x379e59450>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 1000000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_policy(env, policy, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - policy: La policy addestrata da valutare.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Genera un'azione casuale\n",
    "            obs, reward, done, _ = env.step(np.array([action]))  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(env, model)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}, Std: {std_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}, Std: {std_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
