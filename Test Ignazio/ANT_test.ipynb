{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag\n",
    "        # Tensorbaord will find & display metrics from the `SCALARS` tab\n",
    "        metric_dict = {\n",
    "            #\"rollout/ep_len_mean\": 0,\n",
    "            #\"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_1 (PPO_4) -> {'reset_noise_scale': 0.16872520546404454, 'forward_reward_weight': 0.569165596187308, 'ctrl_cost_weight': 0.15369909636721105, 'healthy_reward': 1.1651483169773327, 'learning_rate': 0.00025118614395972893, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9900195327210904, 'gae_lambda': 0.8063306496367846, 'clip_range': 0.1411162146550987, 'ent_coef': 0.006226601057899701, 'variance_penalty_weight': 0.0007310600475679448}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_2 (PPO_6) -> \n",
    "\n",
    "hp_reset_noise_scale=0.10405074414945424 # scala del rumore quando l'ambiente viene resettato \n",
    "\n",
    "hp_forward_reward_weight=0.5940601384640877 # peso del reward per il movimento in avanti\n",
    "\n",
    "hp_ctrl_cost_weight=0.14771040407991193 # peso del reward per il controllo\n",
    "\n",
    "hp_healthy_reward =1.4039427670916238 # reward per la salute\n",
    "\n",
    "\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "\n",
    "hp_learning_rate=0.00014010166026390974           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "\n",
    "hp_gamma=0.9974446213345484      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "\n",
    "hp_gae_lambda=0.8025419607496327              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "\n",
    "hp_clip_range=0.16218657788555388               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "\n",
    "hp_ent_coef=0.00017603718662988996                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_7 {'reset_noise_scale': 0.16260110616284057, 'forward_reward_weight': 0.6594701821995568, 'ctrl_cost_weight': 0.13678469591501632, 'healthy_reward': 1.4384387807236847, 'contact_cost_weight': 0.0007721118603343064, 'healthy_z_lower': 0.11270460095319094, 'healthy_z_upper': 1.1367622027728483, 'contact_force_min': -0.8099290655891269, 'contact_force_max': 0.7683440461793597, 'learning_rate': 0.0001620494220647337, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9960403688730154, 'gae_lambda': 0.8519055821923104, 'clip_range': 0.28172421812629234, 'ent_coef': 0.015960745859518122, 'variance_penalty_weight': 0.011924537413547313}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_8 -> {'reset_noise_scale': 0.14953307712823055, 'forward_reward_weight': 0.5971580841907844, 'ctrl_cost_weight': 0.21085190913852067, 'healthy_reward': 1.3432502908397173, 'contact_cost_weight': 0.0006565424645557624, 'healthy_z_lower': 0.11576255546554826, 'healthy_z_upper': 1.0657755912005253, 'contact_force_min': -0.7947792512332761, 'contact_force_max': 0.7599774107257553, 'learning_rate': 0.0001417417141818677, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9977321276628237, 'gae_lambda': 0.8135998374897728, 'clip_range': 0.2502648636777115, 'ent_coef': 0.006686448422595028, 'variance_penalty_weight': 0.0008985044453683972}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_9 -> {'reset_noise_scale': 0.1224648700491494, 'forward_reward_weight': 1.0798217517026751, 'ctrl_cost_weight': 0.2788960190947023, 'healthy_reward': 1.4972086156641724, 'contact_cost_weight': 0.00019495257535118138, 'healthy_z_lower': 0.10525289571959973, 'healthy_z_upper': 1.1803240798353063, 'contact_force_min': -0.5187992701613672, 'contact_force_max': 0.5870857431066443, 'learning_rate': 0.000983439712869658, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9686135698396399, 'gae_lambda': 0.9145395422692033, 'clip_range': 0.37757085535729756, 'ent_coef': 0.00017055556769922042, 'std_penalty_weight': 0.28813167612676016}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_10 -> {'reset_noise_scale': 0.0811219889284557, 'forward_reward_weight': 0.794019967338759, 'ctrl_cost_weight': 0.1909084649203593, 'healthy_reward': 1.4695470159426132, 'contact_cost_weight': 0.00048075670076003045, 'healthy_z_lower': 0.19353492629665098, 'healthy_z_upper': 1.1936905952567158, 'contact_force_min': -0.5349939620294489, 'contact_force_max': 0.7307512698224117, 'learning_rate': 0.0003564760563058714, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9762294172462653, 'gae_lambda': 0.9261117656360015, 'clip_range': 0.3320669028429513, 'ent_coef': 0.0026780011357637598, 'std_penalty_weight': 0.20050838533111062}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_11 -> {'reset_noise_scale': 0.06289400759020149, 'forward_reward_weight': 1.3699282930520178, 'ctrl_cost_weight': 0.5242914498967779, 'healthy_reward': 1.4968306649109204, 'contact_cost_weight': 0.00024593968525559866, 'healthy_z_lower': 0.11613320759514574, 'healthy_z_upper': 0.9413487983685294, 'contact_force_min': -0.6206213160250571, 'contact_force_max': 0.5772050566570148, 'learning_rate': 5.801543349523308e-05, 'n_steps': 4096, 'batch_size': 1024, 'gamma': 0.9672688138657076, 'gae_lambda': 0.9847655529264521, 'clip_range': 0.4313609104957057, 'ent_coef': 0.08466995353062254}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale= 0.06289400759020149 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight = 1.3699282930520178 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 0.5242914498967779 # peso del reward per il controllo\n",
    "hp_healthy_reward = 1.4968306649109204 # reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 0.00024593968525559866\n",
    "healthy_z = (0.11613320759514574, 0.9413487983685294)\n",
    "contact_force = (-0.6206213160250571, 0.5772050566570148)\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=5.801543349523308e-05           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=1024                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.9672688138657076      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.9847655529264521              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.4313609104957057               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.08466995353062254                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z,\n",
    "                    contact_force_range=contact_force,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "#env = DummyVecEnv([make_env])  \n",
    "\n",
    "\n",
    "NUM_ENVS=8\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=hp_learning_rate,           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "    n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    seed=42,                        # Seed per la riproducibilità\n",
    "    verbose=1,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=10000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_11\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1786  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1358         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041557234 |\n",
      "|    clip_fraction        | 0.000351     |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -1.3         |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -0.863       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.404        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=35.53 +/- 0.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 35.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045937216 |\n",
      "|    clip_fraction        | 0.000528     |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -11.6        |\n",
      "|    explained_variance   | -0.173       |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -0.927       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00273     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.162        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1355  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 98304 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1359         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046936916 |\n",
      "|    clip_fraction        | 0.00054      |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -11.7        |\n",
      "|    explained_variance   | -0.0144      |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -0.95        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00279     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 0.112        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=38.11 +/- 0.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 38.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004221293 |\n",
      "|    clip_fraction        | 0.00033     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -11.8       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -0.95       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1367   |\n",
      "|    iterations      | 5      |\n",
      "|    time_elapsed    | 119    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1328        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 148         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004955177 |\n",
      "|    clip_fraction        | 0.000577    |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -12         |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -0.978      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1353       |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 169        |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00439932 |\n",
      "|    clip_fraction        | 0.000391   |\n",
      "|    clip_range           | 0.431      |\n",
      "|    entropy_loss         | -12.1      |\n",
      "|    explained_variance   | 0.25       |\n",
      "|    learning_rate        | 5.8e-05    |\n",
      "|    loss                 | -0.991     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00288   |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=40.62 +/- 0.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 40.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004374562 |\n",
      "|    clip_fraction        | 0.000327    |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -12.2       |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -0.996      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 0.0989      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1316   |\n",
      "|    iterations      | 8      |\n",
      "|    time_elapsed    | 199    |\n",
      "|    total_timesteps | 262144 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1336         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051404024 |\n",
      "|    clip_fraction        | 0.000455     |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -12.3        |\n",
      "|    explained_variance   | 0.288        |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -1.01        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 0.103        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=40.01 +/- 0.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 40           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058757123 |\n",
      "|    clip_fraction        | 0.000922     |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -12.5        |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -1.02        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00371     |\n",
      "|    std                  | 1.16         |\n",
      "|    value_loss           | 0.105        |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1319   |\n",
      "|    iterations      | 10     |\n",
      "|    time_elapsed    | 248    |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1336        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 269         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006731528 |\n",
      "|    clip_fraction        | 0.00137     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -12.6       |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.03       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.0912      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1347        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 291         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007963864 |\n",
      "|    clip_fraction        | 0.00258     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -12.7       |\n",
      "|    explained_variance   | 0.413       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.04       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=26.91 +/- 14.61\n",
      "Episode length: 702.80 +/- 391.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 703          |\n",
      "|    mean_reward          | 26.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073181083 |\n",
      "|    clip_fraction        | 0.00163      |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -12.8        |\n",
      "|    explained_variance   | 0.444        |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -1.06        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00432     |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 0.0842       |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1342   |\n",
      "|    iterations      | 13     |\n",
      "|    time_elapsed    | 317    |\n",
      "|    total_timesteps | 425984 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1355         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 338          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092983665 |\n",
      "|    clip_fraction        | 0.00309      |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -12.9        |\n",
      "|    explained_variance   | 0.389        |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -1.06        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00478     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 0.108        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=2.84 +/- 2.49\n",
      "Episode length: 86.40 +/- 90.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 86.4        |\n",
      "|    mean_reward          | 2.84        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009457162 |\n",
      "|    clip_fraction        | 0.00387     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.1       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.09       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 0.074       |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1344   |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 365    |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1348        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 388         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012450785 |\n",
      "|    clip_fraction        | 0.00716     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.2       |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.09       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 0.0866      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1341        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 415         |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010566966 |\n",
      "|    clip_fraction        | 0.00523     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.3       |\n",
      "|    explained_variance   | 0.506       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.09       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    std                  | 1.29        |\n",
      "|    value_loss           | 0.0876      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=6.09 +/- 8.28\n",
      "Episode length: 226.60 +/- 386.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 6.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010777289 |\n",
      "|    clip_fraction        | 0.0051      |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.4       |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.12       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    std                  | 1.31        |\n",
      "|    value_loss           | 0.0818      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1343   |\n",
      "|    iterations      | 18     |\n",
      "|    time_elapsed    | 439    |\n",
      "|    total_timesteps | 589824 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1335        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011376881 |\n",
      "|    clip_fraction        | 0.00678     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.6       |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.12       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    std                  | 1.33        |\n",
      "|    value_loss           | 0.0812      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=4.15 +/- 6.70\n",
      "Episode length: 223.00 +/- 388.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 223         |\n",
      "|    mean_reward          | 4.15        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014001649 |\n",
      "|    clip_fraction        | 0.0102      |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.7       |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.14       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00687    |\n",
      "|    std                  | 1.35        |\n",
      "|    value_loss           | 0.0816      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1334   |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 491    |\n",
      "|    total_timesteps | 655360 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1337        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 514         |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010401929 |\n",
      "|    clip_fraction        | 0.00534     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.8       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.14       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00545    |\n",
      "|    std                  | 1.37        |\n",
      "|    value_loss           | 0.065       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=2.90 +/- 4.63\n",
      "Episode length: 211.80 +/- 394.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 212         |\n",
      "|    mean_reward          | 2.9         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009150586 |\n",
      "|    clip_fraction        | 0.00307     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -13.9       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.16       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    std                  | 1.39        |\n",
      "|    value_loss           | 0.0603      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1336   |\n",
      "|    iterations      | 22     |\n",
      "|    time_elapsed    | 539    |\n",
      "|    total_timesteps | 720896 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1335        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 564         |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011348262 |\n",
      "|    clip_fraction        | 0.00588     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14         |\n",
      "|    explained_variance   | 0.614       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.16       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    std                  | 1.41        |\n",
      "|    value_loss           | 0.0653      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1344        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 585         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011652832 |\n",
      "|    clip_fraction        | 0.00616     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14.1       |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.18       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    std                  | 1.43        |\n",
      "|    value_loss           | 0.0719      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=0.85 +/- 0.70\n",
      "Episode length: 25.60 +/- 9.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.6        |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013744935 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14.3       |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.19       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    std                  | 1.45        |\n",
      "|    value_loss           | 0.0667      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1339   |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 611    |\n",
      "|    total_timesteps | 819200 |\n",
      "-------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1343       |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 633        |\n",
      "|    total_timesteps      | 851968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01143948 |\n",
      "|    clip_fraction        | 0.0065     |\n",
      "|    clip_range           | 0.431      |\n",
      "|    entropy_loss         | -14.4      |\n",
      "|    explained_variance   | 0.64       |\n",
      "|    learning_rate        | 5.8e-05    |\n",
      "|    loss                 | -1.2       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.00485   |\n",
      "|    std                  | 1.47       |\n",
      "|    value_loss           | 0.0689     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=0.90 +/- 0.70\n",
      "Episode length: 20.20 +/- 6.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | 0.899       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 880000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009795535 |\n",
      "|    clip_fraction        | 0.00372     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14.5       |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.21       |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    std                  | 1.49        |\n",
      "|    value_loss           | 0.0608      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1349   |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 655    |\n",
      "|    total_timesteps | 884736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1343        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 682         |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011134609 |\n",
      "|    clip_fraction        | 0.00594     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14.6       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.23       |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    std                  | 1.52        |\n",
      "|    value_loss           | 0.0601      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1353        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 702         |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013483207 |\n",
      "|    clip_fraction        | 0.00898     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14.7       |\n",
      "|    explained_variance   | 0.715       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.23       |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    std                  | 1.54        |\n",
      "|    value_loss           | 0.0603      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=0.37 +/- 0.22\n",
      "Episode length: 14.80 +/- 4.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | 0.373       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013466643 |\n",
      "|    clip_fraction        | 0.00868     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -14.9       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.25       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    std                  | 1.56        |\n",
      "|    value_loss           | 0.0529      |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1353   |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 726    |\n",
      "|    total_timesteps | 983040 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1357        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 748         |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010265533 |\n",
      "|    clip_fraction        | 0.00465     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -15         |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.26       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    std                  | 1.59        |\n",
      "|    value_loss           | 0.0535      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=0.12 +/- 0.12\n",
      "Episode length: 14.00 +/- 4.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | 0.122       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010115848 |\n",
      "|    clip_fraction        | 0.00406     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -15.1       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.27       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    std                  | 1.62        |\n",
      "|    value_loss           | 0.0561      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1356    |\n",
      "|    iterations      | 32      |\n",
      "|    time_elapsed    | 773     |\n",
      "|    total_timesteps | 1048576 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1363         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 793          |\n",
      "|    total_timesteps      | 1081344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072586634 |\n",
      "|    clip_fraction        | 0.00172      |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -15.2        |\n",
      "|    explained_variance   | 0.635        |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -1.28        |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    std                  | 1.64         |\n",
      "|    value_loss           | 0.0509       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1370         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 812          |\n",
      "|    total_timesteps      | 1114112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085200565 |\n",
      "|    clip_fraction        | 0.00235      |\n",
      "|    clip_range           | 0.431        |\n",
      "|    entropy_loss         | -15.4        |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 5.8e-05      |\n",
      "|    loss                 | -1.29        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00417     |\n",
      "|    std                  | 1.66         |\n",
      "|    value_loss           | 0.0549       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=0.18 +/- 0.26\n",
      "Episode length: 12.80 +/- 3.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | 0.182       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008770161 |\n",
      "|    clip_fraction        | 0.00295     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -15.5       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.3        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    std                  | 1.69        |\n",
      "|    value_loss           | 0.0504      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1369    |\n",
      "|    iterations      | 35      |\n",
      "|    time_elapsed    | 837     |\n",
      "|    total_timesteps | 1146880 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1373        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010598952 |\n",
      "|    clip_fraction        | 0.00489     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -15.6       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.3        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00445    |\n",
      "|    std                  | 1.71        |\n",
      "|    value_loss           | 0.0488      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=0.31 +/- 0.44\n",
      "Episode length: 20.80 +/- 5.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.8        |\n",
      "|    mean_reward          | 0.311       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010610117 |\n",
      "|    clip_fraction        | 0.00514     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -15.7       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.32       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00495    |\n",
      "|    std                  | 1.74        |\n",
      "|    value_loss           | 0.0474      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1368    |\n",
      "|    iterations      | 37      |\n",
      "|    time_elapsed    | 885     |\n",
      "|    total_timesteps | 1212416 |\n",
      "--------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1374       |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 905        |\n",
      "|    total_timesteps      | 1245184    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01373658 |\n",
      "|    clip_fraction        | 0.00978    |\n",
      "|    clip_range           | 0.431      |\n",
      "|    entropy_loss         | -15.8      |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 5.8e-05    |\n",
      "|    loss                 | -1.34      |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 1.77       |\n",
      "|    value_loss           | 0.0454     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1372        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 930         |\n",
      "|    total_timesteps      | 1277952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009624705 |\n",
      "|    clip_fraction        | 0.00399     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -16         |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.34       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00413    |\n",
      "|    std                  | 1.8         |\n",
      "|    value_loss           | 0.0453      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=0.56 +/- 0.46\n",
      "Episode length: 22.60 +/- 5.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010288856 |\n",
      "|    clip_fraction        | 0.00516     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -16.1       |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.35       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    std                  | 1.82        |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1375    |\n",
      "|    iterations      | 40      |\n",
      "|    time_elapsed    | 952     |\n",
      "|    total_timesteps | 1310720 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1376        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 975         |\n",
      "|    total_timesteps      | 1343488     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011703427 |\n",
      "|    clip_fraction        | 0.00611     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -16.2       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.37       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00521    |\n",
      "|    std                  | 1.86        |\n",
      "|    value_loss           | 0.047       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=0.18 +/- 0.12\n",
      "Episode length: 16.20 +/- 5.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | 0.177       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009964814 |\n",
      "|    clip_fraction        | 0.00408     |\n",
      "|    clip_range           | 0.431       |\n",
      "|    entropy_loss         | -16.4       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 5.8e-05     |\n",
      "|    loss                 | -1.37       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00459    |\n",
      "|    std                  | 1.88        |\n",
      "|    value_loss           | 0.0487      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1373    |\n",
      "|    iterations      | 42      |\n",
      "|    time_elapsed    | 1002    |\n",
      "|    total_timesteps | 1376256 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Alleniamo il modello\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# usato per aggiornare la sua politica interna.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m total_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000000\u001b[39m  \u001b[38;5;66;03m# Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/policies.py:655\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 655\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[1;32m    657\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:89\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode()\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:183\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Reparametrization trick to pass gradients\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/distributions/normal.py:77\u001b[0m, in \u001b[0;36mNormal.rsample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrsample\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample_shape: _size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize()) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     76\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape)\n\u001b[0;32m---> 77\u001b[0m     eps \u001b[38;5;241m=\u001b[39m \u001b[43m_standard_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/torch/distributions/utils.py:65\u001b[0m, in \u001b[0;36m_standard_normal\u001b[0;34m(shape, dtype, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# [JIT WORKAROUND] lack of support for .normal_()\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m     62\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros(shape, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m     63\u001b[0m         torch\u001b[38;5;241m.\u001b[39mones(shape, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m     64\u001b[0m     )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 2000000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "# def evaluate_policy(env, policy, episodes=500):\n",
    "#     \"\"\"\n",
    "#     Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "#     Parametri:\n",
    "#     - env: L'ambiente di simulazione.\n",
    "#     - policy: La policy addestrata da valutare.\n",
    "#     - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "#     Ritorna:\n",
    "#     - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "#     \"\"\"\n",
    "#     total_rewards = []\n",
    "#     for _ in range(episodes):\n",
    "#         obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "#             obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "#             total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "#         total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "#     return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]\n",
    "            obs, rewards, done, infos = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "        total_rewards.extend(episode_rewards)\n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    # std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Valutazione dopo l'addestramento\u001b[39;00m\n\u001b[1;32m      5\u001b[0m mean_reward_trained, std_reward_trained \u001b[38;5;241m=\u001b[39m evaluate_policy(model, env, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)  \u001b[38;5;66;03m# Valuta la policy addestrata\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m mean_reward_random, std_reward_random \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_random_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Valuta la policy casuale\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Stampa dei risultati\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained Policy: Mean Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward_trained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 45\u001b[0m, in \u001b[0;36mevaluate_random_policy\u001b[0;34m(env, episodes)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(done):\n\u001b[1;32m     44\u001b[0m     actions \u001b[38;5;241m=\u001b[39m [env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mnum_envs)]\n\u001b[0;32m---> 45\u001b[0m     obs, rewards, done, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     episode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     47\u001b[0m total_rewards\u001b[38;5;241m.\u001b[39mextend(episode_rewards)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Step the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    :param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    :return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_wait()\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:370\u001b[0m, in \u001b[0;36mVecEnvWrapper.step_async\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:125\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_async\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m remote, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes, actions):\n\u001b[0;32m--> 125\u001b[0m         \u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.training = False # Setta l'environment in modalità di valutazione\n",
    "env.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello.\n",
    "\n",
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(model, env, n_eval_episodes=500)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "# labels = ['Random Policy', 'Trained Policy']\n",
    "# means = [mean_reward_random, mean_reward_trained]\n",
    "# stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "# plt.ylabel('Mean Episodic Reward')\n",
    "# plt.title('Policy Comparison')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
