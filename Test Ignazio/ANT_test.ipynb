{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag\n",
    "        # Tensorbaord will find & display metrics from the `SCALARS` tab\n",
    "        metric_dict = {\n",
    "            #\"rollout/ep_len_mean\": 0,\n",
    "            #\"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (PPO_4) -> {'reset_noise_scale': 0.16872520546404454, 'forward_reward_weight': 0.569165596187308, 'ctrl_cost_weight': 0.15369909636721105, 'healthy_reward': 1.1651483169773327, 'learning_rate': 0.00025118614395972893, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9900195327210904, 'gae_lambda': 0.8063306496367846, 'clip_range': 0.1411162146550987, 'ent_coef': 0.006226601057899701, 'variance_penalty_weight': 0.0007310600475679448}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (PPO_6) -> \n",
    "\n",
    "hp_reset_noise_scale=0.10405074414945424 # scala del rumore quando l'ambiente viene resettato \n",
    "\n",
    "hp_forward_reward_weight=0.5940601384640877 # peso del reward per il movimento in avanti\n",
    "\n",
    "hp_ctrl_cost_weight=0.14771040407991193 # peso del reward per il controllo\n",
    "\n",
    "hp_healthy_reward =1.4039427670916238 # reward per la salute\n",
    "\n",
    "\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "\n",
    "hp_learning_rate=0.00014010166026390974           # Tasso di apprendimento: controlla la velocitÃ  con cui il modello apprende aggiornando i pesi\n",
    "\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "\n",
    "hp_gamma=0.9974446213345484      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "\n",
    "hp_gae_lambda=0.8025419607496327              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "\n",
    "hp_clip_range=0.16218657788555388               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "\n",
    "hp_ent_coef=0.00017603718662988996                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_7 {'reset_noise_scale': 0.16260110616284057, 'forward_reward_weight': 0.6594701821995568, 'ctrl_cost_weight': 0.13678469591501632, 'healthy_reward': 1.4384387807236847, 'contact_cost_weight': 0.0007721118603343064, 'healthy_z_lower': 0.11270460095319094, 'healthy_z_upper': 1.1367622027728483, 'contact_force_min': -0.8099290655891269, 'contact_force_max': 0.7683440461793597, 'learning_rate': 0.0001620494220647337, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9960403688730154, 'gae_lambda': 0.8519055821923104, 'clip_range': 0.28172421812629234, 'ent_coef': 0.015960745859518122, 'variance_penalty_weight': 0.011924537413547313}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_8 -> {'reset_noise_scale': 0.14953307712823055, 'forward_reward_weight': 0.5971580841907844, 'ctrl_cost_weight': 0.21085190913852067, 'healthy_reward': 1.3432502908397173, 'contact_cost_weight': 0.0006565424645557624, 'healthy_z_lower': 0.11576255546554826, 'healthy_z_upper': 1.0657755912005253, 'contact_force_min': -0.7947792512332761, 'contact_force_max': 0.7599774107257553, 'learning_rate': 0.0001417417141818677, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9977321276628237, 'gae_lambda': 0.8135998374897728, 'clip_range': 0.2502648636777115, 'ent_coef': 0.006686448422595028, 'variance_penalty_weight': 0.0008985044453683972}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_9 -> {'reset_noise_scale': 0.1224648700491494, 'forward_reward_weight': 1.0798217517026751, 'ctrl_cost_weight': 0.2788960190947023, 'healthy_reward': 1.4972086156641724, 'contact_cost_weight': 0.00019495257535118138, 'healthy_z_lower': 0.10525289571959973, 'healthy_z_upper': 1.1803240798353063, 'contact_force_min': -0.5187992701613672, 'contact_force_max': 0.5870857431066443, 'learning_rate': 0.000983439712869658, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9686135698396399, 'gae_lambda': 0.9145395422692033, 'clip_range': 0.37757085535729756, 'ent_coef': 0.00017055556769922042, 'std_penalty_weight': 0.28813167612676016}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_10 -> {'reset_noise_scale': 0.0811219889284557, 'forward_reward_weight': 0.794019967338759, 'ctrl_cost_weight': 0.1909084649203593, 'healthy_reward': 1.4695470159426132, 'contact_cost_weight': 0.00048075670076003045, 'healthy_z_lower': 0.19353492629665098, 'healthy_z_upper': 1.1936905952567158, 'contact_force_min': -0.5349939620294489, 'contact_force_max': 0.7307512698224117, 'learning_rate': 0.0003564760563058714, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9762294172462653, 'gae_lambda': 0.9261117656360015, 'clip_range': 0.3320669028429513, 'ent_coef': 0.0026780011357637598, 'std_penalty_weight': 0.20050838533111062}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_11 -> {'reset_noise_scale': 0.07145476067020312, 'forward_reward_weight': 1.545247241271003, 'ctrl_cost_weight': 0.5888754350371963, 'healthy_reward': 1.8961539637830271, 'contact_cost_weight': 0.000378389770529098, 'healthy_z_lower': 0.12761016035917702, 'healthy_z_upper': 1.0403641157003203, 'contact_force_min': -0.8542989596897922, 'contact_force_max': 0.948423208060974, 'learning_rate': 5.070148561650504e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.970695023374151, 'gae_lambda': 0.9257548804331505, 'clip_range': 0.17667299134062348, 'ent_coef': 0.15117794816431884}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_12 -> [I 2025-02-13 21:52:57,075] Trial 142 finished with value: 2147.46312115008 and parameters: {'reset_noise_scale': 0.05057132050677559, 'forward_reward_weight': 1.5242065847007638, 'ctrl_cost_weight': 1.0518615515373424, 'healthy_reward': 1.8906418906546967, 'contact_cost_weight': 0.00017255813970342036, 'healthy_z_lower': 0.171480519365306, 'healthy_z_upper': 0.9415199367799989, 'contact_force_min': -0.8951041717141913, 'contact_force_max': 0.9341249818927484, 'learning_rate': 0.0006673200204296133, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9635194700970308, 'gae_lambda': 0.9254828522668759, 'clip_range': 0.2273207023257754, 'ent_coef': 0.0003757232557716752}. Best is trial 142 with value: 2147.46312115008.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_13 -> [I 2025-02-13 23:00:25,932] Trial 220 finished with value: 2149.324657643946 and parameters: {'reset_noise_scale': 0.190758010402181, 'forward_reward_weight': 1.5237537263020415, 'ctrl_cost_weight': 0.8907565868390414, 'healthy_reward': 1.8601364542985845, 'contact_cost_weight': 0.0001613795517276871, 'healthy_z_lower': 0.15522435940425033, 'healthy_z_upper': 0.9593914101020591, 'contact_force_min': -0.9042700990109689, 'contact_force_max': 0.6524756726457444, 'learning_rate': 2.9911610221111873e-05, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9623881975118631, 'gae_lambda': 0.9240000421659839, 'clip_range': 0.22260259988295936, 'ent_coef': 0.025157413751276267}. Best is trial 220 with value: 2149.324657643946.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PPO_14 -> [I 2025-02-14 13:03:50,697] Trial 84 finished with value: 2515.3490927833504 and parameters: {'reset_noise_scale': 0.05643887152139423, 'forward_reward_weight': 1.6571735389009492, 'ctrl_cost_weight': 1.2459307943421456, 'healthy_reward': 2.1887877456647273, 'contact_cost_weight': 3.359821608235945e-05, 'healthy_z_lower': 0.2931915606319033, 'healthy_z_upper': 1.1481704672437232, 'contact_force_min': -1.0300470905265697, 'contact_force_max': 1.0893204485382713, 'learning_rate': 4.741460073775156e-05, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9544796539113963, 'gae_lambda': 0.9688396129147425, 'clip_range': 0.15375460930777995, 'ent_coef': 0.04630593810555868}. Best is trial 84 with value: 2515.3490927833504.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST PPO_15 -> [I 2025-02-15 10:47:28,629] Trial 196 finished with value: 2681.5108748545686 and parameters: {'reset_noise_scale': 0.06976748570068636, 'forward_reward_weight': 1.7031840270142826, 'ctrl_cost_weight': 1.3736559851030032, 'healthy_reward': 2.370527088282008, 'contact_cost_weight': 5.099569789843523e-05, 'healthy_z_lower': 0.27002630801618377, 'healthy_z_upper': 1.2006270382609852, 'contact_force_min': -1.1373782536958372, 'contact_force_max': 0.9183153315908629, 'learning_rate': 0.00036529761855305995, 'n_steps': 4096, 'batch_size': 1024, 'gamma': 0.9373806202176476, 'gae_lambda': 0.9539161953969619, 'clip_range': 0.10327074757515992, 'ent_coef': 0.013922688117154487}. Best is trial 196 with value: 2681.5108748545686.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_16 -> [I 2025-02-15 15:11:08,399] Trial 119 finished with value: 2805.869046230279 and parameters: {'reset_noise_scale': 0.10340640550745006, 'forward_reward_weight': 1.7316798226755399, 'ctrl_cost_weight': 1.3431791463786378, 'healthy_reward': 2.4675748630080876, 'contact_cost_weight': 9.874201288446414e-05, 'healthy_z_lower': 0.3483299533071902, 'healthy_z_upper': 1.2738868349210701, 'contact_force_min': -1.1329009828313024, 'contact_force_max': 0.9421168936981189, 'learning_rate': 0.0008288719683865925, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9591029149494671, 'gae_lambda': 0.9541845003001236, 'clip_range': 0.10331326443472709, 'ent_coef': 0.02661587350926783}. Best is trial 119 with value: 2805.869046230279.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale= 0.09578929233536823 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight = 1.7429234815641454 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.2489277534682572 # peso del reward per il controllo\n",
    "hp_healthy_reward = 2.465857784440316 # reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 4.3589765806323675e-05\n",
    "healthy_z = (0.3413951260140689, 1.3057167416301527)\n",
    "contact_force = (-1.192846651406859, 1.0724279378513066)\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=0.0008066765307245294          # Tasso di apprendimento: controlla la velocitÃ  con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=12288                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=512                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.956833700373414      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.9782417047710235              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.14429906555607105               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.02251015458483032                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrÃ  effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 Ã¨ lâambiente piÃ¹ recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z,\n",
    "                    contact_force_range=contact_force,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire piÃ¹ istanze dell'ambiente come se fossero una singola entitÃ .\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "#env = DummyVecEnv([make_env])  \n",
    "\n",
    "\n",
    "NUM_ENVS=8\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def linear_schedule(initial_value):\n",
    "#     def func(progress_remaining):\n",
    "#         return progress_remaining * initial_value\n",
    "#     return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=hp_learning_rate,  # usa lo scheduler lineare\n",
    "    n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    #seed=42,                        # Seed per la riproducibilitÃ \n",
    "    verbose=1,                    # Livello di verbositÃ : 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=50000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1964  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 98304 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1614        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005681498 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | -3.68       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.239      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00274    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0694      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1536         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059472867 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.144        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | 0.393        |\n",
      "|    learning_rate        | 0.000807     |\n",
      "|    loss                 | -0.24        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00611     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0499       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1513        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 259         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006706238 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.245      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0482      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=32.40 +/- 0.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 32.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007278113 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.244      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0473      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1485   |\n",
      "|    iterations      | 5      |\n",
      "|    time_elapsed    | 330    |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1475         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 399          |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076271393 |\n",
      "|    clip_fraction        | 0.17         |\n",
      "|    clip_range           | 0.144        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | 0.483        |\n",
      "|    learning_rate        | 0.000807     |\n",
      "|    loss                 | -0.24        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00803     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0487       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1475        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 466         |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008129639 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.242      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0476      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1472        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008326355 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.237      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0083     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0469      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=33.34 +/- 1.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 33.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008444037 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.244      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0485      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1459   |\n",
      "|    iterations      | 9      |\n",
      "|    time_elapsed    | 606    |\n",
      "|    total_timesteps | 884736 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1453        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008947124 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.244      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00828    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0496      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1451        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 744         |\n",
      "|    total_timesteps      | 1081344     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009121558 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.244      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00865    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.0488      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1452        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 812         |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009164092 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.234      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00814    |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=33.93 +/- 3.81\n",
      "Episode length: 942.80 +/- 114.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 943         |\n",
      "|    mean_reward          | 33.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009484522 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.243      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00839    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.0481      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1444    |\n",
      "|    iterations      | 13      |\n",
      "|    time_elapsed    | 884     |\n",
      "|    total_timesteps | 1277952 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1439        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 955         |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009676122 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.485       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.231      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1441        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1023        |\n",
      "|    total_timesteps      | 1474560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009798319 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.233      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.0508      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1439        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 1092        |\n",
      "|    total_timesteps      | 1572864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010150253 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.243      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.0489      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=28.48 +/- 11.92\n",
      "Episode length: 820.40 +/- 359.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 820         |\n",
      "|    mean_reward          | 28.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009930742 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.237      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00834    |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.05        |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1434    |\n",
      "|    iterations      | 17      |\n",
      "|    time_elapsed    | 1165    |\n",
      "|    total_timesteps | 1671168 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1429        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 1237        |\n",
      "|    total_timesteps      | 1769472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010412986 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.239      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00832    |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.0508      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1432        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 1303        |\n",
      "|    total_timesteps      | 1867776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010296447 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.245      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0086     |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 0.0488      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1451        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 1354        |\n",
      "|    total_timesteps      | 1966080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010686961 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | 0.49        |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.0495      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=4.76 +/- 3.67\n",
      "Episode length: 119.00 +/- 101.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 119         |\n",
      "|    mean_reward          | 4.76        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010716044 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.236      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00862    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.0515      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1449    |\n",
      "|    iterations      | 21      |\n",
      "|    time_elapsed    | 1424    |\n",
      "|    total_timesteps | 2064384 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1447        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 1493        |\n",
      "|    total_timesteps      | 2162688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010979979 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.24       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.0492      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1445        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 1564        |\n",
      "|    total_timesteps      | 2260992     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011040978 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.237      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.0513      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1440        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 1637        |\n",
      "|    total_timesteps      | 2359296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011079364 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.236      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00834    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.0509      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=8.46 +/- 8.99\n",
      "Episode length: 190.20 +/- 236.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 190         |\n",
      "|    mean_reward          | 8.46        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011531449 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.24       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00836    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.0532      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1437    |\n",
      "|    iterations      | 25      |\n",
      "|    time_elapsed    | 1709    |\n",
      "|    total_timesteps | 2457600 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1437        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 1778        |\n",
      "|    total_timesteps      | 2555904     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011470157 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.243      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.0526      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1435        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1848        |\n",
      "|    total_timesteps      | 2654208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011200239 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.236      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00838    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.0505      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1432        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1921        |\n",
      "|    total_timesteps      | 2752512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011455382 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.242      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00798    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.0528      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=14.58 +/- 13.21\n",
      "Episode length: 401.60 +/- 386.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 402         |\n",
      "|    mean_reward          | 14.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012012761 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.237      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00798    |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.0534      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1427    |\n",
      "|    iterations      | 29      |\n",
      "|    time_elapsed    | 1996    |\n",
      "|    total_timesteps | 2850816 |\n",
      "--------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1424         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 2069         |\n",
      "|    total_timesteps      | 2949120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118258195 |\n",
      "|    clip_fraction        | 0.252        |\n",
      "|    clip_range           | 0.144        |\n",
      "|    entropy_loss         | -11.2        |\n",
      "|    explained_variance   | 0.502        |\n",
      "|    learning_rate        | 0.000807     |\n",
      "|    loss                 | -0.242       |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00836     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.0547       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1423        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 2140        |\n",
      "|    total_timesteps      | 3047424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011699316 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.233      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00831    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.0552      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1420        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 2214        |\n",
      "|    total_timesteps      | 3145728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011929023 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00877    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.0566      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=6.37 +/- 3.23\n",
      "Episode length: 123.20 +/- 71.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 123         |\n",
      "|    mean_reward          | 6.37        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012089849 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.226      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00839    |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.0553      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1418    |\n",
      "|    iterations      | 33      |\n",
      "|    time_elapsed    | 2287    |\n",
      "|    total_timesteps | 3244032 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1415        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 2361        |\n",
      "|    total_timesteps      | 3342336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012244553 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.234      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00867    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 0.0555      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1412        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 2435        |\n",
      "|    total_timesteps      | 3440640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012058402 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.241      |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 0.0554      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1410        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 2508        |\n",
      "|    total_timesteps      | 3538944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012332074 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.492       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.225      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00823    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 0.0561      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=4.70 +/- 2.72\n",
      "Episode length: 131.80 +/- 86.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 132         |\n",
      "|    mean_reward          | 4.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012551923 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.233      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00808    |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 0.0559      |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 1418    |\n",
      "|    iterations      | 37      |\n",
      "|    time_elapsed    | 2564    |\n",
      "|    total_timesteps | 3637248 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1418        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 2632        |\n",
      "|    total_timesteps      | 3735552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012480875 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.23       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00837    |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 0.0573      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1418        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 2702        |\n",
      "|    total_timesteps      | 3833856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012957879 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.144       |\n",
      "|    entropy_loss         | -11.2       |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.000807    |\n",
      "|    loss                 | -0.23       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00819    |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 0.0581      |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirÃ  durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 5000000  # Puoi aumentare questo valore per permettere al modello di acquisire piÃ¹ esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "# def evaluate_policy(env, policy, episodes=500):\n",
    "#     \"\"\"\n",
    "#     Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "#     Parametri:\n",
    "#     - env: L'ambiente di simulazione.\n",
    "#     - policy: La policy addestrata da valutare.\n",
    "#     - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "#     Ritorna:\n",
    "#     - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "#     \"\"\"\n",
    "#     total_rewards = []\n",
    "#     for _ in range(episodes):\n",
    "#         obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "#             obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "#             total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "#         total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "#     return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]\n",
    "            obs, rewards, done, infos = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "        total_rewards.extend(episode_rewards)\n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    # std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.training = False # Setta l'environment in modalitÃ  di valutazione\n",
    "# env.norm_reward = False # Disabilita la normalizzazione della reward. Questo Ã¨ importante per valutare correttamente il modello.\n",
    "\n",
    "# # Valutazione dopo l'addestramento\n",
    "# mean_reward_trained, std_reward_trained = evaluate_policy(model, env, n_eval_episodes=500)  # Valuta la policy addestrata\n",
    "# mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# # Stampa dei risultati\n",
    "# print(f\"Trained Policy: Mean Reward: {mean_reward_trained}\")\n",
    "# print(f\"Random Policy: Mean Reward: {mean_reward_random}\")\n",
    "\n",
    "# # Creazione del grafico di confronto\n",
    "# # labels = ['Random Policy', 'Trained Policy']\n",
    "# # means = [mean_reward_random, mean_reward_trained]\n",
    "# # stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "# # plt.figure(figsize=(8, 5))\n",
    "# # plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "# # plt.ylabel('Mean Episodic Reward')\n",
    "# # plt.title('Policy Comparison')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
