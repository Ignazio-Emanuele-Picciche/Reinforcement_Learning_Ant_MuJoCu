{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag\n",
    "        # Tensorbaord will find & display metrics from the `SCALARS` tab\n",
    "        metric_dict = {\n",
    "            #\"rollout/ep_len_mean\": 0,\n",
    "            #\"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_1 (PPO_4) -> {'reset_noise_scale': 0.16872520546404454, 'forward_reward_weight': 0.569165596187308, 'ctrl_cost_weight': 0.15369909636721105, 'healthy_reward': 1.1651483169773327, 'learning_rate': 0.00025118614395972893, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9900195327210904, 'gae_lambda': 0.8063306496367846, 'clip_range': 0.1411162146550987, 'ent_coef': 0.006226601057899701, 'variance_penalty_weight': 0.0007310600475679448}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_2 (PPO_6) -> \n",
    "\n",
    "hp_reset_noise_scale=0.10405074414945424 # scala del rumore quando l'ambiente viene resettato \n",
    "\n",
    "hp_forward_reward_weight=0.5940601384640877 # peso del reward per il movimento in avanti\n",
    "\n",
    "hp_ctrl_cost_weight=0.14771040407991193 # peso del reward per il controllo\n",
    "\n",
    "hp_healthy_reward =1.4039427670916238 # reward per la salute\n",
    "\n",
    "\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "\n",
    "hp_learning_rate=0.00014010166026390974           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "\n",
    "hp_gamma=0.9974446213345484      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "\n",
    "hp_gae_lambda=0.8025419607496327              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "\n",
    "hp_clip_range=0.16218657788555388               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "\n",
    "hp_ent_coef=0.00017603718662988996                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_7 {'reset_noise_scale': 0.16260110616284057, 'forward_reward_weight': 0.6594701821995568, 'ctrl_cost_weight': 0.13678469591501632, 'healthy_reward': 1.4384387807236847, 'contact_cost_weight': 0.0007721118603343064, 'healthy_z_lower': 0.11270460095319094, 'healthy_z_upper': 1.1367622027728483, 'contact_force_min': -0.8099290655891269, 'contact_force_max': 0.7683440461793597, 'learning_rate': 0.0001620494220647337, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9960403688730154, 'gae_lambda': 0.8519055821923104, 'clip_range': 0.28172421812629234, 'ent_coef': 0.015960745859518122, 'variance_penalty_weight': 0.011924537413547313}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_8 -> {'reset_noise_scale': 0.14953307712823055, 'forward_reward_weight': 0.5971580841907844, 'ctrl_cost_weight': 0.21085190913852067, 'healthy_reward': 1.3432502908397173, 'contact_cost_weight': 0.0006565424645557624, 'healthy_z_lower': 0.11576255546554826, 'healthy_z_upper': 1.0657755912005253, 'contact_force_min': -0.7947792512332761, 'contact_force_max': 0.7599774107257553, 'learning_rate': 0.0001417417141818677, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9977321276628237, 'gae_lambda': 0.8135998374897728, 'clip_range': 0.2502648636777115, 'ent_coef': 0.006686448422595028, 'variance_penalty_weight': 0.0008985044453683972}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_9 -> {'reset_noise_scale': 0.1224648700491494, 'forward_reward_weight': 1.0798217517026751, 'ctrl_cost_weight': 0.2788960190947023, 'healthy_reward': 1.4972086156641724, 'contact_cost_weight': 0.00019495257535118138, 'healthy_z_lower': 0.10525289571959973, 'healthy_z_upper': 1.1803240798353063, 'contact_force_min': -0.5187992701613672, 'contact_force_max': 0.5870857431066443, 'learning_rate': 0.000983439712869658, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9686135698396399, 'gae_lambda': 0.9145395422692033, 'clip_range': 0.37757085535729756, 'ent_coef': 0.00017055556769922042, 'std_penalty_weight': 0.28813167612676016}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_10 -> {'reset_noise_scale': 0.0811219889284557, 'forward_reward_weight': 0.794019967338759, 'ctrl_cost_weight': 0.1909084649203593, 'healthy_reward': 1.4695470159426132, 'contact_cost_weight': 0.00048075670076003045, 'healthy_z_lower': 0.19353492629665098, 'healthy_z_upper': 1.1936905952567158, 'contact_force_min': -0.5349939620294489, 'contact_force_max': 0.7307512698224117, 'learning_rate': 0.0003564760563058714, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9762294172462653, 'gae_lambda': 0.9261117656360015, 'clip_range': 0.3320669028429513, 'ent_coef': 0.0026780011357637598, 'std_penalty_weight': 0.20050838533111062}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_11 -> {'reset_noise_scale': 0.19063551485460006, 'forward_reward_weight': 1.1874610350303911, 'ctrl_cost_weight': 0.7635871233517055, 'healthy_reward': 0.9167550523607197, 'contact_cost_weight': 0.00031549876314330944, 'healthy_z_lower': 0.20679790820057092, 'healthy_z_upper': 0.9706826850531042, 'contact_force_min': -0.5166260803523359, 'contact_force_max': 0.8952287392588767, 'learning_rate': 2.9390505260778442e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.9776577251734287, 'gae_lambda': 0.9466444608224389, 'clip_range': 0.42321693449630365, 'ent_coef': 0.08075447939975819}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale= 0.19063551485460006 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight = 1.1874610350303911 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 0.7635871233517055 # peso del reward per il controllo\n",
    "hp_healthy_reward = 0.9167550523607197 # reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 0.00031549876314330944\n",
    "healthy_z = (0.20679790820057092, 0.9706826850531042)\n",
    "contact_force = (-0.5166260803523359, 0.8952287392588767)\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=2.9390505260778442e-05           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=6144                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=4096                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=6                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.9776577251734287      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.9466444608224389              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.42321693449630365               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.08075447939975819                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z,\n",
    "                    contact_force_range=contact_force,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "env = DummyVecEnv([make_env])  \n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 6144`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 2048\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=6144 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=hp_learning_rate,           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "    n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    seed=42,                        # Seed per la riproducibilità\n",
    "    verbose=1,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = DummyVecEnv([make_env])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=10000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_11\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 380  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=9.19 +/- 0.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 9.19         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.110051e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -1.94        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.674       |\n",
      "|    n_updates            | 6            |\n",
      "|    policy_gradient_loss | -0.00034     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.542        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 283   |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 308           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 59            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0471714e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -7.26         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.743        |\n",
      "|    n_updates            | 12            |\n",
      "|    policy_gradient_loss | -0.000215     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.359         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=7.50 +/- 3.76\n",
      "Episode length: 801.60 +/- 396.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 802           |\n",
      "|    mean_reward          | 7.5           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2654498e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -6.88         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.742        |\n",
      "|    n_updates            | 18            |\n",
      "|    policy_gradient_loss | -0.000144     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.351         |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 299   |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=9.39 +/- 0.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 9.39         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.065601e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -6.63        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.753       |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.000123    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.343        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 289   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 106   |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 300           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 122           |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2507523e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -7.21         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.764        |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00015      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.324         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=9.68 +/- 0.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 9.68          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4149198e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -8.02         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.77         |\n",
      "|    n_updates            | 36            |\n",
      "|    policy_gradient_loss | -0.00016      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.306         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 293   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 146   |\n",
      "|    total_timesteps | 43008 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.00743e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.423       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | -6.74       |\n",
      "|    learning_rate        | 2.94e-05    |\n",
      "|    loss                 | -0.783      |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | -0.000174   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.286       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=9.96 +/- 0.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 9.96          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 50000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7115824e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -7.14         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.776        |\n",
      "|    n_updates            | 48            |\n",
      "|    policy_gradient_loss | -0.000156     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.291         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 295   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 187   |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=10.03 +/- 0.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 10            |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6400743e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -6.85         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.781        |\n",
      "|    n_updates            | 54            |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.279         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 290   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 211   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 295           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 228           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6657148e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -5.55         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.781        |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000167     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.28          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=10.14 +/- 0.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.009242e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -5.12        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.79        |\n",
      "|    n_updates            | 66           |\n",
      "|    policy_gradient_loss | -0.000186    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.268        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 293   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 251   |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 297           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 268           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6207566e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -5.96         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.797        |\n",
      "|    n_updates            | 72            |\n",
      "|    policy_gradient_loss | -0.000166     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.257         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=10.23 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.819114e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -4.73        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.805       |\n",
      "|    n_updates            | 78           |\n",
      "|    policy_gradient_loss | -0.000206    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.242        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 294   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 292   |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=10.31 +/- 0.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.513051e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -4.66        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.808       |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -0.00019     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.231        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 292   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 314   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 296          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 331          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.232832e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -6.6         |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.795       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00018     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.248        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=10.31 +/- 0.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.646841e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -4.99        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.802       |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -0.000155    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.235        |\n",
      "------------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 294    |\n",
      "|    iterations      | 17     |\n",
      "|    time_elapsed    | 354    |\n",
      "|    total_timesteps | 104448 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=10.40 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 10.4          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 110000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8487124e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -3.98         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.814        |\n",
      "|    n_updates            | 102           |\n",
      "|    policy_gradient_loss | -0.000173     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.218         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 292    |\n",
      "|    iterations      | 18     |\n",
      "|    time_elapsed    | 378    |\n",
      "|    total_timesteps | 110592 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 295          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 394          |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.850459e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -4.28        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.819       |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.00019     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.211        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=10.42 +/- 0.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.4         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.650268e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -5.29        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.817       |\n",
      "|    n_updates            | 114          |\n",
      "|    policy_gradient_loss | -0.000147    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.217        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 293    |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 418    |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 297           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 433           |\n",
      "|    total_timesteps      | 129024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6092752e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -3.71         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.822        |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.000162     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.197         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=10.49 +/- 0.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.922185e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -4.25        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.828       |\n",
      "|    n_updates            | 126          |\n",
      "|    policy_gradient_loss | -0.000165    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.201        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 295    |\n",
      "|    iterations      | 22     |\n",
      "|    time_elapsed    | 457    |\n",
      "|    total_timesteps | 135168 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=10.51 +/- 0.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 10.5          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 140000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9545266e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -4.12         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.826        |\n",
      "|    n_updates            | 132           |\n",
      "|    policy_gradient_loss | -0.000183     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.201         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 294    |\n",
      "|    iterations      | 23     |\n",
      "|    time_elapsed    | 480    |\n",
      "|    total_timesteps | 141312 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 296           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 496           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1959793e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -3.33         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.83         |\n",
      "|    n_updates            | 138           |\n",
      "|    policy_gradient_loss | -0.000173     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.19          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=10.51 +/- 0.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1e+03       |\n",
      "|    mean_reward          | 10.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.18305e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.423       |\n",
      "|    entropy_loss         | -11.4       |\n",
      "|    explained_variance   | -3.74       |\n",
      "|    learning_rate        | 2.94e-05    |\n",
      "|    loss                 | -0.828      |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.000185   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 295    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 519    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 298          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 536          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.085625e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -4.06        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.834       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000176    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.186        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=10.58 +/- 0.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.171452e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -3.17        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.836       |\n",
      "|    n_updates            | 156          |\n",
      "|    policy_gradient_loss | -0.00017     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.171        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 296    |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 559    |\n",
      "|    total_timesteps | 165888 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=10.58 +/- 0.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 10.6          |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 170000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8645812e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -3.64         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.841        |\n",
      "|    n_updates            | 162           |\n",
      "|    policy_gradient_loss | -0.000155     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.176         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 295    |\n",
      "|    iterations      | 28     |\n",
      "|    time_elapsed    | 582    |\n",
      "|    total_timesteps | 172032 |\n",
      "-------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 297           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 598           |\n",
      "|    total_timesteps      | 178176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1349704e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.423         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -2.92         |\n",
      "|    learning_rate        | 2.94e-05      |\n",
      "|    loss                 | -0.846        |\n",
      "|    n_updates            | 168           |\n",
      "|    policy_gradient_loss | -0.000171     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.16          |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=10.56 +/- 0.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1e+03        |\n",
      "|    mean_reward          | 10.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 180000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.852438e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.423        |\n",
      "|    entropy_loss         | -11.4        |\n",
      "|    explained_variance   | -3.23        |\n",
      "|    learning_rate        | 2.94e-05     |\n",
      "|    loss                 | -0.846       |\n",
      "|    n_updates            | 174          |\n",
      "|    policy_gradient_loss | -0.000232    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.167        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 1000000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_policy(env, policy, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - policy: La policy addestrata da valutare.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Genera un'azione casuale\n",
    "            obs, reward, done, _ = env.step(np.array([action]))  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Policy: Mean Reward: 36.27058029174805, Std: 11.181402206420898\n",
      "Random Policy: Mean Reward: 14.548761367797852, Std: 12.113778114318848\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(env, model)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}, Std: {std_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}, Std: {std_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
