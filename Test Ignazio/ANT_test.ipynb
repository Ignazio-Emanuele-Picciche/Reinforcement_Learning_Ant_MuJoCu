{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=0.2282706739101626, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=0.09314040045482441, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=0.028140178122103423, # peso del reward per il controllo\n",
    "                    healthy_reward =0.9926479631637423, # reward per la salute\n",
    "                    render_mode='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "env = DummyVecEnv([make_env])  \n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=0.0008676828845312949,           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "    n_steps=4096,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=64,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=100,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=0.9328230070576791,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=0.95,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=0.2,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=0.0,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    verbose=1,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 410  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 99        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 81        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3490763 |\n",
      "|    clip_fraction        | 0.742     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -11.2     |\n",
      "|    explained_variance   | -3.21     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.16     |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -0.125    |\n",
      "|    std                  | 0.974     |\n",
      "|    value_loss           | 0.0558    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35547638 |\n",
      "|    clip_fraction        | 0.808      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11        |\n",
      "|    explained_variance   | -12.7      |\n",
      "|    learning_rate        | 0.000868   |\n",
      "|    loss                 | -0.131     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.125     |\n",
      "|    std                  | 0.948      |\n",
      "|    value_loss           | 0.00651    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 222        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48548216 |\n",
      "|    clip_fraction        | 0.849      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.7      |\n",
      "|    explained_variance   | -1.39      |\n",
      "|    learning_rate        | 0.000868   |\n",
      "|    loss                 | -0.12      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.132     |\n",
      "|    std                  | 0.915      |\n",
      "|    value_loss           | 0.00208    |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 295       |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6561541 |\n",
      "|    clip_fraction        | 0.868     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -10.4     |\n",
      "|    explained_variance   | -0.446    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -0.135    |\n",
      "|    std                  | 0.881     |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 67         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88240063 |\n",
      "|    clip_fraction        | 0.885      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.1      |\n",
      "|    explained_variance   | -1.03      |\n",
      "|    learning_rate        | 0.000868   |\n",
      "|    loss                 | -0.146     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.133     |\n",
      "|    std                  | 0.85       |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 66        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 433       |\n",
      "|    total_timesteps      | 28672     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8798077 |\n",
      "|    clip_fraction        | 0.866     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.83     |\n",
      "|    explained_variance   | 0.215     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 600       |\n",
      "|    policy_gradient_loss | -0.119    |\n",
      "|    std                  | 0.818     |\n",
      "|    value_loss           | 0.000556  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 64        |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 505       |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9534037 |\n",
      "|    clip_fraction        | 0.886     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.55     |\n",
      "|    explained_variance   | 0.0751    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.142    |\n",
      "|    n_updates            | 700       |\n",
      "|    policy_gradient_loss | -0.124    |\n",
      "|    std                  | 0.795     |\n",
      "|    value_loss           | 0.000702  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 63        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 580       |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0497825 |\n",
      "|    clip_fraction        | 0.885     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.23     |\n",
      "|    explained_variance   | 0.0742    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 800       |\n",
      "|    policy_gradient_loss | -0.125    |\n",
      "|    std                  | 0.761     |\n",
      "|    value_loss           | 0.00077   |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 62       |\n",
      "|    iterations           | 10       |\n",
      "|    time_elapsed         | 651      |\n",
      "|    total_timesteps      | 40960    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.115773 |\n",
      "|    clip_fraction        | 0.89     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -8.93    |\n",
      "|    explained_variance   | -0.0355  |\n",
      "|    learning_rate        | 0.000868 |\n",
      "|    loss                 | -0.144   |\n",
      "|    n_updates            | 900      |\n",
      "|    policy_gradient_loss | -0.126   |\n",
      "|    std                  | 0.739    |\n",
      "|    value_loss           | 0.000955 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 62        |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 721       |\n",
      "|    total_timesteps      | 45056     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7968566 |\n",
      "|    clip_fraction        | 0.897     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.71     |\n",
      "|    explained_variance   | -0.0566   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 1000      |\n",
      "|    policy_gradient_loss | -0.125    |\n",
      "|    std                  | 0.714     |\n",
      "|    value_loss           | 0.000771  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 62        |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 791       |\n",
      "|    total_timesteps      | 49152     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4062843 |\n",
      "|    clip_fraction        | 0.903     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.37     |\n",
      "|    explained_variance   | -0.0727   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 1100      |\n",
      "|    policy_gradient_loss | -0.123    |\n",
      "|    std                  | 0.685     |\n",
      "|    value_loss           | 0.000531  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 860       |\n",
      "|    total_timesteps      | 53248     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4611607 |\n",
      "|    clip_fraction        | 0.89      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.06     |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 1200      |\n",
      "|    policy_gradient_loss | -0.114    |\n",
      "|    std                  | 0.659     |\n",
      "|    value_loss           | 0.00072   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 930       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7562475 |\n",
      "|    clip_fraction        | 0.897     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.73     |\n",
      "|    explained_variance   | -0.0102   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 1300      |\n",
      "|    policy_gradient_loss | -0.113    |\n",
      "|    std                  | 0.638     |\n",
      "|    value_loss           | 0.000599  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 1000      |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3428221 |\n",
      "|    clip_fraction        | 0.886     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.54     |\n",
      "|    explained_variance   | -0.24     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0992   |\n",
      "|    std                  | 0.621     |\n",
      "|    value_loss           | 0.0003    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 1069      |\n",
      "|    total_timesteps      | 65536     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4419641 |\n",
      "|    clip_fraction        | 0.89      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.38     |\n",
      "|    explained_variance   | 0.0508    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 1500      |\n",
      "|    policy_gradient_loss | -0.0976   |\n",
      "|    std                  | 0.613     |\n",
      "|    value_loss           | 0.000357  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 1138      |\n",
      "|    total_timesteps      | 69632     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4249127 |\n",
      "|    clip_fraction        | 0.887     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.17     |\n",
      "|    explained_variance   | 0.208     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 1600      |\n",
      "|    policy_gradient_loss | -0.107    |\n",
      "|    std                  | 0.594     |\n",
      "|    value_loss           | 0.000614  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 1208      |\n",
      "|    total_timesteps      | 73728     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5083287 |\n",
      "|    clip_fraction        | 0.889     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.94     |\n",
      "|    explained_variance   | 0.0627    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.127    |\n",
      "|    n_updates            | 1700      |\n",
      "|    policy_gradient_loss | -0.114    |\n",
      "|    std                  | 0.577     |\n",
      "|    value_loss           | 0.000837  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 1277      |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.8853574 |\n",
      "|    clip_fraction        | 0.906     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.69     |\n",
      "|    explained_variance   | -0.102    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 1800      |\n",
      "|    policy_gradient_loss | -0.117    |\n",
      "|    std                  | 0.561     |\n",
      "|    value_loss           | 0.000901  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 1346      |\n",
      "|    total_timesteps      | 81920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6963869 |\n",
      "|    clip_fraction        | 0.898     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.41     |\n",
      "|    explained_variance   | 0.0872    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.11     |\n",
      "|    std                  | 0.542     |\n",
      "|    value_loss           | 0.000793  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 1415      |\n",
      "|    total_timesteps      | 86016     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1149035 |\n",
      "|    clip_fraction        | 0.917     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.2      |\n",
      "|    explained_variance   | -0.0848   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 2000      |\n",
      "|    policy_gradient_loss | -0.108    |\n",
      "|    std                  | 0.527     |\n",
      "|    value_loss           | 0.000638  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 1485      |\n",
      "|    total_timesteps      | 90112     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9298136 |\n",
      "|    clip_fraction        | 0.909     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.94     |\n",
      "|    explained_variance   | -0.0837   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 2100      |\n",
      "|    policy_gradient_loss | -0.111    |\n",
      "|    std                  | 0.511     |\n",
      "|    value_loss           | 0.000821  |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 60       |\n",
      "|    iterations           | 23       |\n",
      "|    time_elapsed         | 1554     |\n",
      "|    total_timesteps      | 94208    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.177987 |\n",
      "|    clip_fraction        | 0.903    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -5.69    |\n",
      "|    explained_variance   | -0.0979  |\n",
      "|    learning_rate        | 0.000868 |\n",
      "|    loss                 | -0.0886  |\n",
      "|    n_updates            | 2200     |\n",
      "|    policy_gradient_loss | -0.115   |\n",
      "|    std                  | 0.493    |\n",
      "|    value_loss           | 0.00115  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 60       |\n",
      "|    iterations           | 24       |\n",
      "|    time_elapsed         | 1623     |\n",
      "|    total_timesteps      | 98304    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.378542 |\n",
      "|    clip_fraction        | 0.918    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -5.39    |\n",
      "|    explained_variance   | -0.239   |\n",
      "|    learning_rate        | 0.000868 |\n",
      "|    loss                 | -0.106   |\n",
      "|    n_updates            | 2300     |\n",
      "|    policy_gradient_loss | -0.106   |\n",
      "|    std                  | 0.478    |\n",
      "|    value_loss           | 0.000629 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 1692      |\n",
      "|    total_timesteps      | 102400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.9681566 |\n",
      "|    clip_fraction        | 0.927     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.26     |\n",
      "|    explained_variance   | -0.0503   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    std                  | 0.469     |\n",
      "|    value_loss           | 0.000499  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x37c76fc40>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 100000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, episodes=50):\n",
    "    \"\"\"\n",
    "    Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - policy: La policy addestrata da valutare.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "\n",
    "def evaluate_random_policy(env, episodes=50):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Genera un'azione casuale\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(env, model)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}, Std: {std_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}, Std: {std_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Valutazione\n",
    "# Qui creiamo un ambiente specifico per la valutazione del modello\n",
    "#\n",
    "# L'obiettivo è osservare in tempo reale come il modello interagisce con l'ambiente.\n",
    "# Questo ambiente differisce da quello usato durante l'allenamento (che era vettorializzato e normalizzato)\n",
    "# Per la valutazione, possiamo utilizzare l'ambiente \"grezzo\" di Gymnasium con rendering.\n",
    "#\n",
    "# Parametri:\n",
    "#   - \"Ant-v5\": Nome dell'ambiente;\n",
    "#   - reset_noise_scale=0.1: Scala del rumore applicato durante il reset dell'ambiente,\n",
    "#       utile per mantenere la coerenza con le condizioni viste durante l'allenamento.\n",
    "#   - render_mode=\"human\": Abilita il rendering dell'ambiente in tempo reale, permettendoci di osservare visivamente il comportamento del modello.\n",
    "# eval_env = gym.make(\"Ant-v5\", reset_noise_scale=0.1)\n",
    "#                     #, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ricreiamo il wrapper di normalizzazione con i parametri salvati\n",
    "# \"\"\"\n",
    "# Questo snippet di codice configura un ambiente di valutazione normalizzato per un compito di reinforcement learning.\n",
    "# Esegue le seguenti operazioni:\n",
    "\n",
    "# 1. Avvolge l'ambiente originale (eval_env) in un ambiente vettorializzato utilizzando DummyVecEnv.\n",
    "#     Questo è necessario per la compatibilità con algoritmi che richiedono un'interfaccia vettorializzata.\n",
    "# 2. Carica i parametri di normalizzazione precedentemente salvati (da 'vecnormalize_Ant.pkl') nel nuovo ambiente.\n",
    "#     Ciò garantisce che le osservazioni dello stato (e potenzialmente le ricompense) siano normalizzate\n",
    "#     nello stesso modo in cui sono state normalizzate durante l'addestramento.\n",
    "# 3. Imposta l'ambiente in modalità di valutazione:\n",
    "#     - Disabilita la modalità training (eval_env.training = False) in modo che le statistiche di normalizzazione non vengano aggiornate.\n",
    "#     - Disabilita la normalizzazione della ricompensa (eval_env.norm_reward = False) perché durante la valutazione le ricompense\n",
    "#       vengono tipicamente utilizzate nella loro forma originale.\n",
    "\n",
    "# Parametri:\n",
    "# - eval_env: L'istanza dell'ambiente che deve essere avvolta e normalizzata.\n",
    "# - \"vecnormalize_Ant.pkl\": Il percorso del file contenente i parametri di normalizzazione salvati specifici per l'ambiente 'Ant'.\n",
    "#   Questi parametri regolano la scala delle osservazioni e delle ricompense dell'ambiente.\n",
    "\n",
    "# Nota:\n",
    "# Questa configurazione è essenziale durante la valutazione di un modello addestrato per garantire che il preprocessamento\n",
    "# applicato corrisponda a quello dell'addestramento, senza modificare ulteriormente i parametri di normalizzazione.\n",
    "# \"\"\"\n",
    "# eval_env = DummyVecEnv([lambda: eval_env])\n",
    "# eval_env = VecNormalize.load(\"vecnormalize_Ant.pkl\", eval_env)\n",
    "# eval_env.training = False\n",
    "# eval_env.norm_reward = False  # In valutazione di solito non normalizziamo la ricompensa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.86552584\n",
      "Ricompensa totale episodio: -0.86552584\n"
     ]
    }
   ],
   "source": [
    "# # Reset dell'ambiente di valutazione per ottenere l'osservazione iniziale.\n",
    "# obs = eval_env.reset()\n",
    "\n",
    "# # Inizializzazione della variabile che accumulerà la ricompensa totale ottenuta nell'episodio.\n",
    "# episode_reward = 0.0\n",
    "\n",
    "# # Variabile booleana per controllare se l'episodio è terminato o troncato.\n",
    "# done = False\n",
    "\n",
    "# # Ciclo per eseguire l'iterazione dell'episodio\n",
    "# while not done:\n",
    "#     # Otteniamo l'azione in modalità deterministica dal modello (questo assicura che il modello non agisca con esplorazione casuale)\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "#     # Eseguiamo l'azione nell'ambiente di valutazione utilizzando il metodo step per ottenere\n",
    "#     # - observation: l'osservazione successiva\n",
    "#     # - reward: la ricompensa ottenuta per l'azione intrapresa\n",
    "#     # - terminated: flag che indica se l'episodio è terminato normalmente\n",
    "#     # - truncated: flag che indica se l'episodio è stato interrotto (es. timeout)\n",
    "#     observation, reward, terminated, truncated = eval_env.step(action)\n",
    "    \n",
    "#     # Aggiornamento della ricompensa totale: poiché l'ambiente è vettorializzato, reward è un array\n",
    "#     episode_reward += reward[0]  \n",
    "#     print(reward[0])\n",
    "    \n",
    "#     # L'episodio termina se risulta terminato (terminated=True) oppure se è troncato (truncated=True)\n",
    "#     done = terminated or truncated\n",
    "\n",
    "# # Stampa della ricompensa totale accumulata durante l'episodio.\n",
    "# print(\"Ricompensa totale episodio:\", episode_reward)\n",
    "\n",
    "# # Chiusura dell'ambiente per liberare le risorse.\n",
    "# eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
