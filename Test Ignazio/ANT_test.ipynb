{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial (consigliati 500 trial)\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHAT con search dice che per il train vanno bene anche 1mln di timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Saves the hyperparameters and metrics at the start of the training, and logs them to TensorBoard.\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        hparam_dict = {\n",
    "            \"algorithm\": self.model.__class__.__name__,\n",
    "            \"learning rate\": self.model.learning_rate,\n",
    "            \"gamma\": self.model.gamma,\n",
    "        }\n",
    "        # define the metrics that will appear in the `HPARAMS` Tensorboard tab by referencing their tag\n",
    "        # Tensorbaord will find & display metrics from the `SCALARS` tab\n",
    "        metric_dict = {\n",
    "            #\"rollout/ep_len_mean\": 0,\n",
    "            #\"train/value_loss\": 0.0,\n",
    "        }\n",
    "        self.logger.record(\n",
    "            \"hparams\",\n",
    "            HParam(hparam_dict, metric_dict),\n",
    "            exclude=(\"stdout\", \"log\", \"json\", \"csv\"),\n",
    "        )\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_1 (PPO_4) -> {'reset_noise_scale': 0.16872520546404454, 'forward_reward_weight': 0.569165596187308, 'ctrl_cost_weight': 0.15369909636721105, 'healthy_reward': 1.1651483169773327, 'learning_rate': 0.00025118614395972893, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9900195327210904, 'gae_lambda': 0.8063306496367846, 'clip_range': 0.1411162146550987, 'ent_coef': 0.006226601057899701, 'variance_penalty_weight': 0.0007310600475679448}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST_2 (PPO_6) -> \n",
    "\n",
    "hp_reset_noise_scale=0.10405074414945424 # scala del rumore quando l'ambiente viene resettato \n",
    "\n",
    "hp_forward_reward_weight=0.5940601384640877 # peso del reward per il movimento in avanti\n",
    "\n",
    "hp_ctrl_cost_weight=0.14771040407991193 # peso del reward per il controllo\n",
    "\n",
    "hp_healthy_reward =1.4039427670916238 # reward per la salute\n",
    "\n",
    "\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "\n",
    "hp_learning_rate=0.00014010166026390974           # Tasso di apprendimento: controlla la velocitÃ  con cui il modello apprende aggiornando i pesi\n",
    "\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "\n",
    "hp_gamma=0.9974446213345484      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "\n",
    "hp_gae_lambda=0.8025419607496327              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "\n",
    "hp_clip_range=0.16218657788555388               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "\n",
    "hp_ent_coef=0.00017603718662988996                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_7 {'reset_noise_scale': 0.16260110616284057, 'forward_reward_weight': 0.6594701821995568, 'ctrl_cost_weight': 0.13678469591501632, 'healthy_reward': 1.4384387807236847, 'contact_cost_weight': 0.0007721118603343064, 'healthy_z_lower': 0.11270460095319094, 'healthy_z_upper': 1.1367622027728483, 'contact_force_min': -0.8099290655891269, 'contact_force_max': 0.7683440461793597, 'learning_rate': 0.0001620494220647337, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9960403688730154, 'gae_lambda': 0.8519055821923104, 'clip_range': 0.28172421812629234, 'ent_coef': 0.015960745859518122, 'variance_penalty_weight': 0.011924537413547313}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_8 -> {'reset_noise_scale': 0.14953307712823055, 'forward_reward_weight': 0.5971580841907844, 'ctrl_cost_weight': 0.21085190913852067, 'healthy_reward': 1.3432502908397173, 'contact_cost_weight': 0.0006565424645557624, 'healthy_z_lower': 0.11576255546554826, 'healthy_z_upper': 1.0657755912005253, 'contact_force_min': -0.7947792512332761, 'contact_force_max': 0.7599774107257553, 'learning_rate': 0.0001417417141818677, 'n_steps': 4096, 'batch_size': 64, 'gamma': 0.9977321276628237, 'gae_lambda': 0.8135998374897728, 'clip_range': 0.2502648636777115, 'ent_coef': 0.006686448422595028, 'variance_penalty_weight': 0.0008985044453683972}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_9 -> {'reset_noise_scale': 0.1224648700491494, 'forward_reward_weight': 1.0798217517026751, 'ctrl_cost_weight': 0.2788960190947023, 'healthy_reward': 1.4972086156641724, 'contact_cost_weight': 0.00019495257535118138, 'healthy_z_lower': 0.10525289571959973, 'healthy_z_upper': 1.1803240798353063, 'contact_force_min': -0.5187992701613672, 'contact_force_max': 0.5870857431066443, 'learning_rate': 0.000983439712869658, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9686135698396399, 'gae_lambda': 0.9145395422692033, 'clip_range': 0.37757085535729756, 'ent_coef': 0.00017055556769922042, 'std_penalty_weight': 0.28813167612676016}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_10 -> {'reset_noise_scale': 0.0811219889284557, 'forward_reward_weight': 0.794019967338759, 'ctrl_cost_weight': 0.1909084649203593, 'healthy_reward': 1.4695470159426132, 'contact_cost_weight': 0.00048075670076003045, 'healthy_z_lower': 0.19353492629665098, 'healthy_z_upper': 1.1936905952567158, 'contact_force_min': -0.5349939620294489, 'contact_force_max': 0.7307512698224117, 'learning_rate': 0.0003564760563058714, 'n_steps': 2048, 'batch_size': 512, 'gamma': 0.9762294172462653, 'gae_lambda': 0.9261117656360015, 'clip_range': 0.3320669028429513, 'ent_coef': 0.0026780011357637598, 'std_penalty_weight': 0.20050838533111062}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_11 -> {'reset_noise_scale': 0.07145476067020312, 'forward_reward_weight': 1.545247241271003, 'ctrl_cost_weight': 0.5888754350371963, 'healthy_reward': 1.8961539637830271, 'contact_cost_weight': 0.000378389770529098, 'healthy_z_lower': 0.12761016035917702, 'healthy_z_upper': 1.0403641157003203, 'contact_force_min': -0.8542989596897922, 'contact_force_max': 0.948423208060974, 'learning_rate': 5.070148561650504e-05, 'n_steps': 6144, 'batch_size': 4096, 'gamma': 0.970695023374151, 'gae_lambda': 0.9257548804331505, 'clip_range': 0.17667299134062348, 'ent_coef': 0.15117794816431884}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO_12 -> [I 2025-02-13 21:52:57,075] Trial 142 finished with value: 2147.46312115008 and parameters: {'reset_noise_scale': 0.05057132050677559, 'forward_reward_weight': 1.5242065847007638, 'ctrl_cost_weight': 1.0518615515373424, 'healthy_reward': 1.8906418906546967, 'contact_cost_weight': 0.00017255813970342036, 'healthy_z_lower': 0.171480519365306, 'healthy_z_upper': 0.9415199367799989, 'contact_force_min': -0.8951041717141913, 'contact_force_max': 0.9341249818927484, 'learning_rate': 0.0006673200204296133, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9635194700970308, 'gae_lambda': 0.9254828522668759, 'clip_range': 0.2273207023257754, 'ent_coef': 0.0003757232557716752}. Best is trial 142 with value: 2147.46312115008.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2025-02-13 23:00:25,932] Trial 220 finished with value: 2149.324657643946 and parameters: {'reset_noise_scale': 0.190758010402181, 'forward_reward_weight': 1.5237537263020415, 'ctrl_cost_weight': 0.8907565868390414, 'healthy_reward': 1.8601364542985845, 'contact_cost_weight': 0.0001613795517276871, 'healthy_z_lower': 0.15522435940425033, 'healthy_z_upper': 0.9593914101020591, 'contact_force_min': -0.9042700990109689, 'contact_force_max': 0.6524756726457444, 'learning_rate': 2.9911610221111873e-05, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9623881975118631, 'gae_lambda': 0.9240000421659839, 'clip_range': 0.22260259988295936, 'ent_coef': 0.025157413751276267}. Best is trial 220 with value: 2149.324657643946.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale= 0.05057132050677559 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight = 1.5242065847007638 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.0518615515373424 # peso del reward per il controllo\n",
    "hp_healthy_reward = 1.8906418906546967 # reward per la salute\n",
    "\n",
    "hp_contact_cost_weight = 0.00017255813970342036\n",
    "healthy_z = (0.171480519365306, 0.9415199367799989)\n",
    "contact_force = (-0.8951041717141913, 0.9341249818927484)\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=0.0006673200204296133           # Tasso di apprendimento: controlla la velocitÃ  con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=8192                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=2048                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.9635194700970308      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.9254828522668759              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.2273207023257754               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.0003757232557716752                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrÃ  effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 Ã¨ lâambiente piÃ¹ recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z,\n",
    "                    contact_force_range=contact_force,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire piÃ¹ istanze dell'ambiente come se fossero una singola entitÃ .\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "#env = DummyVecEnv([make_env])  \n",
    "\n",
    "\n",
    "NUM_ENVS=8\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def linear_schedule(initial_value):\n",
    "#     def func(progress_remaining):\n",
    "#         return progress_remaining * initial_value\n",
    "#     return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=hp_learning_rate,  # usa lo scheduler lineare\n",
    "    n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    #seed=42,                        # Seed per la riproducibilitÃ \n",
    "    verbose=1,                    # Livello di verbositÃ : 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#eval_env = DummyVecEnv([make_env])\n",
    "\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=50000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_12\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2346  |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2344        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011144906 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -11.3       |\n",
      "|    explained_variance   | -3.39       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | 0.00294     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.0634      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2424        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012187309 |\n",
      "|    clip_fraction        | 0.099       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -11.1       |\n",
      "|    explained_variance   | -0.0634     |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2453        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013912931 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -10.9       |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.944       |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2459        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014005961 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -10.7       |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    std                  | 0.921       |\n",
      "|    value_loss           | 0.013       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2438        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014844805 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -10.5       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 0.0134      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=400000, episode_reward=20.73 +/- 1.93\n",
      "Episode length: 930.80 +/- 138.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 931         |\n",
      "|    mean_reward          | 20.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015377429 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -10.3       |\n",
      "|    explained_variance   | 0.587       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.874       |\n",
      "|    value_loss           | 0.0131      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 2416   |\n",
      "|    iterations      | 7      |\n",
      "|    time_elapsed    | 189    |\n",
      "|    total_timesteps | 458752 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2436        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016354714 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -10.1       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    std                  | 0.851       |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2440        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015982363 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -9.87       |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0232     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    std                  | 0.83        |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2436        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016443472 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -9.66       |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.809       |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2436        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 720896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017246569 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -9.45       |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.788       |\n",
      "|    value_loss           | 0.0128      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2445        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 321         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018811066 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -9.23       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.026      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 0.0102      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=4.71 +/- 6.20\n",
      "Episode length: 249.40 +/- 376.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 249         |\n",
      "|    mean_reward          | 4.71        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019728575 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -9          |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.745       |\n",
      "|    value_loss           | 0.00972     |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 2442   |\n",
      "|    iterations      | 13     |\n",
      "|    time_elapsed    | 348    |\n",
      "|    total_timesteps | 851968 |\n",
      "-------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2447        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 374         |\n",
      "|    total_timesteps      | 917504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019834934 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -8.78       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 0.0101      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2450        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020572511 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -8.54       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0322     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    std                  | 0.703       |\n",
      "|    value_loss           | 0.00855     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2458        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 1048576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021519361 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -8.29       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.681       |\n",
      "|    value_loss           | 0.00839     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2453       |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 454        |\n",
      "|    total_timesteps      | 1114112    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02136382 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.227      |\n",
      "|    entropy_loss         | -8.04      |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.000667   |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.66       |\n",
      "|    value_loss           | 0.00839    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2456        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 480         |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022215473 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -7.79       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 0.00824     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=1.65 +/- 1.67\n",
      "Episode length: 60.40 +/- 74.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 60.4        |\n",
      "|    mean_reward          | 1.65        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023672584 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -7.53       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.041      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.619       |\n",
      "|    value_loss           | 0.00753     |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 2453    |\n",
      "|    iterations      | 19      |\n",
      "|    time_elapsed    | 507     |\n",
      "|    total_timesteps | 1245184 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2450        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 1310720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023098934 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -7.27       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.599       |\n",
      "|    value_loss           | 0.00762     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2458        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 559         |\n",
      "|    total_timesteps      | 1376256     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023354407 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -6.99       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.579       |\n",
      "|    value_loss           | 0.00611     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2456        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 586         |\n",
      "|    total_timesteps      | 1441792     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023586756 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -6.74       |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.561       |\n",
      "|    value_loss           | 0.00714     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2457        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 613         |\n",
      "|    total_timesteps      | 1507328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024910346 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -6.47       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    std                  | 0.542       |\n",
      "|    value_loss           | 0.00687     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2451        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 641         |\n",
      "|    total_timesteps      | 1572864     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024394723 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -6.21       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.525       |\n",
      "|    value_loss           | 0.0076      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=2.14 +/- 1.13\n",
      "Episode length: 74.20 +/- 51.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 74.2        |\n",
      "|    mean_reward          | 2.14        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023987617 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -5.96       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0282     |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 0.00786     |\n",
      "-----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 2447    |\n",
      "|    iterations      | 25      |\n",
      "|    time_elapsed    | 669     |\n",
      "|    total_timesteps | 1638400 |\n",
      "--------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2441        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 697         |\n",
      "|    total_timesteps      | 1703936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025008252 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -5.7        |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 0.00693     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2438        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 725         |\n",
      "|    total_timesteps      | 1769472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025053501 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -5.44       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.039      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.477       |\n",
      "|    value_loss           | 0.00676     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2437       |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 752        |\n",
      "|    total_timesteps      | 1835008    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02581071 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.227      |\n",
      "|    entropy_loss         | -5.19      |\n",
      "|    explained_variance   | 0.793      |\n",
      "|    learning_rate        | 0.000667   |\n",
      "|    loss                 | -0.0322    |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.462      |\n",
      "|    value_loss           | 0.00709    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2438        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 779         |\n",
      "|    total_timesteps      | 1900544     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026369307 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -4.94       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.448       |\n",
      "|    value_loss           | 0.00653     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2439        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 805         |\n",
      "|    total_timesteps      | 1966080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025544401 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.227       |\n",
      "|    entropy_loss         | -4.7        |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.000667    |\n",
      "|    loss                 | -0.0323     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 0.00688     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=12.99 +/- 9.47\n",
      "Episode length: 611.60 +/- 475.76\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 612        |\n",
      "|    mean_reward          | 13         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02601606 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.227      |\n",
      "|    entropy_loss         | -4.45      |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.000667   |\n",
      "|    loss                 | -0.0333    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.421      |\n",
      "|    value_loss           | 0.00711    |\n",
      "----------------------------------------\n",
      "--------------------------------\n",
      "| time/              |         |\n",
      "|    fps             | 2436    |\n",
      "|    iterations      | 31      |\n",
      "|    time_elapsed    | 833     |\n",
      "|    total_timesteps | 2031616 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x178a6b220>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirÃ  durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 2000000  # Puoi aumentare questo valore per permettere al modello di acquisire piÃ¹ esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "# def evaluate_policy(env, policy, episodes=500):\n",
    "#     \"\"\"\n",
    "#     Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "#     Parametri:\n",
    "#     - env: L'ambiente di simulazione.\n",
    "#     - policy: La policy addestrata da valutare.\n",
    "#     - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "#     Ritorna:\n",
    "#     - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "#     \"\"\"\n",
    "#     total_rewards = []\n",
    "#     for _ in range(episodes):\n",
    "#         obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         while not done:\n",
    "#             action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "#             obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "#             total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "#         total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "#     return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]\n",
    "            obs, rewards, done, infos = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "        total_rewards.extend(episode_rewards)\n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    # std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mnorm_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# Disabilita la normalizzazione della reward. Questo Ã¨ importante per valutare correttamente il modello.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Valutazione dopo l'addestramento\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m mean_reward_trained, std_reward_trained \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Valuta la policy addestrata\u001b[39;00m\n\u001b[1;32m      6\u001b[0m mean_reward_random, std_reward_random \u001b[38;5;241m=\u001b[39m evaluate_random_policy(env)  \u001b[38;5;66;03m# Valuta la policy casuale\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Stampa dei risultati\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:84\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     82\u001b[0m current_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_envs)\n\u001b[1;32m     83\u001b[0m current_lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_envs, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m observations \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:297\u001b[0m, in \u001b[0;36mVecNormalize.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]]:\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Reset all environments\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    :return: first observation of the episode\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:138\u001b[0m, in \u001b[0;36mSubprocVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     remote\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx])))\n\u001b[1;32m    137\u001b[0m results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[0;32m--> 138\u001b[0m obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_seeds()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "# env.training = False # Setta l'environment in modalitÃ  di valutazione\n",
    "# env.norm_reward = False # Disabilita la normalizzazione della reward. Questo Ã¨ importante per valutare correttamente il modello.\n",
    "\n",
    "# # Valutazione dopo l'addestramento\n",
    "# mean_reward_trained, std_reward_trained = evaluate_policy(model, env, n_eval_episodes=500)  # Valuta la policy addestrata\n",
    "# mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# # Stampa dei risultati\n",
    "# print(f\"Trained Policy: Mean Reward: {mean_reward_trained}\")\n",
    "# print(f\"Random Policy: Mean Reward: {mean_reward_random}\")\n",
    "\n",
    "# # Creazione del grafico di confronto\n",
    "# # labels = ['Random Policy', 'Trained Policy']\n",
    "# # means = [mean_reward_random, mean_reward_trained]\n",
    "# # stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "# # plt.figure(figsize=(8, 5))\n",
    "# # plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "# # plt.ylabel('Mean Episodic Reward')\n",
    "# # plt.title('Policy Comparison')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
