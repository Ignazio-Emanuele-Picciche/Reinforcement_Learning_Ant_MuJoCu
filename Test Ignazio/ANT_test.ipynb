{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Training completo tra 5mln a 10mln di TimeStamp e tra 5000 e 10000 episodi\n",
    "\n",
    "-Per un tuning rapido da 500k a 1mln di TimeStamp e tra 500 a 1k episodi per trial\n",
    "\n",
    "-Per i test preliminari 1mln di timestamp e 1k/2k episodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ipreparametri dell'envrionment\n",
    "hp_reset_noise_scale=0.2282706739101626 # scala del rumore quando l'ambiente viene resettato \n",
    "hp_forward_reward_weight=0.09314040045482441 # peso del reward per il movimento in avanti\n",
    "hp_ctrl_cost_weight=0.028140178122103423 # peso del reward per il controllo\n",
    "hp_healthy_reward =0.9926479631637423 # reward per la salute\n",
    "\n",
    "\n",
    "# Iperparametri del modello/policy\n",
    "hp_policy=\"MlpPolicy\"           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "hp_learning_rate=0.0008676828845312949           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "hp_n_steps=4096                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "hp_batch_size=64                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "hp_n_epochs=10                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "hp_gamma=0.9328230070576791      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "hp_gae_lambda=0.95              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "hp_clip_range=0.2               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "hp_ent_coef=0.0                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    render_mode='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "env = DummyVecEnv([make_env])  \n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=hp_policy,           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=hp_learning_rate,           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "    n_steps=hp_n_steps,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=hp_batch_size,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=hp_n_epochs,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=hp_gamma,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=hp_gae_lambda,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=hp_clip_range,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=hp_ent_coef,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    verbose=1,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 369  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Alleniamo il modello\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# usato per aggiornare la sua politica interna.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m total_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200000\u001b[39m  \u001b[38;5;66;03m# Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rollout_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m    208\u001b[0m     actions \u001b[38;5;241m=\u001b[39m rollout_data\u001b[38;5;241m.\u001b[39mactions\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:483\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Generator[RolloutBufferSamples, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 483\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_ready:\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:4749\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.permutation\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/numpy/_core/multiarray.py:678\u001b[0m, in \u001b[0;36mresult_type\u001b[0;34m(*arrays_and_dtypes)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    min_scalar_type(a, /)\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m \n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mresult_type)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresult_type\u001b[39m(\u001b[38;5;241m*\u001b[39marrays_and_dtypes):\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 200000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, episodes=50):\n",
    "    \"\"\"\n",
    "    Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - policy: La policy addestrata da valutare.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "\n",
    "def evaluate_random_policy(env, episodes=50):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Genera un'azione casuale\n",
    "            obs, reward, done, _ = env.step(np.array(action))  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Policy: Mean Reward: 5.23187255859375, Std: 9.203064918518066\n",
      "Random Policy: Mean Reward: 6.862292289733887, Std: 9.959856033325195\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(env, model)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}, Std: {std_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}, Std: {std_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
