{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Usa SubprocVecEnv per sfruttare il multiprocessing (più veloce di DummyVecEnv)\n",
    "NUM_ENVS = 4  # Numero di ambienti paralleli per accelerare il training\n",
    "\n",
    "# ✅ Definiamo la funzione per creare un ambiente vettorializzato\n",
    "def make_env(healthy_z_lower=0.2746892357443735, healthy_z_upper= 0.9845961239589529,contact_force_min= -0.7057855820088252,contact_force_max=0.6829751913715009):\n",
    "    return Monitor(gym.make(\"Ant-v5\",\n",
    "                            reset_noise_scale=0.15280089381590506,\n",
    "                            forward_reward_weight=0.8100861873330247,\n",
    "                            ctrl_cost_weight=0.7787466257062721,\n",
    "                            healthy_reward=1.0,\n",
    "                            contact_cost_weight=5e-4,\n",
    "                            healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "                            contact_force_range=(contact_force_min, contact_force_max),\n",
    "                            render_mode='none'))\n",
    "\n",
    "# ✅ Creiamo gli ambienti paralleli\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/gymnasium/envs/registration.py:734: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='none' that is not in the possible render_modes (['human', 'rgb_array', 'depth_array']).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ Parametri del modello (puoi ottimizzarli con Optuna)\n",
    "model_params = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env,\n",
    "    \"learning_rate\": 1.023387964377036e-05,  # Usa Optuna per trovare il migliore\n",
    "    \"n_steps\": 4096,\n",
    "    \"batch_size\": 256,\n",
    "    \"n_epochs\": 10,\n",
    "    \"gamma\": 0.992330458473981,\n",
    "    \"gae_lambda\": 0.8064387416284831,\n",
    "    \"clip_range\": 0.14933184841818672,\n",
    "    \"ent_coef\": 0.017031658560933347,\n",
    "    \"verbose\": 1,\n",
    "    \"tensorboard_log\": \"./ppo_Ant_tensorboard/\",\n",
    "    \"device\": \"mps\"  # Usa GPU se disponibile\n",
    "}\n",
    "\n",
    "# ✅ Definiamo i callback per salvataggio e valutazione\n",
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/best_model\",\n",
    "                             log_path=\"./logs/\", eval_freq=5000, deterministic=True, render=False)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=\"./logs/checkpoints/\",\n",
    "                                         name_prefix=\"ppo_ant_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabiodigregorio/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Logging to ./ppo_Ant_tensorboard/PPO_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45.6     |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    fps             | 1267     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=1000.60 +/- 3.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 1e+03         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015621135 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.149         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -2.89         |\n",
      "|    learning_rate        | 1.02e-05      |\n",
      "|    loss                 | -0.115        |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00128      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.269         |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.8     |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    fps             | 927      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=997.71 +/- 1.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 998           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 40000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013216221 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.149         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -2.29         |\n",
      "|    learning_rate        | 1.02e-05      |\n",
      "|    loss                 | -0.139        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00118      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.12          |\n",
      "-------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46.1     |\n",
      "|    ep_rew_mean     | -105     |\n",
      "| time/              |          |\n",
      "|    fps             | 840      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1003.59 +/- 5.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 1e+03         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 60000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014204938 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.149         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -1.05         |\n",
      "|    learning_rate        | 1.02e-05      |\n",
      "|    loss                 | -0.161        |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00128      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0858        |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.2     |\n",
      "|    ep_rew_mean     | -111     |\n",
      "| time/              |          |\n",
      "|    fps             | 798      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1013.41 +/- 7.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 1e+03         |\n",
      "|    mean_reward          | 1.01e+03      |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 80000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012705397 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.149         |\n",
      "|    entropy_loss         | -11.4         |\n",
      "|    explained_variance   | -0.743        |\n",
      "|    learning_rate        | 1.02e-05      |\n",
      "|    loss                 | -0.166        |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00123      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.0755        |\n",
      "-------------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46.5     |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 55.5          |\n",
      "|    ep_rew_mean          | -121          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 791           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 124           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012690571 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.149         |\n",
      "|    entropy_loss         | -11.3         |\n",
      "|    explained_variance   | -0.603        |\n",
      "|    learning_rate        | 1.02e-05      |\n",
      "|    loss                 | -0.162        |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.00123      |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 0.067         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1003.74 +/- 1.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "--------------------------------------------\n",
      "| eval/                   |                |\n",
      "|    mean_ep_length       | 1e+03          |\n",
      "|    mean_reward          | 1e+03          |\n",
      "| time/                   |                |\n",
      "|    total_timesteps      | 100000         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000121971876 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.149          |\n",
      "|    entropy_loss         | -11.3          |\n",
      "|    explained_variance   | -0.543         |\n",
      "|    learning_rate        | 1.02e-05       |\n",
      "|    loss                 | -0.17          |\n",
      "|    n_updates            | 60             |\n",
      "|    policy_gradient_loss | -0.00117       |\n",
      "|    std                  | 0.999          |\n",
      "|    value_loss           | 0.0648         |\n",
      "--------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.8     |\n",
      "|    ep_rew_mean     | -106     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Mean Reward: 1033.34 ± 21.35\n"
     ]
    }
   ],
   "source": [
    "# ✅ Training del modello\n",
    "model = PPO(**model_params)\n",
    "model.learn(total_timesteps=100_000, callback=CallbackList([eval_callback, checkpoint_callback]))\n",
    "\n",
    "# ✅ Salvataggio del modello e della normalizzazione\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")\n",
    "\n",
    "# ✅ Funzione di valutazione migliorata\n",
    "def evaluate_agent(model, env, episodes=100):\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=episodes, deterministic=True)\n",
    "    print(f\"Mean Reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "# ✅ Valutiamo il modello addestrato\n",
    "mean_reward_trained, std_reward_trained = evaluate_agent(model, env, episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione\n",
    "# ✅ Disattiva la normalizzazione della reward per la valutazione corretta\n",
    "env.training = False\n",
    "env.norm_reward = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_policy(env, policy, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - policy: La policy addestrata da valutare.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not np.all(done):\n",
    "            action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "# 200-400 episodi sono adeguati \n",
    "def evaluate_random_policy(env, episodes=500):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not np.all(done):\n",
    "            action = env.action_space.sample()  # Genera un'azione casuale\n",
    "            obs, reward, done, _ = env.step(np.array([action]))  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Valutazione dopo l'addestramento\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_reward_trained, std_reward_trained \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Valuta la policy addestrata\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mean_reward_random, std_reward_random \u001b[38;5;241m=\u001b[39m evaluate_random_policy(env)  \u001b[38;5;66;03m# Valuta la policy casuale\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Stampa dei risultati\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 19\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(env, policy, episodes)\u001b[0m\n\u001b[1;32m     17\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     20\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mpredict(obs)  \u001b[38;5;66;03m# Predice l'azione da eseguire\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Esegue l'azione e ottiene il feedback dall'ambiente\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(env, model)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}, Std: {std_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}, Std: {std_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
