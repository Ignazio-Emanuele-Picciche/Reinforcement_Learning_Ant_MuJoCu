{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=0.2282706739101626, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=0.09314040045482441, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=0.028140178122103423, # peso del reward per il controllo\n",
    "                    healthy_reward =0.9926479631637423, # reward per la salute\n",
    "                    render_mode='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "MuJoCo is not installed, run `pip install \"gymnasium[mujoco]\"`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/mujoco_env.py:13\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmujoco\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mujoco'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Parametri:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m env \u001b[38;5;241m=\u001b[39m VecNormalize(env, norm_obs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, norm_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clip_obs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.13/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:31\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: \u001b[38;5;28mlist\u001b[39m[Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]]):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m [_patch_env(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mid\u001b[39m(env\u001b[38;5;241m.\u001b[39munwrapped) \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs])) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCrea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    gym.Env: l'ambiente Ant-v5 inizializzato.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Ant-v5 è l’ambiente più recente in Gymnasium.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnt-v5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreset_noise_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2282706739101626\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# scala del rumore quando l'ambiente viene resettato \u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforward_reward_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.09314040045482441\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# peso del reward per il movimento in avanti\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mctrl_cost_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.028140178122103423\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# peso del reward per il controllo\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhealthy_reward\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9926479631637423\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# reward per la salute\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.13/site-packages/gymnasium/envs/registration.py:702\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    705\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.13/site-packages/gymnasium/envs/registration.py:549\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    548\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 549\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.0_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/importlib/__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1310\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1022\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MujocoEnv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmujoco_rendering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MujocoRenderer\n",
      "File \u001b[0;32m~/Desktop/campus bio iscrizione/ Magistrale/Merone/RL/Reinforcement_Learning_Ant_MuJoCu/venv/lib/python3.13/site-packages/gymnasium/envs/mujoco/mujoco_env.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmujoco\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMuJoCo is not installed, run `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgymnasium[mujoco]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     20\u001b[0m DEFAULT_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m480\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexpand_model_path\u001b[39m(model_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: MuJoCo is not installed, run `pip install \"gymnasium[mujoco]\"`"
     ]
    }
   ],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "env = DummyVecEnv([make_env])  \n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# 3. Definiamo il modello RL (PPO) con spiegazioni dettagliate per ciascun parametro\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",           # Tipo di policy: una rete neurale MLP (Multilayer Perceptron) che mappa osservazioni ad azioni\n",
    "    env=env,                      # Ambiente di addestramento: usa l'ambiente vettorializzato e normalizzato creato in precedenza\n",
    "    learning_rate=0.0008676828845312949,           # Tasso di apprendimento: controlla la velocità con cui il modello apprende aggiornando i pesi\n",
    "    n_steps=4096,                 # Numero di passi da eseguire nell'ambiente per ogni ciclo di aggiornamento della policy\n",
    "    batch_size=64,                # Dimensione del batch per gli aggiornamenti stocastici: suddivide i dati raccolti nei mini-batch\n",
    "    n_epochs=100,                  # Numero di volte (epoch) che il dataset raccolto viene utilizzato per aggiornare la policy\n",
    "    gamma=0.9328230070576791,      # Fattore di sconto: determina l'importanza delle ricompense future rispetto a quelle immediate\n",
    "    gae_lambda=0.95,              # Parametro per il Generalized Advantage Estimation (GAE): bilancia bias e varianza nella stima dell'advantage\n",
    "    clip_range=0.2,               # Intervallo di clipping: limita le variazioni della policy per mantenere aggiornamenti stabili\n",
    "    ent_coef=0.0,                 # Coefficiente di entropia: controlla l'incentivo all'esplorazione; 0 significa nessun bonus per l'entropia\n",
    "    verbose=1,                    # Livello di verbosità: 1 per stampare informazioni di log utili durante l'addestramento\n",
    "    tensorboard_log=\"./ppo_Ant_tensorboard/\",  # Cartella per salvare i log di TensorBoard\n",
    "    device='mps'                    # Specifica l'uso della GPU su Apple Silicon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_Ant_tensorboard/PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 410  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 99        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 81        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3490763 |\n",
      "|    clip_fraction        | 0.742     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -11.2     |\n",
      "|    explained_variance   | -3.21     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.16     |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -0.125    |\n",
      "|    std                  | 0.974     |\n",
      "|    value_loss           | 0.0558    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35547638 |\n",
      "|    clip_fraction        | 0.808      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11        |\n",
      "|    explained_variance   | -12.7      |\n",
      "|    learning_rate        | 0.000868   |\n",
      "|    loss                 | -0.131     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.125     |\n",
      "|    std                  | 0.948      |\n",
      "|    value_loss           | 0.00651    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 73         |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 222        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48548216 |\n",
      "|    clip_fraction        | 0.849      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.7      |\n",
      "|    explained_variance   | -1.39      |\n",
      "|    learning_rate        | 0.000868   |\n",
      "|    loss                 | -0.12      |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.132     |\n",
      "|    std                  | 0.915      |\n",
      "|    value_loss           | 0.00208    |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 295       |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6561541 |\n",
      "|    clip_fraction        | 0.868     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -10.4     |\n",
      "|    explained_variance   | -0.446    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 400       |\n",
      "|    policy_gradient_loss | -0.135    |\n",
      "|    std                  | 0.881     |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 67         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88240063 |\n",
      "|    clip_fraction        | 0.885      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -10.1      |\n",
      "|    explained_variance   | -1.03      |\n",
      "|    learning_rate        | 0.000868   |\n",
      "|    loss                 | -0.146     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.133     |\n",
      "|    std                  | 0.85       |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 66        |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 433       |\n",
      "|    total_timesteps      | 28672     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8798077 |\n",
      "|    clip_fraction        | 0.866     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.83     |\n",
      "|    explained_variance   | 0.215     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 600       |\n",
      "|    policy_gradient_loss | -0.119    |\n",
      "|    std                  | 0.818     |\n",
      "|    value_loss           | 0.000556  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 64        |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 505       |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9534037 |\n",
      "|    clip_fraction        | 0.886     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.55     |\n",
      "|    explained_variance   | 0.0751    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.142    |\n",
      "|    n_updates            | 700       |\n",
      "|    policy_gradient_loss | -0.124    |\n",
      "|    std                  | 0.795     |\n",
      "|    value_loss           | 0.000702  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 63        |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 580       |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0497825 |\n",
      "|    clip_fraction        | 0.885     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.23     |\n",
      "|    explained_variance   | 0.0742    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 800       |\n",
      "|    policy_gradient_loss | -0.125    |\n",
      "|    std                  | 0.761     |\n",
      "|    value_loss           | 0.00077   |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 62       |\n",
      "|    iterations           | 10       |\n",
      "|    time_elapsed         | 651      |\n",
      "|    total_timesteps      | 40960    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.115773 |\n",
      "|    clip_fraction        | 0.89     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -8.93    |\n",
      "|    explained_variance   | -0.0355  |\n",
      "|    learning_rate        | 0.000868 |\n",
      "|    loss                 | -0.144   |\n",
      "|    n_updates            | 900      |\n",
      "|    policy_gradient_loss | -0.126   |\n",
      "|    std                  | 0.739    |\n",
      "|    value_loss           | 0.000955 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 62        |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 721       |\n",
      "|    total_timesteps      | 45056     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7968566 |\n",
      "|    clip_fraction        | 0.897     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.71     |\n",
      "|    explained_variance   | -0.0566   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 1000      |\n",
      "|    policy_gradient_loss | -0.125    |\n",
      "|    std                  | 0.714     |\n",
      "|    value_loss           | 0.000771  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 62        |\n",
      "|    iterations           | 12        |\n",
      "|    time_elapsed         | 791       |\n",
      "|    total_timesteps      | 49152     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4062843 |\n",
      "|    clip_fraction        | 0.903     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.37     |\n",
      "|    explained_variance   | -0.0727   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 1100      |\n",
      "|    policy_gradient_loss | -0.123    |\n",
      "|    std                  | 0.685     |\n",
      "|    value_loss           | 0.000531  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 860       |\n",
      "|    total_timesteps      | 53248     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4611607 |\n",
      "|    clip_fraction        | 0.89      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.06     |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 1200      |\n",
      "|    policy_gradient_loss | -0.114    |\n",
      "|    std                  | 0.659     |\n",
      "|    value_loss           | 0.00072   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 930       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7562475 |\n",
      "|    clip_fraction        | 0.897     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.73     |\n",
      "|    explained_variance   | -0.0102   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 1300      |\n",
      "|    policy_gradient_loss | -0.113    |\n",
      "|    std                  | 0.638     |\n",
      "|    value_loss           | 0.000599  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 1000      |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3428221 |\n",
      "|    clip_fraction        | 0.886     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.54     |\n",
      "|    explained_variance   | -0.24     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0992   |\n",
      "|    std                  | 0.621     |\n",
      "|    value_loss           | 0.0003    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 16        |\n",
      "|    time_elapsed         | 1069      |\n",
      "|    total_timesteps      | 65536     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4419641 |\n",
      "|    clip_fraction        | 0.89      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.38     |\n",
      "|    explained_variance   | 0.0508    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 1500      |\n",
      "|    policy_gradient_loss | -0.0976   |\n",
      "|    std                  | 0.613     |\n",
      "|    value_loss           | 0.000357  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 1138      |\n",
      "|    total_timesteps      | 69632     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4249127 |\n",
      "|    clip_fraction        | 0.887     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.17     |\n",
      "|    explained_variance   | 0.208     |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 1600      |\n",
      "|    policy_gradient_loss | -0.107    |\n",
      "|    std                  | 0.594     |\n",
      "|    value_loss           | 0.000614  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 61        |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 1208      |\n",
      "|    total_timesteps      | 73728     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5083287 |\n",
      "|    clip_fraction        | 0.889     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.94     |\n",
      "|    explained_variance   | 0.0627    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.127    |\n",
      "|    n_updates            | 1700      |\n",
      "|    policy_gradient_loss | -0.114    |\n",
      "|    std                  | 0.577     |\n",
      "|    value_loss           | 0.000837  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 1277      |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.8853574 |\n",
      "|    clip_fraction        | 0.906     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.69     |\n",
      "|    explained_variance   | -0.102    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 1800      |\n",
      "|    policy_gradient_loss | -0.117    |\n",
      "|    std                  | 0.561     |\n",
      "|    value_loss           | 0.000901  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 1346      |\n",
      "|    total_timesteps      | 81920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6963869 |\n",
      "|    clip_fraction        | 0.898     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.41     |\n",
      "|    explained_variance   | 0.0872    |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.11     |\n",
      "|    std                  | 0.542     |\n",
      "|    value_loss           | 0.000793  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 1415      |\n",
      "|    total_timesteps      | 86016     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1149035 |\n",
      "|    clip_fraction        | 0.917     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -6.2      |\n",
      "|    explained_variance   | -0.0848   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 2000      |\n",
      "|    policy_gradient_loss | -0.108    |\n",
      "|    std                  | 0.527     |\n",
      "|    value_loss           | 0.000638  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 1485      |\n",
      "|    total_timesteps      | 90112     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9298136 |\n",
      "|    clip_fraction        | 0.909     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.94     |\n",
      "|    explained_variance   | -0.0837   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 2100      |\n",
      "|    policy_gradient_loss | -0.111    |\n",
      "|    std                  | 0.511     |\n",
      "|    value_loss           | 0.000821  |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 60       |\n",
      "|    iterations           | 23       |\n",
      "|    time_elapsed         | 1554     |\n",
      "|    total_timesteps      | 94208    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.177987 |\n",
      "|    clip_fraction        | 0.903    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -5.69    |\n",
      "|    explained_variance   | -0.0979  |\n",
      "|    learning_rate        | 0.000868 |\n",
      "|    loss                 | -0.0886  |\n",
      "|    n_updates            | 2200     |\n",
      "|    policy_gradient_loss | -0.115   |\n",
      "|    std                  | 0.493    |\n",
      "|    value_loss           | 0.00115  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 60       |\n",
      "|    iterations           | 24       |\n",
      "|    time_elapsed         | 1623     |\n",
      "|    total_timesteps      | 98304    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.378542 |\n",
      "|    clip_fraction        | 0.918    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -5.39    |\n",
      "|    explained_variance   | -0.239   |\n",
      "|    learning_rate        | 0.000868 |\n",
      "|    loss                 | -0.106   |\n",
      "|    n_updates            | 2300     |\n",
      "|    policy_gradient_loss | -0.106   |\n",
      "|    std                  | 0.478    |\n",
      "|    value_loss           | 0.000629 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 60        |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 1692      |\n",
      "|    total_timesteps      | 102400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.9681566 |\n",
      "|    clip_fraction        | 0.927     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -5.26     |\n",
      "|    explained_variance   | -0.0503   |\n",
      "|    learning_rate        | 0.000868  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    std                  | 0.469     |\n",
      "|    value_loss           | 0.000499  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x37c76fc40>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = 100000  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Salviamo il modello\n",
    "model.save(\"ppo_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\")  # salviamo anche i parametri di normalizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, episodes=50):\n",
    "    \"\"\"\n",
    "    Valuta una policy addestrata su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - policy: La policy addestrata da valutare.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)  # Predice l'azione da eseguire\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense\n",
    "\n",
    "\n",
    "def evaluate_random_policy(env, episodes=50):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()  # Reset dell'ambiente per iniziare un nuovo episodio\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Genera un'azione casuale\n",
    "            if isinstance(action, np.ndarray):\n",
    "                action = action.flatten()\n",
    "            obs, reward, done, _ = env.step(action)  # Esegue l'azione e ottiene il feedback dall'ambiente\n",
    "            total_reward += reward  # Accumula la ricompensa ottenuta\n",
    "        total_rewards.append(total_reward)  # Aggiunge la ricompensa totale dell'episodio alla lista\n",
    "    return np.mean(total_rewards), np.std(total_rewards)  # Calcola e ritorna la media e la deviazione standard delle ricompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione dopo l'addestramento\n",
    "mean_reward_trained, std_reward_trained = evaluate_policy(env, model)  # Valuta la policy addestrata\n",
    "mean_reward_random, std_reward_random = evaluate_random_policy(env)  # Valuta la policy casuale\n",
    "\n",
    "# Stampa dei risultati\n",
    "print(f\"Trained Policy: Mean Reward: {mean_reward_trained}, Std: {std_reward_trained}\")\n",
    "print(f\"Random Policy: Mean Reward: {mean_reward_random}, Std: {std_reward_random}\")\n",
    "\n",
    "# Creazione del grafico di confronto\n",
    "labels = ['Random Policy', 'Trained Policy']\n",
    "means = [mean_reward_random, mean_reward_trained]\n",
    "stds = [std_reward_random, std_reward_trained]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, means, yerr=stds, capsize=10, color=['skyblue', 'lightgreen'])\n",
    "plt.ylabel('Mean Episodic Reward')\n",
    "plt.title('Policy Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Valutazione\n",
    "# Qui creiamo un ambiente specifico per la valutazione del modello\n",
    "#\n",
    "# L'obiettivo è osservare in tempo reale come il modello interagisce con l'ambiente.\n",
    "# Questo ambiente differisce da quello usato durante l'allenamento (che era vettorializzato e normalizzato)\n",
    "# Per la valutazione, possiamo utilizzare l'ambiente \"grezzo\" di Gymnasium con rendering.\n",
    "#\n",
    "# Parametri:\n",
    "#   - \"Ant-v5\": Nome dell'ambiente;\n",
    "#   - reset_noise_scale=0.1: Scala del rumore applicato durante il reset dell'ambiente,\n",
    "#       utile per mantenere la coerenza con le condizioni viste durante l'allenamento.\n",
    "#   - render_mode=\"human\": Abilita il rendering dell'ambiente in tempo reale, permettendoci di osservare visivamente il comportamento del modello.\n",
    "# eval_env = gym.make(\"Ant-v5\", reset_noise_scale=0.1)\n",
    "#                     #, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ricreiamo il wrapper di normalizzazione con i parametri salvati\n",
    "# \"\"\"\n",
    "# Questo snippet di codice configura un ambiente di valutazione normalizzato per un compito di reinforcement learning.\n",
    "# Esegue le seguenti operazioni:\n",
    "\n",
    "# 1. Avvolge l'ambiente originale (eval_env) in un ambiente vettorializzato utilizzando DummyVecEnv.\n",
    "#     Questo è necessario per la compatibilità con algoritmi che richiedono un'interfaccia vettorializzata.\n",
    "# 2. Carica i parametri di normalizzazione precedentemente salvati (da 'vecnormalize_Ant.pkl') nel nuovo ambiente.\n",
    "#     Ciò garantisce che le osservazioni dello stato (e potenzialmente le ricompense) siano normalizzate\n",
    "#     nello stesso modo in cui sono state normalizzate durante l'addestramento.\n",
    "# 3. Imposta l'ambiente in modalità di valutazione:\n",
    "#     - Disabilita la modalità training (eval_env.training = False) in modo che le statistiche di normalizzazione non vengano aggiornate.\n",
    "#     - Disabilita la normalizzazione della ricompensa (eval_env.norm_reward = False) perché durante la valutazione le ricompense\n",
    "#       vengono tipicamente utilizzate nella loro forma originale.\n",
    "\n",
    "# Parametri:\n",
    "# - eval_env: L'istanza dell'ambiente che deve essere avvolta e normalizzata.\n",
    "# - \"vecnormalize_Ant.pkl\": Il percorso del file contenente i parametri di normalizzazione salvati specifici per l'ambiente 'Ant'.\n",
    "#   Questi parametri regolano la scala delle osservazioni e delle ricompense dell'ambiente.\n",
    "\n",
    "# Nota:\n",
    "# Questa configurazione è essenziale durante la valutazione di un modello addestrato per garantire che il preprocessamento\n",
    "# applicato corrisponda a quello dell'addestramento, senza modificare ulteriormente i parametri di normalizzazione.\n",
    "# \"\"\"\n",
    "# eval_env = DummyVecEnv([lambda: eval_env])\n",
    "# eval_env = VecNormalize.load(\"vecnormalize_Ant.pkl\", eval_env)\n",
    "# eval_env.training = False\n",
    "# eval_env.norm_reward = False  # In valutazione di solito non normalizziamo la ricompensa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.86552584\n",
      "Ricompensa totale episodio: -0.86552584\n"
     ]
    }
   ],
   "source": [
    "# # Reset dell'ambiente di valutazione per ottenere l'osservazione iniziale.\n",
    "# obs = eval_env.reset()\n",
    "\n",
    "# # Inizializzazione della variabile che accumulerà la ricompensa totale ottenuta nell'episodio.\n",
    "# episode_reward = 0.0\n",
    "\n",
    "# # Variabile booleana per controllare se l'episodio è terminato o troncato.\n",
    "# done = False\n",
    "\n",
    "# # Ciclo per eseguire l'iterazione dell'episodio\n",
    "# while not done:\n",
    "#     # Otteniamo l'azione in modalità deterministica dal modello (questo assicura che il modello non agisca con esplorazione casuale)\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "#     # Eseguiamo l'azione nell'ambiente di valutazione utilizzando il metodo step per ottenere\n",
    "#     # - observation: l'osservazione successiva\n",
    "#     # - reward: la ricompensa ottenuta per l'azione intrapresa\n",
    "#     # - terminated: flag che indica se l'episodio è terminato normalmente\n",
    "#     # - truncated: flag che indica se l'episodio è stato interrotto (es. timeout)\n",
    "#     observation, reward, terminated, truncated = eval_env.step(action)\n",
    "    \n",
    "#     # Aggiornamento della ricompensa totale: poiché l'ambiente è vettorializzato, reward è un array\n",
    "#     episode_reward += reward[0]  \n",
    "#     print(reward[0])\n",
    "    \n",
    "#     # L'episodio termina se risulta terminato (terminated=True) oppure se è troncato (truncated=True)\n",
    "#     done = terminated or truncated\n",
    "\n",
    "# # Stampa della ricompensa totale accumulata durante l'episodio.\n",
    "# print(\"Ricompensa totale episodio:\", episode_reward)\n",
    "\n",
    "# # Chiusura dell'ambiente per liberare le risorse.\n",
    "# eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
