{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "import optuna\n",
    "import gc\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(reset_noise_scale, forward_reward_weight, ctrl_cost_weight, healthy_reward, contact_cost_weight, healthy_z_range, contact_force_range):\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium con i parametri specificati.\n",
    "    \"\"\"\n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=reset_noise_scale, \n",
    "                    forward_reward_weight=forward_reward_weight, \n",
    "                    ctrl_cost_weight=ctrl_cost_weight, \n",
    "                    healthy_reward=healthy_reward, \n",
    "                    contact_cost_weight = contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z_range,\n",
    "                    contact_force_range=contact_force_range)\n",
    "                   # render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 09:34:26,458] A new study created in memory with name: no-name-d41f3e7c-b192-488e-92c1-47df582b5a34\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Parametri dell'environment\n",
    "    reset_noise_scale = trial.suggest_float('reset_noise_scale', 0, 0.3)\n",
    "    forward_reward_weight = trial.suggest_float('forward_reward_weight', 1.6, 1.9)\n",
    "    ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 1.2, 1.6)\n",
    "    healthy_reward = trial.suggest_float('healthy_reward', 2.1, 2.5)\n",
    "    contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-6, 1e-4)\n",
    "    healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.4)\n",
    "    healthy_z_upper = trial.suggest_float('healthy_z_upper', 1.1, 1.4)\n",
    "    contact_force_min = trial.suggest_float('contact_force_min', -1.3, -1)\n",
    "    contact_force_max = trial.suggest_float('contact_force_max', 0.8, 1.1)\n",
    "\n",
    "    NUM_ENVS = 6\n",
    "    env = SubprocVecEnv([\n",
    "        lambda: make_env(\n",
    "            reset_noise_scale,\n",
    "            forward_reward_weight,\n",
    "            ctrl_cost_weight,\n",
    "            healthy_reward,\n",
    "            contact_cost_weight, \n",
    "            healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "            contact_force_range=(contact_force_min, contact_force_max)\n",
    "        ) for _ in range(NUM_ENVS)\n",
    "    ])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "    env.training = False\n",
    "    env.norm_reward = False\n",
    "\n",
    "    # Parametri di ottimizzazione per TD3\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    learning_starts = trial.suggest_int('learning_starts', 1000, 10000, step=1000)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048])  \n",
    "    gamma = trial.suggest_float('gamma', 0.93, 0.96)\n",
    "    tau = trial.suggest_float('tau', 0.005, 0.05)\n",
    "    noise_std = trial.suggest_float('noise_std', 0.1, 0.5)  # Stddev per NormalActionNoise\n",
    "    noise_clip = trial.suggest_float('noise_clip', 0.2, 0.5)\n",
    "    policy_delay = trial.suggest_int('policy_delay', 1, 3)\n",
    "    train_freq = trial.suggest_int('train_freq', 1, 10)\n",
    "    gradient_steps = trial.suggest_int('gradient_steps', 1, 10)\n",
    "\n",
    "    # Definiamo il noise per l'azione\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(8), sigma=noise_std * np.ones(8))\n",
    "\n",
    "    model = TD3(\"MlpPolicy\", env,\n",
    "                learning_rate=learning_rate,\n",
    "                buffer_size=50000,\n",
    "                learning_starts=learning_starts,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                tau=tau,\n",
    "                action_noise=action_noise,\n",
    "                policy_delay=policy_delay,\n",
    "                train_freq=train_freq,\n",
    "                gradient_steps=gradient_steps,\n",
    "                seed=42,\n",
    "                verbose=0)\n",
    "    model.learn(total_timesteps=50000)\n",
    "\n",
    "    episodes = 200\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
    "\n",
    "    env.close()\n",
    "    del model, env\n",
    "    gc.collect()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
