{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import HParam\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri dell'ambiente\n",
    "hp_reset_noise_scale = 0.04759391555142866  # Scala del rumore quando l'ambiente viene resettato\n",
    "hp_forward_reward_weight = 1.895790973082321  # Peso della ricompensa per il movimento in avanti\n",
    "hp_ctrl_cost_weight = 1.5077149854491863  # Peso del costo di controllo del robot\n",
    "hp_healthy_reward = 2.4052783506228486  # Ricompensa per mantenere lo stato \"sano\"\n",
    "hp_contact_cost_weight = 6.97442897111752e-05  # Peso del costo per i contatti\n",
    "hp_healthy_z_lower = 0.38253131966695203  # Altezza minima considerata \"sana\"\n",
    "hp_healthy_z_upper = 1.1546046945435202  # Altezza massima considerata \"sana\"\n",
    "hp_contact_force_min = -1.254958844476713  # Forza minima di contatto considerata\n",
    "hp_contact_force_max = 0.9972723819502675  # Forza massima di contatto considerata\n",
    "hp_healthy_z_range=(hp_healthy_z_lower, hp_healthy_z_upper),\n",
    "hp_contact_force_range=(hp_contact_force_min, hp_contact_force_max)\n",
    "\n",
    "# Parametri di ottimizzazione per TD3\n",
    "hp_learning_rate = 0.0001500747188896999  # Learning rate per la rete di attore-critico\n",
    "hp_learning_starts = 5000  # Numero di step prima di iniziare gli aggiornamenti della rete\n",
    "hp_batch_size = 1024  # Dimensione del batch per SGD\n",
    "hp_gamma = 0.943633865948463  # Fattore di sconto per il futuro reward\n",
    "hp_tau = 0.00823476649680571  # Fattore di interpolazione per l'aggiornamento della rete target\n",
    "hp_noise_std = 0.11889226516450727  # Deviazione standard del rumore di esplorazione\n",
    "hp_noise_clip = 0.3182225104935072  # Clipping del rumore per la policy target smoothing\n",
    "hp_policy_delay = 3  # Delay tra gli aggiornamenti della politica rispetto ai Q-network updates\n",
    "hp_train_freq = 10  # Frequenza di aggiornamento della rete (ogni X step di interazione)\n",
    "hp_gradient_steps = 8  # Numero di passi di aggiornamento eseguiti dopo ogni batch\n",
    "\n",
    "# Parametri dell'azione (rumore per esplorazione)\n",
    "hp_action_noise_mean = [0.0] * 8  # Media del rumore gaussiano per l'esplorazione\n",
    "hp_action_noise_sigma = [0.3426192847562814] * 8  # Deviazione standard del rumore gaussiano\n",
    "\n",
    "# Parametri globali\n",
    "hp_num_envs = 6  # Numero di environment paralleli per il training\n",
    "hp_total_timesteps = 3_000_000  # Numero totale di timesteps per l'addestramento\n",
    "hp_episodes_evaluation = 200  # Numero di episodi usati per valutare la policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium.\n",
    "\n",
    "    Questa funzione istanzia l'ambiente \"Ant-v5\", uno degli ambienti recenti e ben supportati\n",
    "    in Gymnasium. I parametri usati sono:\n",
    "    - reset_noise_scale (0.1): determina la scala del rumore quando l'ambiente viene resettato.\n",
    "    - render_mode ('None'): indica che non verrà effettuato il rendering durante l'esecuzione.\n",
    "\n",
    "    Ritorna:\n",
    "        gym.Env: l'ambiente Ant-v5 inizializzato.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=hp_reset_noise_scale, # scala del rumore quando l'ambiente viene resettato \n",
    "                    forward_reward_weight=hp_forward_reward_weight, # peso del reward per il movimento in avanti\n",
    "                    ctrl_cost_weight=hp_ctrl_cost_weight, # peso del reward per il controllo\n",
    "                    healthy_reward =hp_healthy_reward, # reward per la salute\n",
    "                    contact_cost_weight=hp_contact_cost_weight,\n",
    "                    healthy_z_range=hp_healthy_z_range,\n",
    "                    contact_force_range=hp_contact_force_range,\n",
    "                    render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creiamo un ambiente vettorializzato (Vectorized Environment)\n",
    "# Utilizziamo DummyVecEnv per gestire più istanze dell'ambiente come se fossero una singola entità.\n",
    "# Qui passiamo la funzione make_env (definita in un'altra cella) che crea l'ambiente \"Ant-v5\".\n",
    "#env = DummyVecEnv([make_env])  \n",
    "\n",
    "\n",
    "NUM_ENVS=hp_num_envs\n",
    "env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "\n",
    "# 2. Normalizziamo osservazioni (obs) e ricompense (reward)\n",
    "# VecNormalize scala le osservazioni e le ricompense per stabilizzare l'allenamento.\n",
    "# Parametri:\n",
    "#   norm_obs=True   -> Abilita la normalizzazione delle osservazioni.\n",
    "#   norm_reward=True -> Abilita la normalizzazione delle ricompense.\n",
    "#   clip_obs=10.     -> Limita i valori normalizzati dell'osservazione a un range [-10, 10] per evitare estremi.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TD3(\"MlpPolicy\", env,\n",
    "                learning_rate=hp_learning_rate,\n",
    "                buffer_size=50000,\n",
    "                learning_starts=hp_learning_starts,\n",
    "                batch_size=hp_batch_size,\n",
    "                gamma=hp_gamma,\n",
    "                tau=hp_tau,\n",
    "                action_noise=hp_action_noise_mean,\n",
    "                policy_delay=hp_policy_delay,\n",
    "                train_freq=hp_train_freq,\n",
    "                gradient_steps=hp_gradient_steps,\n",
    "                seed=42,\n",
    "                verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = SubprocVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=50000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Alleniamo il modello\n",
    "# Il parametro total_timesteps indica il numero totale di iterazioni (o passi)\n",
    "# che il modello eseguirà durante l'allenamento. Ogni timestep rappresenta un'interazione\n",
    "# con l'ambiente in cui il modello esegue un'azione e riceve un feedback, che viene poi\n",
    "# usato per aggiornare la sua politica interna.\n",
    "total_timesteps = hp_total_timesteps  # Puoi aumentare questo valore per permettere al modello di acquisire più esperienza.\n",
    "model.learn(total_timesteps=total_timesteps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"td3_Ant_model\")\n",
    "env.save(\"vecnormalize_Ant.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_policy(env, episodes=hp_episodes_evaluation):\n",
    "    \"\"\"\n",
    "    Valuta una policy casuale su un ambiente dato.\n",
    "\n",
    "    Parametri:\n",
    "    - env: L'ambiente di simulazione.\n",
    "    - episodes: Numero di episodi da eseguire per la valutazione.\n",
    "\n",
    "    Ritorna:\n",
    "    - La ricompensa media e la deviazione standard delle ricompense ottenute.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = [False] * env.num_envs\n",
    "        episode_rewards = np.zeros(env.num_envs)\n",
    "        while not all(done):\n",
    "            actions = [env.action_space.sample() for _ in range(env.num_envs)]\n",
    "            obs, rewards, done, infos = env.step(actions)\n",
    "            episode_rewards += rewards\n",
    "        total_rewards.extend(episode_rewards)\n",
    "    mean_reward_random = np.mean(total_rewards)\n",
    "    # std_reward_random = np.std(total_rewards)\n",
    "    return mean_reward_random"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
