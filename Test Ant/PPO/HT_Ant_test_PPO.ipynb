{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv\n",
    "import numpy as np\n",
    "import tensorboard\n",
    "import optuna\n",
    "import gc\n",
    "\n",
    "# Install tqdm if not already installed\n",
    "from tqdm.autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(reset_noise_scale, forward_reward_weight, ctrl_cost_weight, healthy_reward, contact_cost_weight, healthy_z_range, contact_force_range):\n",
    "    \"\"\"\n",
    "    Crea e restituisce l'ambiente Ant-v5 dalla libreria Gymnasium con i parametri specificati.\n",
    "    \"\"\"\n",
    "    # Ant-v5 è l’ambiente più recente in Gymnasium.\n",
    "    return gym.make(\"Ant-v5\", \n",
    "                    reset_noise_scale=reset_noise_scale, \n",
    "                    forward_reward_weight=forward_reward_weight, \n",
    "                    ctrl_cost_weight=ctrl_cost_weight, \n",
    "                    healthy_reward=healthy_reward, \n",
    "                    contact_cost_weight = contact_cost_weight,\n",
    "                    healthy_z_range=healthy_z_range,\n",
    "                    contact_force_range=contact_force_range)\n",
    "                   # render_mode='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 max 30\n",
    "\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "    # forward_reward_weight = trial.suggest_float('forward_reward_weight', 0.5, 1.5)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "    # ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.1, 1.0)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "    # healthy_reward = trial.suggest_float('healthy_reward', 0.5, 1.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "    \n",
    "    # # Parametri aggiuntivi per Ant-v5\n",
    "    # contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "    # healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.3)             # Per definire l'intervallo di altezze \"sane\"\n",
    "    # healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.8, 1.2)\n",
    "    # contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "    # contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "\n",
    "    # learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    # n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])  \n",
    "    # # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "    # gamma = trial.suggest_float('gamma', 0.99, 0.999)\n",
    "    # gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)\n",
    "    # clip_range = trial.suggest_float('clip_range', 0.1, 0.3) \n",
    "    # ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "# V2 max 1600\n",
    " # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 0.5, 1.5)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.1, 1.0)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 0.5, 1.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.3)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.8, 1.2)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "# n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048, 4096])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.965, 0.98)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.9, 1.0)\n",
    "# clip_range = trial.suggest_float('clip_range', 0.3, 0.5) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "\n",
    "\n",
    "\n",
    "# V3 max 2130\n",
    " # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0.05, 0.2)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 1, 1.6)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 0.5, 1.2)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 1.4, 1.9)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-4, 1e-3)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0, 0.2)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 0.9, 1.1)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.0, -0.5)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.5, 1.0)\n",
    "\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "# n_steps = trial.suggest_int('n_steps', 2048, 8192, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [512, 1024, 2048, 4096])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.96, 0.98)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.88, 0.99)\n",
    "# clip_range = trial.suggest_float('clip_range', 0.1, 0.3) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.2)\n",
    "\n",
    "\n",
    "# V4 max 2681 (BEST)\n",
    "# # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0, 0.8)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 1.4, 1.8)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 1.1, 1.5)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 2, 2.4)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-5, 1e-4)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.25, 0.5)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 1, 1.3)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.2, -0.9)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.9, 1.2)\n",
    "\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "# n_steps = trial.suggest_int('n_steps', 4096, 12288, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.93, 0.96)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.95, 0.98)\n",
    "# clip_range = trial.suggest_float('clip_range', 0, 0.2) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "\n",
    "\n",
    "# V4\n",
    "# # Parametri dell'environment\n",
    "# reset_noise_scale = trial.suggest_float('reset_noise_scale', 0, 0.3)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "# forward_reward_weight = trial.suggest_float('forward_reward_weight', 1.6, 1.9)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "# ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 1.2, 1.6)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "# healthy_reward = trial.suggest_float('healthy_reward', 2.1, 2.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "# # Parametri aggiuntivi per Ant-v5\n",
    "# contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-6, 1e-4)  # Es. range intorno a 5e-4 come default\n",
    "# healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.4)             # Per definire l'intervallo di altezze \"sane\"\n",
    "# healthy_z_upper = trial.suggest_float('healthy_z_upper', 1.1, 1.4)\n",
    "# contact_force_min = trial.suggest_float('contact_force_min', -1.3, -1)         # Modificabile se usi forze di contatto\n",
    "# contact_force_max = trial.suggest_float('contact_force_max', 0.8, 1.1)\n",
    "\n",
    "# # Iperparametri per il modello PPO\n",
    "# learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "# n_steps = trial.suggest_int('n_steps', 4096, 12288, step=2048)\n",
    "# batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048])  \n",
    "# # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "# gamma = trial.suggest_float('gamma', 0.93, 0.96)\n",
    "# gae_lambda = trial.suggest_float('gae_lambda', 0.95, 0.98)\n",
    "# clip_range = trial.suggest_float('clip_range', 0, 0.2) \n",
    "# ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-18 21:24:49,075] A new study created in memory with name: no-name-04d095c6-f609-4a98-a9a8-71054dae3c16\n",
      "/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "[I 2025-02-18 21:25:52,712] Trial 0 finished with value: 2187.6477459304533 and parameters: {'reset_noise_scale': 0.09171603251283615, 'forward_reward_weight': 1.699998402180089, 'ctrl_cost_weight': 1.2941677136934688, 'healthy_reward': 2.2217415271854217, 'contact_cost_weight': 6.28068848090443e-05, 'healthy_z_lower': 0.39169059404509193, 'healthy_z_upper': 1.3511596695071864, 'contact_force_min': -1.0479202703677075, 'contact_force_max': 1.046298090989106, 'learning_rate': 0.00019209806109147747, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.9554652774628443, 'gae_lambda': 0.9730517003767273, 'clip_range': 0.12509694668314306, 'ent_coef': 0.09068818729604611}. Best is trial 0 with value: 2187.6477459304533.\n",
      "[I 2025-02-18 21:27:03,221] Trial 1 finished with value: 2436.9049594089515 and parameters: {'reset_noise_scale': 0.1742617040424867, 'forward_reward_weight': 1.781771799819307, 'ctrl_cost_weight': 1.2043361805084873, 'healthy_reward': 2.4425308270371966, 'contact_cost_weight': 7.646621535415296e-05, 'healthy_z_lower': 0.16025782764148447, 'healthy_z_upper': 1.3528658858936011, 'contact_force_min': -1.037370130050246, 'contact_force_max': 1.021388082905304, 'learning_rate': 3.259168042805593e-05, 'n_steps': 12288, 'batch_size': 256, 'gamma': 0.9457372976715991, 'gae_lambda': 0.9528885639829018, 'clip_range': 0.1788489322247949, 'ent_coef': 0.04852254397870497}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:28:04,163] Trial 2 finished with value: 2187.6021324902104 and parameters: {'reset_noise_scale': 0.06610949642568772, 'forward_reward_weight': 1.6866947364095826, 'ctrl_cost_weight': 1.5682915188064284, 'healthy_reward': 2.1624799442429516, 'contact_cost_weight': 5.593781922431726e-06, 'healthy_z_lower': 0.20556527990580745, 'healthy_z_upper': 1.107359018046072, 'contact_force_min': -1.1045942847950636, 'contact_force_max': 0.9565130666772097, 'learning_rate': 8.083460051535892e-05, 'n_steps': 10240, 'batch_size': 512, 'gamma': 0.953656032442174, 'gae_lambda': 0.9532584683575748, 'clip_range': 0.1835127388053449, 'ent_coef': 0.03758614617100037}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:29:02,565] Trial 3 finished with value: 2229.1268126692144 and parameters: {'reset_noise_scale': 0.06387618117207414, 'forward_reward_weight': 1.7546494518244666, 'ctrl_cost_weight': 1.204042012995473, 'healthy_reward': 2.2231729611769184, 'contact_cost_weight': 9.314163698403317e-05, 'healthy_z_lower': 0.20646839485968566, 'healthy_z_upper': 1.2019443141043415, 'contact_force_min': -1.2856710113337635, 'contact_force_max': 0.9463532873339663, 'learning_rate': 0.00012946718624557, 'n_steps': 4096, 'batch_size': 2048, 'gamma': 0.9501873341210832, 'gae_lambda': 0.97564263704997, 'clip_range': 0.18272188465966654, 'ent_coef': 0.03482346226395178}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:29:55,295] Trial 4 finished with value: 1536.6159985460636 and parameters: {'reset_noise_scale': 0.1785695759628143, 'forward_reward_weight': 1.8384517271811116, 'ctrl_cost_weight': 1.4745126037616347, 'healthy_reward': 2.328867662058896, 'contact_cost_weight': 1.2007814829962056e-05, 'healthy_z_lower': 0.3849406749608498, 'healthy_z_upper': 1.1926790276109305, 'contact_force_min': -1.2936504676696772, 'contact_force_max': 1.0601463502243613, 'learning_rate': 6.297852049935693e-05, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.9465544440580368, 'gae_lambda': 0.9609268314720121, 'clip_range': 0.04465455392698481, 'ent_coef': 0.03225091754162629}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:30:57,682] Trial 5 finished with value: 2211.7028648985674 and parameters: {'reset_noise_scale': 0.24369192076953544, 'forward_reward_weight': 1.684930507528688, 'ctrl_cost_weight': 1.5180863419827337, 'healthy_reward': 2.288551476465235, 'contact_cost_weight': 2.088866402906234e-06, 'healthy_z_lower': 0.26777976719180263, 'healthy_z_upper': 1.3118745481812462, 'contact_force_min': -1.016844582369351, 'contact_force_max': 0.8959237078939543, 'learning_rate': 0.0003075440547407254, 'n_steps': 12288, 'batch_size': 1024, 'gamma': 0.9397027826608929, 'gae_lambda': 0.9630286006059635, 'clip_range': 0.13245548472057386, 'ent_coef': 0.010182535350223565}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:32:08,870] Trial 6 finished with value: 2321.5613995211065 and parameters: {'reset_noise_scale': 0.2891261767087391, 'forward_reward_weight': 1.709038198394625, 'ctrl_cost_weight': 1.3827909391179738, 'healthy_reward': 2.4755947627074946, 'contact_cost_weight': 9.374419829792409e-05, 'healthy_z_lower': 0.10523457935526329, 'healthy_z_upper': 1.1468343070693194, 'contact_force_min': -1.297891875109169, 'contact_force_max': 0.8476098682514616, 'learning_rate': 0.00012069817055000222, 'n_steps': 6144, 'batch_size': 256, 'gamma': 0.9466015577801539, 'gae_lambda': 0.97768802656234, 'clip_range': 0.137071138281711, 'ent_coef': 0.09737726265204255}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:33:21,176] Trial 7 finished with value: 2416.7776478193205 and parameters: {'reset_noise_scale': 0.027149587522248886, 'forward_reward_weight': 1.8562355652969718, 'ctrl_cost_weight': 1.2008526496519085, 'healthy_reward': 2.4119785608821975, 'contact_cost_weight': 5.9557131486728575e-05, 'healthy_z_lower': 0.2353391938673541, 'healthy_z_upper': 1.358869840265177, 'contact_force_min': -1.032815297502067, 'contact_force_max': 0.8731161885915245, 'learning_rate': 4.2233727759008066e-05, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9310831603671329, 'gae_lambda': 0.9629242356032628, 'clip_range': 0.020604373188491155, 'ent_coef': 0.06134524017778734}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:34:29,377] Trial 8 finished with value: 2207.3683031307005 and parameters: {'reset_noise_scale': 0.1905569931219479, 'forward_reward_weight': 1.7487317582418067, 'ctrl_cost_weight': 1.245064843253641, 'healthy_reward': 2.2066544475569803, 'contact_cost_weight': 1.1463744712280755e-05, 'healthy_z_lower': 0.3625576910476408, 'healthy_z_upper': 1.3132898177816252, 'contact_force_min': -1.1441180551273062, 'contact_force_max': 1.0902826510327157, 'learning_rate': 1.432869220366202e-05, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9321457090349251, 'gae_lambda': 0.9643327669383218, 'clip_range': 0.0019311325841673277, 'ent_coef': 0.005889635366124702}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:35:31,313] Trial 9 finished with value: 2428.020608108469 and parameters: {'reset_noise_scale': 0.09628219980549092, 'forward_reward_weight': 1.8605456879474591, 'ctrl_cost_weight': 1.3455858901547946, 'healthy_reward': 2.3885502817764284, 'contact_cost_weight': 5.586684075983062e-05, 'healthy_z_lower': 0.24330338815650115, 'healthy_z_upper': 1.1787507684905905, 'contact_force_min': -1.134841520464153, 'contact_force_max': 0.9578564931334929, 'learning_rate': 0.00011283196042434473, 'n_steps': 6144, 'batch_size': 2048, 'gamma': 0.9512864121184805, 'gae_lambda': 0.9654229538896406, 'clip_range': 0.12976451653761376, 'ent_coef': 0.008536769974803416}. Best is trial 1 with value: 2436.9049594089515.\n",
      "[I 2025-02-18 21:36:40,440] Trial 10 finished with value: 2477.649387068674 and parameters: {'reset_noise_scale': 0.13963484977348478, 'forward_reward_weight': 1.6130170799528534, 'ctrl_cost_weight': 1.4288281073679574, 'healthy_reward': 2.469060617406831, 'contact_cost_weight': 7.780465588882423e-05, 'healthy_z_lower': 0.10234148146018668, 'healthy_z_upper': 1.3924608821537525, 'contact_force_min': -1.2096158393974727, 'contact_force_max': 0.8012469231215186, 'learning_rate': 1.1477509776848969e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9592509593792616, 'gae_lambda': 0.9523024779675273, 'clip_range': 0.07005722621822287, 'ent_coef': 0.06945028598306861}. Best is trial 10 with value: 2477.649387068674.\n",
      "[I 2025-02-18 21:37:44,958] Trial 11 finished with value: 2494.8594861353936 and parameters: {'reset_noise_scale': 0.14465768884466512, 'forward_reward_weight': 1.6020770353772131, 'ctrl_cost_weight': 1.466729195817967, 'healthy_reward': 2.498064436732048, 'contact_cost_weight': 7.75011501632955e-05, 'healthy_z_lower': 0.11996822094014307, 'healthy_z_upper': 1.397065377748852, 'contact_force_min': -1.209168627487126, 'contact_force_max': 0.8188655721495939, 'learning_rate': 1.2057002098814803e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9592453908857448, 'gae_lambda': 0.9513910567305441, 'clip_range': 0.07526032253866288, 'ent_coef': 0.06782270460174225}. Best is trial 11 with value: 2494.8594861353936.\n",
      "[I 2025-02-18 21:38:58,228] Trial 12 finished with value: 2498.3136826099453 and parameters: {'reset_noise_scale': 0.1306706901537252, 'forward_reward_weight': 1.6030055079381176, 'ctrl_cost_weight': 1.4428425696306015, 'healthy_reward': 2.495554356305477, 'contact_cost_weight': 3.743004057282231e-05, 'healthy_z_lower': 0.10984583071007484, 'healthy_z_upper': 1.3910181263133983, 'contact_force_min': -1.2241745773602117, 'contact_force_max': 0.8016279998428124, 'learning_rate': 1.138802370315244e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9598901939626853, 'gae_lambda': 0.95009141789242, 'clip_range': 0.0705411137215337, 'ent_coef': 0.0749718783021142}. Best is trial 12 with value: 2498.3136826099453.\n",
      "[I 2025-02-18 21:40:06,995] Trial 13 finished with value: 2524.329217849262 and parameters: {'reset_noise_scale': 0.13590687927306733, 'forward_reward_weight': 1.6005197976563248, 'ctrl_cost_weight': 1.4598400575335593, 'healthy_reward': 2.497320689026318, 'contact_cost_weight': 2.8846068611312874e-05, 'healthy_z_lower': 0.15039426384051213, 'healthy_z_upper': 1.25635469714309, 'contact_force_min': -1.2140586348882698, 'contact_force_max': 0.8039093837175783, 'learning_rate': 0.0008109622878597958, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9582744052771575, 'gae_lambda': 0.9572182056535408, 'clip_range': 0.07438019980681113, 'ent_coef': 0.07414722378307775}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:41:11,999] Trial 14 finished with value: 2346.1351892583293 and parameters: {'reset_noise_scale': 0.11458839259798109, 'forward_reward_weight': 1.643046470118347, 'ctrl_cost_weight': 1.5911664593684312, 'healthy_reward': 2.366906765365525, 'contact_cost_weight': 3.3006251873241206e-05, 'healthy_z_lower': 0.15659848266986748, 'healthy_z_upper': 1.250863829412288, 'contact_force_min': -1.2168865340035606, 'contact_force_max': 0.8958045523800133, 'learning_rate': 0.0007766329407018705, 'n_steps': 8192, 'batch_size': 1024, 'gamma': 0.9598022264351597, 'gae_lambda': 0.9573500459437209, 'clip_range': 0.07221476995005419, 'ent_coef': 0.08234192207775089}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:42:17,224] Trial 15 finished with value: 2333.1166000021994 and parameters: {'reset_noise_scale': 0.22302792116832088, 'forward_reward_weight': 1.6421045327047497, 'ctrl_cost_weight': 1.3997340027938343, 'healthy_reward': 2.429497448403725, 'contact_cost_weight': 3.09580842981402e-05, 'healthy_z_lower': 0.15605552552175206, 'healthy_z_upper': 1.2710471752273589, 'contact_force_min': -1.2505936452039816, 'contact_force_max': 0.831929369700366, 'learning_rate': 0.0009011423382790537, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9398212452157093, 'gae_lambda': 0.957098388352906, 'clip_range': 0.09751526660680396, 'ent_coef': 0.08165005544755251}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:43:24,522] Trial 16 finished with value: 2170.365242522756 and parameters: {'reset_noise_scale': 0.009684910696718896, 'forward_reward_weight': 1.6522150263375608, 'ctrl_cost_weight': 1.516381848370164, 'healthy_reward': 2.105903766133508, 'contact_cost_weight': 3.681842747275086e-05, 'healthy_z_lower': 0.3017459112363966, 'healthy_z_upper': 1.2886963951864172, 'contact_force_min': -1.1869604473969175, 'contact_force_max': 0.846233847655462, 'learning_rate': 0.00032201712984799403, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9556002977865627, 'gae_lambda': 0.957098870804941, 'clip_range': 0.0516758820574251, 'ent_coef': 0.05710251513365913}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:44:27,764] Trial 17 finished with value: 2325.928267003245 and parameters: {'reset_noise_scale': 0.13168257556419055, 'forward_reward_weight': 1.6017934625472006, 'ctrl_cost_weight': 1.4527364153028328, 'healthy_reward': 2.332659657156266, 'contact_cost_weight': 4.500756697005426e-05, 'healthy_z_lower': 0.18524584937821378, 'healthy_z_upper': 1.2206234537056322, 'contact_force_min': -1.1755675178308536, 'contact_force_max': 0.801994062313549, 'learning_rate': 2.6363962222812657e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9406676125348299, 'gae_lambda': 0.9682974937065065, 'clip_range': 0.09688986840873297, 'ent_coef': 0.07743967737065298}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:45:38,853] Trial 18 finished with value: 2294.8896768337877 and parameters: {'reset_noise_scale': 0.21248650652044432, 'forward_reward_weight': 1.6529600096701116, 'ctrl_cost_weight': 1.3382704602441102, 'healthy_reward': 2.272220541600889, 'contact_cost_weight': 2.310515322484491e-05, 'healthy_z_lower': 0.13298708847802398, 'healthy_z_upper': 1.1424199808471387, 'contact_force_min': -1.2433759652205267, 'contact_force_max': 0.9131235582509385, 'learning_rate': 0.0004767711474149571, 'n_steps': 6144, 'batch_size': 1024, 'gamma': 0.9560416443185131, 'gae_lambda': 0.9556178537089619, 'clip_range': 0.03712905522873005, 'ent_coef': 0.04816187830135917}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:46:50,021] Trial 19 finished with value: 2457.844110410934 and parameters: {'reset_noise_scale': 0.05978460799011745, 'forward_reward_weight': 1.805125820676768, 'ctrl_cost_weight': 1.5294519394673505, 'healthy_reward': 2.4504055760098935, 'contact_cost_weight': 2.0911908972050603e-05, 'healthy_z_lower': 0.2969294184903841, 'healthy_z_upper': 1.2430315427542251, 'contact_force_min': -1.0965050387868265, 'contact_force_max': 0.9853505764596022, 'learning_rate': 2.0823972349245626e-05, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9497137370575216, 'gae_lambda': 0.9597900244909507, 'clip_range': 0.10785669168998883, 'ent_coef': 0.09764937093179547}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:47:58,192] Trial 20 finished with value: 2437.9100456999777 and parameters: {'reset_noise_scale': 0.29678320577950945, 'forward_reward_weight': 1.736458138940091, 'ctrl_cost_weight': 1.3633399166054403, 'healthy_reward': 2.497857392163614, 'contact_cost_weight': 4.627648076782761e-05, 'healthy_z_lower': 0.18306890446592475, 'healthy_z_upper': 1.3160340595530229, 'contact_force_min': -1.2498273006674214, 'contact_force_max': 0.8624002191361406, 'learning_rate': 4.899744200415635e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9530456286627768, 'gae_lambda': 0.9687601009785554, 'clip_range': 0.15042644823331172, 'ent_coef': 0.07501429027855873}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:49:10,267] Trial 21 finished with value: 2500.840063824126 and parameters: {'reset_noise_scale': 0.14548356490885375, 'forward_reward_weight': 1.6214685508761282, 'ctrl_cost_weight': 1.4718705361529762, 'healthy_reward': 2.4996086829451953, 'contact_cost_weight': 7.134820230816006e-05, 'healthy_z_lower': 0.12959652759147736, 'healthy_z_upper': 1.3915384884829698, 'contact_force_min': -1.213763520250042, 'contact_force_max': 0.8209234837013929, 'learning_rate': 1.0065940016886787e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9579999487342508, 'gae_lambda': 0.9501007580058911, 'clip_range': 0.07638621384577585, 'ent_coef': 0.06799456121406829}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:50:18,591] Trial 22 finished with value: 2407.2143770984194 and parameters: {'reset_noise_scale': 0.1679324578518604, 'forward_reward_weight': 1.6269275413498725, 'ctrl_cost_weight': 1.4340375087507322, 'healthy_reward': 2.4020577385907456, 'contact_cost_weight': 4.040048402746845e-05, 'healthy_z_lower': 0.13853724765047748, 'healthy_z_upper': 1.37381554655479, 'contact_force_min': -1.1656031546967165, 'contact_force_max': 0.826423679425829, 'learning_rate': 1.8388937937281618e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9571536181450621, 'gae_lambda': 0.9501215100843385, 'clip_range': 0.08417426536518234, 'ent_coef': 0.060109781743977034}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:51:27,911] Trial 23 finished with value: 2495.5425318631032 and parameters: {'reset_noise_scale': 0.11268556831323143, 'forward_reward_weight': 1.6662663321172946, 'ctrl_cost_weight': 1.4908504290885118, 'healthy_reward': 2.497280072241679, 'contact_cost_weight': 6.921454614841731e-05, 'healthy_z_lower': 0.13199174042573175, 'healthy_z_upper': 1.3374709630846922, 'contact_force_min': -1.2316394356929092, 'contact_force_max': 0.8000376567914462, 'learning_rate': 1.6906252852949377e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9569858673582005, 'gae_lambda': 0.9547295592314557, 'clip_range': 0.05530196092704505, 'ent_coef': 0.08661584064486572}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:52:44,281] Trial 24 finished with value: 2460.4249611047157 and parameters: {'reset_noise_scale': 0.15443967744342657, 'forward_reward_weight': 1.626845162918709, 'ctrl_cost_weight': 1.420911923982044, 'healthy_reward': 2.461966357770115, 'contact_cost_weight': 2.4526707703371127e-05, 'healthy_z_lower': 0.10022625210515332, 'healthy_z_upper': 1.3719876933131765, 'contact_force_min': -1.2719120341115069, 'contact_force_max': 0.8692823168245494, 'learning_rate': 2.9140697945058978e-05, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9531529284606542, 'gae_lambda': 0.950073507142938, 'clip_range': 0.11094748072875785, 'ent_coef': 0.06964601764609292}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:53:55,345] Trial 25 finished with value: 2429.8671822297574 and parameters: {'reset_noise_scale': 0.11282917329519634, 'forward_reward_weight': 1.6246848841135357, 'ctrl_cost_weight': 1.5532848463805897, 'healthy_reward': 2.4224745026734507, 'contact_cost_weight': 4.7788844974886065e-05, 'healthy_z_lower': 0.17616426134554417, 'healthy_z_upper': 1.3992008117867323, 'contact_force_min': -1.19297793891789, 'contact_force_max': 0.8304395303046336, 'learning_rate': 1.0944431025728183e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9597949845433169, 'gae_lambda': 0.9546511868765167, 'clip_range': 0.032399435667560976, 'ent_coef': 0.056616427578231945}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:55:08,977] Trial 26 finished with value: 2339.9243623473744 and parameters: {'reset_noise_scale': 0.19719579265806464, 'forward_reward_weight': 1.6005310475003167, 'ctrl_cost_weight': 1.4900568938752137, 'healthy_reward': 2.3713022819582004, 'contact_cost_weight': 5.3924196909988946e-05, 'healthy_z_lower': 0.20635389094598827, 'healthy_z_upper': 1.3303847625769871, 'contact_force_min': -1.121710776560791, 'contact_force_max': 0.9228897759478569, 'learning_rate': 0.00019850595849921977, 'n_steps': 12288, 'batch_size': 256, 'gamma': 0.9427351222249132, 'gae_lambda': 0.9591018476830657, 'clip_range': 0.06210824193613487, 'ent_coef': 0.074224496973803}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:56:20,646] Trial 27 finished with value: 2360.449260036846 and parameters: {'reset_noise_scale': 0.08944996937964772, 'forward_reward_weight': 1.66932385039751, 'ctrl_cost_weight': 1.4490511368445407, 'healthy_reward': 2.4387244644273127, 'contact_cost_weight': 6.379953297494932e-05, 'healthy_z_lower': 0.14585958235654134, 'healthy_z_upper': 1.286706427064585, 'contact_force_min': -1.2672574474899194, 'contact_force_max': 0.8500796623640243, 'learning_rate': 0.0005831290742991909, 'n_steps': 8192, 'batch_size': 1024, 'gamma': 0.9356972605676989, 'gae_lambda': 0.9532684598035133, 'clip_range': 0.08776461829241511, 'ent_coef': 0.08983152145684536}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:57:26,329] Trial 28 finished with value: 2481.0147874570016 and parameters: {'reset_noise_scale': 0.1559388992519453, 'forward_reward_weight': 1.8892181450820236, 'ctrl_cost_weight': 1.3960444238520842, 'healthy_reward': 2.4726596169481856, 'contact_cost_weight': 8.267321564347504e-05, 'healthy_z_lower': 0.12360958911987131, 'healthy_z_upper': 1.374231064943087, 'contact_force_min': -1.157802751549585, 'contact_force_max': 0.884270386594151, 'learning_rate': 2.190011020037413e-05, 'n_steps': 10240, 'batch_size': 512, 'gamma': 0.9576012234034221, 'gae_lambda': 0.9503467064588269, 'clip_range': 0.02093424757945956, 'ent_coef': 0.042097699043531646}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:58:39,829] Trial 29 finished with value: 2363.2033352478347 and parameters: {'reset_noise_scale': 0.08377252539730054, 'forward_reward_weight': 1.7149532687082103, 'ctrl_cost_weight': 1.5028691617007044, 'healthy_reward': 2.3618297035473765, 'contact_cost_weight': 2.8073364548894354e-05, 'healthy_z_lower': 0.16769570692223856, 'healthy_z_upper': 1.3382054953746758, 'contact_force_min': -1.2279905175624377, 'contact_force_max': 0.828637739159146, 'learning_rate': 0.00019032837610276807, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.9542271206509225, 'gae_lambda': 0.9715331000886074, 'clip_range': 0.11438009913724956, 'ent_coef': 0.021053669285985732}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 21:59:56,078] Trial 30 finished with value: 2426.22963410071 and parameters: {'reset_noise_scale': 0.25216876368569163, 'forward_reward_weight': 1.63317483383432, 'ctrl_cost_weight': 1.3109982747789768, 'healthy_reward': 2.4720731416888686, 'contact_cost_weight': 1.703835307091677e-05, 'healthy_z_lower': 0.2237535759327096, 'healthy_z_upper': 1.2932246654297037, 'contact_force_min': -1.20215187843641, 'contact_force_max': 0.818479037427998, 'learning_rate': 3.8685782788371984e-05, 'n_steps': 4096, 'batch_size': 256, 'gamma': 0.9513787484791066, 'gae_lambda': 0.955631801651817, 'clip_range': 0.15783847805258813, 'ent_coef': 0.06405806858032782}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:01:11,820] Trial 31 finished with value: 2503.5376352791363 and parameters: {'reset_noise_scale': 0.1185782133158589, 'forward_reward_weight': 1.6655897275491913, 'ctrl_cost_weight': 1.4744008989348056, 'healthy_reward': 2.4999339506519287, 'contact_cost_weight': 6.755926203328501e-05, 'healthy_z_lower': 0.12261963103447943, 'healthy_z_upper': 1.338153504342793, 'contact_force_min': -1.2366374308034418, 'contact_force_max': 0.8012012543795676, 'learning_rate': 1.6329372540269106e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9573986424069303, 'gae_lambda': 0.9540907137913961, 'clip_range': 0.05966697553840353, 'ent_coef': 0.08827835078439907}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:02:26,581] Trial 32 finished with value: 2506.446479405214 and parameters: {'reset_noise_scale': 0.1283081527131348, 'forward_reward_weight': 1.6709803214981531, 'ctrl_cost_weight': 1.539079929717412, 'healthy_reward': 2.4971933383854563, 'contact_cost_weight': 6.727970746046286e-05, 'healthy_z_lower': 0.11825421399732186, 'healthy_z_upper': 1.363203110449479, 'contact_force_min': -1.2304912000679258, 'contact_force_max': 0.8123170385821729, 'learning_rate': 1.5041474290250143e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9579030928667872, 'gae_lambda': 0.9524495220231299, 'clip_range': 0.08456132261762289, 'ent_coef': 0.0843295860406057}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:03:49,091] Trial 33 finished with value: 2442.0863537208115 and parameters: {'reset_noise_scale': 0.12639318027622318, 'forward_reward_weight': 1.67766286979251, 'ctrl_cost_weight': 1.5464880105768999, 'healthy_reward': 2.4386623993845538, 'contact_cost_weight': 6.892918482337297e-05, 'healthy_z_lower': 0.1500948663636473, 'healthy_z_upper': 1.3567356213563306, 'contact_force_min': -1.266937309128946, 'contact_force_max': 0.848797366638486, 'learning_rate': 1.5084894930338384e-05, 'n_steps': 12288, 'batch_size': 256, 'gamma': 0.9547263454756848, 'gae_lambda': 0.9537896799007539, 'clip_range': 0.08972680314817946, 'ent_coef': 0.0930291892681253}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:05:07,449] Trial 34 finished with value: 2463.841250159662 and parameters: {'reset_noise_scale': 0.16732161062924225, 'forward_reward_weight': 1.699908055219851, 'ctrl_cost_weight': 1.5881147513519802, 'healthy_reward': 2.453141369565231, 'contact_cost_weight': 8.602502974287295e-05, 'healthy_z_lower': 0.12313392312438234, 'healthy_z_upper': 1.3464716908460872, 'contact_force_min': -1.0716671978198997, 'contact_force_max': 0.81804880905508, 'learning_rate': 1.0021709588203105e-05, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9485334873733466, 'gae_lambda': 0.9522106553829458, 'clip_range': 0.0586625072836536, 'ent_coef': 0.08512517741787717}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:06:23,145] Trial 35 finished with value: 2466.9258165151873 and parameters: {'reset_noise_scale': 0.10013811917018459, 'forward_reward_weight': 1.6555354853314563, 'ctrl_cost_weight': 1.4722805667503218, 'healthy_reward': 2.4782087821451952, 'contact_cost_weight': 6.87440280107923e-05, 'healthy_z_lower': 0.18330145161719313, 'healthy_z_upper': 1.370666436032769, 'contact_force_min': -1.1817906059224506, 'contact_force_max': 0.8385935165268303, 'learning_rate': 7.231276573361892e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9580057704563734, 'gae_lambda': 0.9582491931040648, 'clip_range': 0.0780839006181242, 'ent_coef': 0.09158695824782523}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:07:39,164] Trial 36 finished with value: 2402.7204036805033 and parameters: {'reset_noise_scale': 0.04792276033840155, 'forward_reward_weight': 1.6942827979149107, 'ctrl_cost_weight': 1.5479493004735145, 'healthy_reward': 2.3988832858715523, 'contact_cost_weight': 8.612219066138722e-05, 'healthy_z_lower': 0.1406926802016343, 'healthy_z_upper': 1.3243791538147407, 'contact_force_min': -1.243616999502639, 'contact_force_max': 0.8599877502803639, 'learning_rate': 2.521214248248177e-05, 'n_steps': 12288, 'batch_size': 2048, 'gamma': 0.9522495368013274, 'gae_lambda': 0.9561705559392926, 'clip_range': 0.04608664140488891, 'ent_coef': 0.08079496178974105}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:08:56,643] Trial 37 finished with value: 2258.2292580601284 and parameters: {'reset_noise_scale': 0.0822740756764372, 'forward_reward_weight': 1.717696012094946, 'ctrl_cost_weight': 1.5714838459260732, 'healthy_reward': 2.2495537106256958, 'contact_cost_weight': 6.448702839940175e-05, 'healthy_z_lower': 0.2672210767672057, 'healthy_z_upper': 1.241560817383543, 'contact_force_min': -1.2749914038884298, 'contact_force_max': 1.0161787081979303, 'learning_rate': 5.705871178981365e-05, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9552466445676501, 'gae_lambda': 0.9614245293442818, 'clip_range': 0.12037489795018064, 'ent_coef': 0.05170099642454132}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:10:10,683] Trial 38 finished with value: 2335.593815789409 and parameters: {'reset_noise_scale': 0.18862991254442024, 'forward_reward_weight': 1.7764477553324798, 'ctrl_cost_weight': 1.4959035241580416, 'healthy_reward': 2.334259710484791, 'contact_cost_weight': 9.812695724298658e-05, 'healthy_z_lower': 0.19896935818805153, 'healthy_z_upper': 1.2679049215726936, 'contact_force_min': -1.234645524074329, 'contact_force_max': 0.8150123918913005, 'learning_rate': 3.5193336804487085e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9578568173669474, 'gae_lambda': 0.9535238604320806, 'clip_range': 0.10270617198185625, 'ent_coef': 0.0658169186945884}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:11:24,044] Trial 39 finished with value: 2427.1514610270165 and parameters: {'reset_noise_scale': 0.15600207357557894, 'forward_reward_weight': 1.669517093341591, 'ctrl_cost_weight': 1.5295521887753614, 'healthy_reward': 2.4215379863057382, 'contact_cost_weight': 5.907537726569657e-05, 'healthy_z_lower': 0.16969820606799663, 'healthy_z_upper': 1.2210210701230422, 'contact_force_min': -1.1981128428775958, 'contact_force_max': 0.8807246138392651, 'learning_rate': 8.607145360555056e-05, 'n_steps': 8192, 'batch_size': 1024, 'gamma': 0.9561310323016724, 'gae_lambda': 0.9517882432975796, 'clip_range': 0.06369666186762285, 'ent_coef': 0.09955725156326535}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:12:31,930] Trial 40 finished with value: 2143.8804507175587 and parameters: {'reset_noise_scale': 0.0747157565938385, 'forward_reward_weight': 1.6161630815916759, 'ctrl_cost_weight': 1.4143045023185075, 'healthy_reward': 2.1609217170557065, 'contact_cost_weight': 7.224497665060483e-05, 'healthy_z_lower': 0.11992336006752154, 'healthy_z_upper': 1.301860875063028, 'contact_force_min': -1.2871978226927183, 'contact_force_max': 0.921799913458816, 'learning_rate': 0.00016819672394882693, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9541557715600332, 'gae_lambda': 0.9604950600744551, 'clip_range': 0.03646028713772027, 'ent_coef': 0.08595668843296377}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:13:53,234] Trial 41 finished with value: 2480.8639346848117 and parameters: {'reset_noise_scale': 0.12326266867292425, 'forward_reward_weight': 1.641549294784083, 'ctrl_cost_weight': 1.4510350993932242, 'healthy_reward': 2.4877560352445647, 'contact_cost_weight': 3.971300727795768e-05, 'healthy_z_lower': 0.11515102176657223, 'healthy_z_upper': 1.3830606775038163, 'contact_force_min': -1.2217920422125772, 'contact_force_max': 0.8085146379683413, 'learning_rate': 1.3975160431057583e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9578070309575621, 'gae_lambda': 0.951361573269155, 'clip_range': 0.06629295329726471, 'ent_coef': 0.07274561403944856}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:15:03,040] Trial 42 finished with value: 2454.4778476415077 and parameters: {'reset_noise_scale': 0.14085864874937762, 'forward_reward_weight': 1.6314978979658776, 'ctrl_cost_weight': 1.4704761132720856, 'healthy_reward': 2.4494406099965147, 'contact_cost_weight': 5.170923788441272e-05, 'healthy_z_lower': 0.10830294359525698, 'healthy_z_upper': 1.3584042372426717, 'contact_force_min': -1.2517456222464978, 'contact_force_max': 0.8371951838482066, 'learning_rate': 1.8805966565919795e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9587557207945464, 'gae_lambda': 0.9543270045677229, 'clip_range': 0.08015730894705729, 'ent_coef': 0.08049278948787045}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:16:12,418] Trial 43 finished with value: 2486.4506211907665 and parameters: {'reset_noise_scale': 0.10245696029608005, 'forward_reward_weight': 1.616274670478052, 'ctrl_cost_weight': 1.4446880297086255, 'healthy_reward': 2.4825125111652984, 'contact_cost_weight': 5.985897045626358e-05, 'healthy_z_lower': 0.1343530621264285, 'healthy_z_upper': 1.3854472081303417, 'contact_force_min': -1.2191727339886476, 'contact_force_max': 0.8000100917481332, 'learning_rate': 1.3461034910884007e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9599564165616108, 'gae_lambda': 0.9526912025551403, 'clip_range': 0.0907594447031177, 'ent_coef': 0.07712302473210267}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:17:36,246] Trial 44 finished with value: 2492.805829239846 and parameters: {'reset_noise_scale': 0.17313486025529637, 'forward_reward_weight': 1.684226012633179, 'ctrl_cost_weight': 1.3738883286039283, 'healthy_reward': 2.4953384997949954, 'contact_cost_weight': 7.391782763865622e-05, 'healthy_z_lower': 0.15422753366609163, 'healthy_z_upper': 1.3516801517288635, 'contact_force_min': -1.171060539056215, 'contact_force_max': 0.8177002234690859, 'learning_rate': 1.033816563079264e-05, 'n_steps': 12288, 'batch_size': 256, 'gamma': 0.9563792778202267, 'gae_lambda': 0.9788587024958461, 'clip_range': 0.048211395521051026, 'ent_coef': 0.09406951941615405}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:18:45,319] Trial 45 finished with value: 2459.075209764143 and parameters: {'reset_noise_scale': 0.1280265114059687, 'forward_reward_weight': 1.6092375735638114, 'ctrl_cost_weight': 1.5109770051287819, 'healthy_reward': 2.4615499941743457, 'contact_cost_weight': 8.185939117305763e-05, 'healthy_z_lower': 0.11103750151707813, 'healthy_z_upper': 1.1071824360401188, 'contact_force_min': -1.2100020059620409, 'contact_force_max': 0.8564251521704356, 'learning_rate': 1.6401702620645623e-05, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9583844553500532, 'gae_lambda': 0.9513435127402008, 'clip_range': 0.07053616304323687, 'ent_coef': 0.07125101337795779}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:20:00,840] Trial 46 finished with value: 2476.2575972679165 and parameters: {'reset_noise_scale': 0.13879722451952123, 'forward_reward_weight': 1.6531923394607382, 'ctrl_cost_weight': 1.4803665483090802, 'healthy_reward': 2.478980084715698, 'contact_cost_weight': 9.024153268175285e-06, 'healthy_z_lower': 0.10053517730104652, 'healthy_z_upper': 1.1727784096353815, 'contact_force_min': -1.261990740740107, 'contact_force_max': 1.0635104671198217, 'learning_rate': 2.3319515516175234e-05, 'n_steps': 6144, 'batch_size': 256, 'gamma': 0.9450697457653613, 'gae_lambda': 0.9526240545062692, 'clip_range': 0.07933389719395655, 'ent_coef': 0.08744852454093055}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:21:19,872] Trial 47 finished with value: 2458.2603040484464 and parameters: {'reset_noise_scale': 0.11287212020977552, 'forward_reward_weight': 1.7330593539764276, 'ctrl_cost_weight': 1.4095744192989357, 'healthy_reward': 2.4542736656347808, 'contact_cost_weight': 3.5593651288258726e-05, 'healthy_z_lower': 0.36627035875738245, 'healthy_z_upper': 1.399703127450802, 'contact_force_min': -1.2358610866287874, 'contact_force_max': 0.812394788539715, 'learning_rate': 0.0003132672689482077, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9489338427166206, 'gae_lambda': 0.9559140585926209, 'clip_range': 0.0017323415363262248, 'ent_coef': 0.07815696212103353}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:22:40,767] Trial 48 finished with value: 2441.7203572252392 and parameters: {'reset_noise_scale': 0.14934883757403442, 'forward_reward_weight': 1.6397249159992702, 'ctrl_cost_weight': 1.252498220416336, 'healthy_reward': 2.432365562460227, 'contact_cost_weight': 4.1189106013519864e-05, 'healthy_z_lower': 0.16414027920595206, 'healthy_z_upper': 1.382388174496104, 'contact_force_min': -1.1458221871791536, 'contact_force_max': 0.8419090323328167, 'learning_rate': 1.2464784890274968e-05, 'n_steps': 12288, 'batch_size': 512, 'gamma': 0.9368137769064467, 'gae_lambda': 0.9581844445670646, 'clip_range': 0.023944369934776034, 'ent_coef': 0.0635268789644876}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:23:53,952] Trial 49 finished with value: 2498.8870833964147 and parameters: {'reset_noise_scale': 0.18154224255813767, 'forward_reward_weight': 1.6170503145202744, 'ctrl_cost_weight': 1.4608787144095832, 'healthy_reward': 2.4976661283728387, 'contact_cost_weight': 2.897332958653674e-05, 'healthy_z_lower': 0.12775690749350793, 'healthy_z_upper': 1.3618898459381703, 'contact_force_min': -1.188314026642258, 'contact_force_max': 0.9663945290664544, 'learning_rate': 3.1255335667107226e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9552119998166424, 'gae_lambda': 0.951113572377268, 'clip_range': 0.09859011552229759, 'ent_coef': 0.0005771112621223776}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:25:07,539] Trial 50 finished with value: 2456.8251422069625 and parameters: {'reset_noise_scale': 0.20152393699696672, 'forward_reward_weight': 1.6176277034538173, 'ctrl_cost_weight': 1.4615013656738023, 'healthy_reward': 2.465529244759308, 'contact_cost_weight': 1.690525852624922e-05, 'healthy_z_lower': 0.12834930179516654, 'healthy_z_upper': 1.3641468113777404, 'contact_force_min': -1.2030804323658655, 'contact_force_max': 0.9848297340598801, 'learning_rate': 2.948153959532194e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9470923684540479, 'gae_lambda': 0.9757285514184592, 'clip_range': 0.09744467484263977, 'ent_coef': 0.024471834416025514}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:26:18,379] Trial 51 finished with value: 2502.6048385758945 and parameters: {'reset_noise_scale': 0.1743569727777721, 'forward_reward_weight': 1.6016214482190663, 'ctrl_cost_weight': 1.4344643548552278, 'healthy_reward': 2.4959190222744203, 'contact_cost_weight': 2.7676464360268157e-05, 'healthy_z_lower': 0.14027092541828262, 'healthy_z_upper': 1.3396720702581573, 'contact_force_min': -1.0007978848365977, 'contact_force_max': 0.9722796379187673, 'learning_rate': 1.4711496963369775e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9563517284379465, 'gae_lambda': 0.9509877051454575, 'clip_range': 0.07158580937421204, 'ent_coef': 0.02007770074448665}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:27:25,874] Trial 52 finished with value: 2500.2509144233663 and parameters: {'reset_noise_scale': 0.18576721898119894, 'forward_reward_weight': 1.6149895130250265, 'ctrl_cost_weight': 1.4272578076345368, 'healthy_reward': 2.4985731000754723, 'contact_cost_weight': 1.3290042227879928e-06, 'healthy_z_lower': 0.15147104919879506, 'healthy_z_upper': 1.321566640053648, 'contact_force_min': -1.183088613333332, 'contact_force_max': 0.9565936220801172, 'learning_rate': 1.9579520176079284e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9555041457087853, 'gae_lambda': 0.950932137312952, 'clip_range': 0.0936878074048772, 'ent_coef': 0.0002226189374946036}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:28:40,353] Trial 53 finished with value: 2481.7233882228284 and parameters: {'reset_noise_scale': 0.22913563298639028, 'forward_reward_weight': 1.6579343142033702, 'ctrl_cost_weight': 1.4311138459683677, 'healthy_reward': 2.4813113969027727, 'contact_cost_weight': 5.9885544031848205e-06, 'healthy_z_lower': 0.14712506954693114, 'healthy_z_upper': 1.3140430855575513, 'contact_force_min': -1.0039705721927372, 'contact_force_max': 0.9473268036666579, 'learning_rate': 1.9263201830095258e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.956912358757238, 'gae_lambda': 0.9545272725235011, 'clip_range': 0.19173177075353523, 'ent_coef': 0.014890923450220993}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:29:51,210] Trial 54 finished with value: 2480.2917795426943 and parameters: {'reset_noise_scale': 0.16487344206399357, 'forward_reward_weight': 1.6003360124210093, 'ctrl_cost_weight': 1.4026043141781006, 'healthy_reward': 2.4835821955645527, 'contact_cost_weight': 1.230827635966834e-06, 'healthy_z_lower': 0.22181022691104796, 'healthy_z_upper': 1.3428768052555986, 'contact_force_min': -1.0399041097393023, 'contact_force_max': 0.9988781476590631, 'learning_rate': 1.532748776474609e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.951246575565884, 'gae_lambda': 0.9530379672868461, 'clip_range': 0.05771554641760371, 'ent_coef': 0.028593674915536504}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:30:54,411] Trial 55 finished with value: 2465.245349461035 and parameters: {'reset_noise_scale': 0.1800524357976903, 'forward_reward_weight': 1.6451171270357778, 'ctrl_cost_weight': 1.3819505106741226, 'healthy_reward': 2.465554957062507, 'contact_cost_weight': 1.4486840533112885e-05, 'healthy_z_lower': 0.19680806825570668, 'healthy_z_upper': 1.3218924023945482, 'contact_force_min': -1.0772873539256638, 'contact_force_max': 0.9714305182151243, 'learning_rate': 1.220889520138191e-05, 'n_steps': 10240, 'batch_size': 1024, 'gamma': 0.9536340892614833, 'gae_lambda': 0.9510484042033815, 'clip_range': 0.07444775199475037, 'ent_coef': 0.015111341405135918}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:32:13,845] Trial 56 finished with value: 2435.261098735432 and parameters: {'reset_noise_scale': 0.20413677717017495, 'forward_reward_weight': 1.6267529258372473, 'ctrl_cost_weight': 1.5262923240061308, 'healthy_reward': 2.4467050108120962, 'contact_cost_weight': 5.6450427409719924e-05, 'healthy_z_lower': 0.16301165027814074, 'healthy_z_upper': 1.3002925685799618, 'contact_force_min': -1.117954983977176, 'contact_force_max': 1.0188017143950945, 'learning_rate': 4.716058542541535e-05, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9561973234101401, 'gae_lambda': 0.9653691964754099, 'clip_range': 0.08326191845551614, 'ent_coef': 0.0344745908228998}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:33:33,008] Trial 57 finished with value: 2266.9522948956987 and parameters: {'reset_noise_scale': 0.26180277873602753, 'forward_reward_weight': 1.6133621791963366, 'ctrl_cost_weight': 1.485558046817125, 'healthy_reward': 2.3135974001894968, 'contact_cost_weight': 6.637524966639753e-05, 'healthy_z_lower': 0.14103930212081725, 'healthy_z_upper': 1.2760395966523834, 'contact_force_min': -1.2137230404965331, 'contact_force_max': 0.9290182183283968, 'learning_rate': 0.0001435694611700608, 'n_steps': 12288, 'batch_size': 256, 'gamma': 0.9585982073940048, 'gae_lambda': 0.952193449796521, 'clip_range': 0.10485068585416121, 'ent_coef': 0.04175404628482376}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:34:39,860] Trial 58 finished with value: 2503.449614652943 and parameters: {'reset_noise_scale': 0.13780175909961845, 'forward_reward_weight': 1.8189747534747012, 'ctrl_cost_weight': 1.428089576708476, 'healthy_reward': 2.4994794105848976, 'contact_cost_weight': 6.447603828174881e-06, 'healthy_z_lower': 0.29579950972772895, 'healthy_z_upper': 1.3315861380426677, 'contact_force_min': -1.1597323913180044, 'contact_force_max': 0.9333238743793545, 'learning_rate': 0.0003746281819939336, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9521461059448446, 'gae_lambda': 0.9564634951352478, 'clip_range': 0.09006426630709839, 'ent_coef': 0.00020324219122715845}. Best is trial 13 with value: 2524.329217849262.\n",
      "[I 2025-02-18 22:35:42,768] Trial 59 finished with value: 2550.1617701530704 and parameters: {'reset_noise_scale': 0.13736775859680747, 'forward_reward_weight': 1.8363206932106197, 'ctrl_cost_weight': 1.3572635424755846, 'healthy_reward': 2.413546587122839, 'contact_cost_weight': 7.827727515831175e-05, 'healthy_z_lower': 0.3225374826078632, 'healthy_z_upper': 1.1285943288518527, 'contact_force_min': -1.1536921914284146, 'contact_force_max': 0.9380502575698462, 'learning_rate': 0.00039213941455949037, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9517911218645528, 'gae_lambda': 0.9629208767589895, 'clip_range': 0.0415221971733879, 'ent_coef': 0.006009476113197319}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:36:44,035] Trial 60 finished with value: 2423.471979447409 and parameters: {'reset_noise_scale': 0.10518598374294236, 'forward_reward_weight': 1.8284347682289217, 'ctrl_cost_weight': 1.361367280326215, 'healthy_reward': 2.4132235245353297, 'contact_cost_weight': 7.57901386005356e-05, 'healthy_z_lower': 0.30593348651891494, 'healthy_z_upper': 1.1232784645187668, 'contact_force_min': -1.133893848480369, 'contact_force_max': 0.934980958770706, 'learning_rate': 0.0004014809582496028, 'n_steps': 6144, 'batch_size': 2048, 'gamma': 0.9512430039478383, 'gae_lambda': 0.9616229624021101, 'clip_range': 0.011446850818172621, 'ent_coef': 0.009219340869308354}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:37:50,586] Trial 61 finished with value: 2494.077063099121 and parameters: {'reset_noise_scale': 0.13744218352033974, 'forward_reward_weight': 1.816307468651647, 'ctrl_cost_weight': 1.352570431535297, 'healthy_reward': 2.488632647454492, 'contact_cost_weight': 7.981159690032997e-05, 'healthy_z_lower': 0.3213932347413046, 'healthy_z_upper': 1.1816097087914983, 'contact_force_min': -1.1533914442560245, 'contact_force_max': 0.9022982713071663, 'learning_rate': 0.0006702571036750347, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9525863231428414, 'gae_lambda': 0.9626298447660452, 'clip_range': 0.05323419217961342, 'ent_coef': 0.0031726602443126993}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:38:58,329] Trial 62 finished with value: 2500.1002917931105 and parameters: {'reset_noise_scale': 0.12149519804854837, 'forward_reward_weight': 1.7951306757194478, 'ctrl_cost_weight': 1.3074032969585483, 'healthy_reward': 2.468376112307774, 'contact_cost_weight': 8.785563987899082e-05, 'healthy_z_lower': 0.3280065251133605, 'healthy_z_upper': 1.1587021432714015, 'contact_force_min': -1.0987727719201217, 'contact_force_max': 1.0001874495929495, 'learning_rate': 0.0008708870677368512, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9573619712683986, 'gae_lambda': 0.9568762645978984, 'clip_range': 0.04273699596806284, 'ent_coef': 0.005411334397783442}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:40:08,226] Trial 63 finished with value: 2369.58042336901 and parameters: {'reset_noise_scale': 0.16098094245371186, 'forward_reward_weight': 1.8443333635368324, 'ctrl_cost_weight': 1.3384322613117143, 'healthy_reward': 2.3820202129770607, 'contact_cost_weight': 6.132861129811272e-05, 'healthy_z_lower': 0.2895544768067333, 'healthy_z_upper': 1.1973899797695822, 'contact_force_min': -1.1614690668112688, 'contact_force_max': 1.0334926281854895, 'learning_rate': 0.0005118441866827633, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9589804742025524, 'gae_lambda': 0.9641777980154959, 'clip_range': 0.06679398180670575, 'ent_coef': 0.01216476274357158}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:41:24,784] Trial 64 finished with value: 2337.452601633941 and parameters: {'reset_noise_scale': 0.1454343345237764, 'forward_reward_weight': 1.8856693067490613, 'ctrl_cost_weight': 1.3947478575385477, 'healthy_reward': 2.349624559679602, 'contact_cost_weight': 7.239033092937032e-05, 'healthy_z_lower': 0.34472049043117026, 'healthy_z_upper': 1.2166971253797174, 'contact_force_min': -1.1191692421122403, 'contact_force_max': 0.9373023848023744, 'learning_rate': 0.0002301891705211046, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9545428743048183, 'gae_lambda': 0.9551395644280668, 'clip_range': 0.08407836570754101, 'ent_coef': 0.019170421698499862}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:42:39,521] Trial 65 finished with value: 2461.0896748978644 and parameters: {'reset_noise_scale': 0.13428327910950888, 'forward_reward_weight': 1.8650875191233822, 'ctrl_cost_weight': 1.4395588562013812, 'healthy_reward': 2.456651920577509, 'contact_cost_weight': 9.136798591425573e-05, 'healthy_z_lower': 0.2689528906723761, 'healthy_z_upper': 1.3345653644953768, 'contact_force_min': -1.1344031794114076, 'contact_force_max': 0.8256441817943819, 'learning_rate': 0.0003726235834504572, 'n_steps': 8192, 'batch_size': 2048, 'gamma': 0.9567556512350178, 'gae_lambda': 0.9588647542720934, 'clip_range': 0.04122068084145616, 'ent_coef': 0.00583381280971062}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:43:46,899] Trial 66 finished with value: 2418.6931972454818 and parameters: {'reset_noise_scale': 0.14725830215022206, 'forward_reward_weight': 1.7691666153415915, 'ctrl_cost_weight': 1.414937579282321, 'healthy_reward': 2.4393774593484085, 'contact_cost_weight': 2.4120519169652957e-05, 'healthy_z_lower': 0.28211811905361367, 'healthy_z_upper': 1.2536385531868157, 'contact_force_min': -1.1737509312976673, 'contact_force_max': 0.9064846650186213, 'learning_rate': 0.0007340631083835376, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9519889820016815, 'gae_lambda': 0.9538293296690034, 'clip_range': 0.07305190843711931, 'ent_coef': 0.01172171914534161}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:44:58,508] Trial 67 finished with value: 2484.053865905404 and parameters: {'reset_noise_scale': 0.11246060725456121, 'forward_reward_weight': 1.848939332696402, 'ctrl_cost_weight': 1.5072843770372204, 'healthy_reward': 2.4854618476582377, 'contact_cost_weight': 2.0383046987036032e-05, 'healthy_z_lower': 0.32263821885821425, 'healthy_z_upper': 1.3488790762634688, 'contact_force_min': -1.1952667931202747, 'contact_force_max': 1.0923483482553813, 'learning_rate': 0.0002450344917298228, 'n_steps': 6144, 'batch_size': 2048, 'gamma': 0.9475216891661944, 'gae_lambda': 0.9674072174075367, 'clip_range': 0.059245901952656235, 'ent_coef': 0.08902140716934827}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:46:22,865] Trial 68 finished with value: 2485.038148079018 and parameters: {'reset_noise_scale': 0.12138648032473394, 'forward_reward_weight': 1.8139404300525945, 'ctrl_cost_weight': 1.3202500389675205, 'healthy_reward': 2.4991383965223237, 'contact_cost_weight': 4.9392746427238694e-05, 'healthy_z_lower': 0.342595255706056, 'healthy_z_upper': 1.330117479525497, 'contact_force_min': -1.2401331553123074, 'contact_force_max': 0.9685849956351392, 'learning_rate': 0.0009745736889878302, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9496228235648552, 'gae_lambda': 0.9568477967426696, 'clip_range': 0.03212203230485933, 'ent_coef': 0.08300612512288508}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:47:06,030] Trial 69 finished with value: 58.315859723731606 and parameters: {'reset_noise_scale': 0.09502614347550503, 'forward_reward_weight': 1.757844132710971, 'ctrl_cost_weight': 1.275761823773222, 'healthy_reward': 2.4728613656821152, 'contact_cost_weight': 7.861125908864889e-05, 'healthy_z_lower': 0.38979404879329166, 'healthy_z_upper': 1.1239672730928096, 'contact_force_min': -1.2566246500892941, 'contact_force_max': 0.8089336839876353, 'learning_rate': 0.000586508883558369, 'n_steps': 8192, 'batch_size': 1024, 'gamma': 0.9535014868559981, 'gae_lambda': 0.9666610417092025, 'clip_range': 0.050928744764407244, 'ent_coef': 0.057574187530060716}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:48:13,179] Trial 70 finished with value: 2406.3100581789577 and parameters: {'reset_noise_scale': 0.17175279861411658, 'forward_reward_weight': 1.8733745189280058, 'ctrl_cost_weight': 1.5619841102239909, 'healthy_reward': 2.4051018830241984, 'contact_cost_weight': 4.373749267310191e-05, 'healthy_z_lower': 0.2544689241847747, 'healthy_z_upper': 1.303364679584814, 'contact_force_min': -1.227407437149863, 'contact_force_max': 0.9807715356439485, 'learning_rate': 9.526913227106929e-05, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9550420887334302, 'gae_lambda': 0.9580880910271965, 'clip_range': 0.08699224315888478, 'ent_coef': 0.06746406691604087}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:49:20,997] Trial 71 finished with value: 2521.838499859683 and parameters: {'reset_noise_scale': 0.18973860617443883, 'forward_reward_weight': 1.8313526001260192, 'ctrl_cost_weight': 1.4298175329525322, 'healthy_reward': 2.4893917465164193, 'contact_cost_weight': 2.9814677531713797e-06, 'healthy_z_lower': 0.31422818943718994, 'healthy_z_upper': 1.3212821384098643, 'contact_force_min': -1.1844782551836501, 'contact_force_max': 0.9485764147522271, 'learning_rate': 0.0002633157315588036, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9558966277624004, 'gae_lambda': 0.9508807796451865, 'clip_range': 0.0937905165604616, 'ent_coef': 0.0014461025290536964}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:50:25,353] Trial 72 finished with value: 2411.733088370059 and parameters: {'reset_noise_scale': 0.21667898131768695, 'forward_reward_weight': 1.8339845502918601, 'ctrl_cost_weight': 1.460595663942791, 'healthy_reward': 2.4875418404125567, 'contact_cost_weight': 1.138600008148937e-05, 'healthy_z_lower': 0.3412304932977701, 'healthy_z_upper': 1.3742718657339037, 'contact_force_min': -1.1692417325167137, 'contact_force_max': 0.9474239033148601, 'learning_rate': 0.0002734126402974638, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9432371118706079, 'gae_lambda': 0.9502691295678043, 'clip_range': 0.11574552630539944, 'ent_coef': 0.007963338122724468}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:51:27,004] Trial 73 finished with value: 2452.8294113748957 and parameters: {'reset_noise_scale': 0.15642590919289737, 'forward_reward_weight': 1.794063958161105, 'ctrl_cost_weight': 1.3906274717940077, 'healthy_reward': 2.472846801087711, 'contact_cost_weight': 4.557269070701323e-06, 'healthy_z_lower': 0.3091867678227697, 'healthy_z_upper': 1.3414352732936903, 'contact_force_min': -1.2071494526248434, 'contact_force_max': 0.9154392358466726, 'learning_rate': 0.0004634003901872645, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9589524328184362, 'gae_lambda': 0.9539424318830084, 'clip_range': 0.06566212084147188, 'ent_coef': 0.002297807776483062}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:52:32,973] Trial 74 finished with value: 2480.838296303172 and parameters: {'reset_noise_scale': 0.1937801964361228, 'forward_reward_weight': 1.818515806652993, 'ctrl_cost_weight': 1.4253240495497133, 'healthy_reward': 2.4265319893374455, 'contact_cost_weight': 5.796222737361341e-06, 'healthy_z_lower': 0.35641922279718463, 'healthy_z_upper': 1.3062766080975532, 'contact_force_min': -1.1778215206934173, 'contact_force_max': 0.9383834960113865, 'learning_rate': 0.00035908091619226014, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9558992820276263, 'gae_lambda': 0.9711935516550494, 'clip_range': 0.07769816886207513, 'ent_coef': 0.09532074942821868}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:53:35,604] Trial 75 finished with value: 2361.9452188078812 and parameters: {'reset_noise_scale': 0.16139909424909693, 'forward_reward_weight': 1.8048689880011755, 'ctrl_cost_weight': 1.481215967960597, 'healthy_reward': 2.461305151893928, 'contact_cost_weight': 1.0380531556431242e-05, 'healthy_z_lower': 0.11713942949744316, 'healthy_z_upper': 1.364484711230588, 'contact_force_min': -1.1420225373585575, 'contact_force_max': 0.9608968396113239, 'learning_rate': 0.00043449274456389825, 'n_steps': 8192, 'batch_size': 256, 'gamma': 0.9573261967141962, 'gae_lambda': 0.9518943300792606, 'clip_range': 0.12444295607409483, 'ent_coef': 0.019232882394120385}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:54:38,169] Trial 76 finished with value: 2454.701778391401 and parameters: {'reset_noise_scale': 0.13244203539766658, 'forward_reward_weight': 1.608463538707061, 'ctrl_cost_weight': 1.4461678380545333, 'healthy_reward': 2.4996162169942795, 'contact_cost_weight': 6.615979647015975e-05, 'healthy_z_lower': 0.28733756348080014, 'healthy_z_upper': 1.2848126274865883, 'contact_force_min': -1.2166216933031324, 'contact_force_max': 0.8881327433279308, 'learning_rate': 0.00011232165560210141, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9541420595026803, 'gae_lambda': 0.9551655341575083, 'clip_range': 0.0940959466839135, 'ent_coef': 0.05091198977270981}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:55:47,953] Trial 77 finished with value: 2447.742044790027 and parameters: {'reset_noise_scale': 0.1495851998947241, 'forward_reward_weight': 1.8257870754258896, 'ctrl_cost_weight': 1.3759959034666462, 'healthy_reward': 2.4447402238245806, 'contact_cost_weight': 3.338640750103708e-05, 'healthy_z_lower': 0.3087139960271826, 'healthy_z_upper': 1.3517881614606915, 'contact_force_min': -1.1578795592684905, 'contact_force_max': 0.870821470395406, 'learning_rate': 1.679112327799176e-05, 'n_steps': 10240, 'batch_size': 256, 'gamma': 0.9581092943855939, 'gae_lambda': 0.9530206438100264, 'clip_range': 0.10356643489580043, 'ent_coef': 0.005274189861027628}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[I 2025-02-18 22:56:50,827] Trial 78 finished with value: 2446.2158212154886 and parameters: {'reset_noise_scale': 0.11886436835090852, 'forward_reward_weight': 1.8525636966463215, 'ctrl_cost_weight': 1.4047356966930276, 'healthy_reward': 2.487022626546893, 'contact_cost_weight': 1.408851434050062e-05, 'healthy_z_lower': 0.317156825883974, 'healthy_z_upper': 1.3904643907055676, 'contact_force_min': -1.1907404507850676, 'contact_force_max': 1.0052792524021976, 'learning_rate': 0.0005430312910277402, 'n_steps': 10240, 'batch_size': 2048, 'gamma': 0.9564649408191656, 'gae_lambda': 0.956122629537956, 'clip_range': 0.13790479296573774, 'ent_coef': 0.003303697490991426}. Best is trial 59 with value: 2550.1617701530704.\n",
      "[W 2025-02-18 22:57:18,896] Trial 79 failed with parameters: {'reset_noise_scale': 0.10810399610902288, 'forward_reward_weight': 1.8391587681722872, 'ctrl_cost_weight': 1.536113428601923, 'healthy_reward': 2.4778628751269425, 'contact_cost_weight': 7.531781280580351e-05, 'healthy_z_lower': 0.296265307001552, 'healthy_z_upper': 1.261085342708649, 'contact_force_min': -1.0272762554399029, 'contact_force_max': 0.8249705432013178, 'learning_rate': 0.0002670057945236365, 'n_steps': 8192, 'batch_size': 512, 'gamma': 0.9504895534642267, 'gae_lambda': 0.9500238431072182, 'clip_range': 0.07027227038328278, 'ent_coef': 0.01622643306356706} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/5w/qb_kxxjs5lscg9_8tpttrzs40000gn/T/ipykernel_75499/3727025327.py\", line 75, in objective\n",
      "    model.learn(total_timesteps=200000)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\", line 311, in learn\n",
      "    return super().learn(\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 323, in learn\n",
      "    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 218, in collect_rollouts\n",
      "    new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\", line 207, in step\n",
      "    return self.step_wait()\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py\", line 181, in step_wait\n",
      "    obs, rewards, dones, infos = self.venv.step_wait()\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 129, in step_wait\n",
      "    results = [remote.recv() for remote in self.remotes]\n",
      "  File \"/Users/ignazioemanuelepicciche/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 129, in <listcomp>\n",
      "    results = [remote.recv() for remote in self.remotes]\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "[W 2025-02-18 22:57:18,907] Trial 79 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Crea uno studio Optuna e ottimizza l'obiettivo\u001b[39;00m\n\u001b[1;32m    112\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Stampa i migliori iperparametri trovati\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[4], line 75\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Nuovo iperparametro per la penalizzazione della varianza\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# std_penalty_weight = trial.suggest_float('std_penalty_weight', 0.0, 0.5)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Crea ed allena il modello PPO\u001b[39;00m\n\u001b[1;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env,\n\u001b[1;32m     66\u001b[0m             learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     67\u001b[0m             n_steps\u001b[38;5;241m=\u001b[39mn_steps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m             seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     74\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Valuta il modello su 200 episodi (200 è ottimale)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_normalize.py:181\u001b[0m, in \u001b[0;36mVecNormalize.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Apply sequence of actions to sequence of environments\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    actions -> (observations, rewards, dones)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    where ``dones`` is a boolean vector indicating whether each element is new.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, (np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m))  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_obs \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[0m, in \u001b[0;36mSubprocVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Ignazio PC/ucbm/deep_learning/Reinforcement_Learning_MuJoCu/.venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m--> 129\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes]\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning con Optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Parametri dell'environment\n",
    "    reset_noise_scale = trial.suggest_float('reset_noise_scale', 0, 0.3)           # Default circa 0.1; esploriamo da 0.05 a 0.2\n",
    "    forward_reward_weight = trial.suggest_float('forward_reward_weight', 1.6, 1.9)     # Default tipico è 1; esploriamo da 0.5 a 1.5\n",
    "    ctrl_cost_weight = trial.suggest_float('ctrl_cost_weight', 1.2, 1.6)               # Default tipico 0.5; esploriamo da 0.1 a 1.0\n",
    "    healthy_reward = trial.suggest_float('healthy_reward', 2.1, 2.5)                   # Default tipico 1; esploriamo da 0.5 a 1.5\n",
    "\n",
    "    # Parametri aggiuntivi per Ant-v5\n",
    "    contact_cost_weight = trial.suggest_float('contact_cost_weight', 1e-6, 1e-4)  # Es. range intorno a 5e-4 come default\n",
    "    healthy_z_lower = trial.suggest_float('healthy_z_lower', 0.1, 0.4)             # Per definire l'intervallo di altezze \"sane\"\n",
    "    healthy_z_upper = trial.suggest_float('healthy_z_upper', 1.1, 1.4)\n",
    "    contact_force_min = trial.suggest_float('contact_force_min', -1.3, -1)         # Modificabile se usi forze di contatto\n",
    "    contact_force_max = trial.suggest_float('contact_force_max', 0.8, 1.1)\n",
    "\n",
    "    # Crea l'ambiente passando tutti i parametri\n",
    "    # env = make_env(\n",
    "    #     reset_noise_scale,\n",
    "    #     forward_reward_weight,\n",
    "    #     ctrl_cost_weight,\n",
    "    #     healthy_reward,\n",
    "    #     contact_cost_weight=contact_cost_weight,\n",
    "    #     healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "    #     contact_force_range=(contact_force_min, contact_force_max)\n",
    "    # )\n",
    "    #env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    # MULTIPROCESSING (MULTIENVIRONMENTS) \n",
    "    NUM_ENVS=4\n",
    "    env = SubprocVecEnv([\n",
    "        lambda: make_env(\n",
    "            reset_noise_scale,\n",
    "            forward_reward_weight,\n",
    "            ctrl_cost_weight,\n",
    "            healthy_reward,\n",
    "            contact_cost_weight=contact_cost_weight,\n",
    "            healthy_z_range=(healthy_z_lower, healthy_z_upper),\n",
    "            contact_force_range=(contact_force_min, contact_force_max)\n",
    "        ) for _ in range(NUM_ENVS)\n",
    "    ])\n",
    "    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "    \n",
    "\n",
    "    env.training = False # Setta l'environment in modalità di valutazione\n",
    "    env.norm_reward = False # Disabilita la normalizzazione della reward. Questo è importante per valutare correttamente il modello.\n",
    "    \n",
    "\n",
    "    # Iperparametri per il modello PPO\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    n_steps = trial.suggest_int('n_steps', 4096, 12288, step=2048)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [256, 512, 1024, 2048])  \n",
    "    # Per ambienti complessi come Ant, molti esperimenti usano gamma intorno a 0.99-0.995\n",
    "    gamma = trial.suggest_float('gamma', 0.93, 0.96)\n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.95, 0.98)\n",
    "    clip_range = trial.suggest_float('clip_range', 0, 0.2) \n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.0, 0.1)\n",
    "    \n",
    "    # Nuovo iperparametro per la penalizzazione della varianza\n",
    "    # std_penalty_weight = trial.suggest_float('std_penalty_weight', 0.0, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "    # Crea ed allena il modello PPO\n",
    "    model = PPO(\"MlpPolicy\", env,\n",
    "                learning_rate=learning_rate,\n",
    "                n_steps=n_steps,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=gae_lambda,\n",
    "                clip_range=clip_range,\n",
    "                ent_coef=ent_coef,\n",
    "                seed=42,\n",
    "                verbose=0)\n",
    "    model.learn(total_timesteps=200000)\n",
    "\n",
    "    # Valuta il modello su 200 episodi (200 è ottimale)\n",
    "    episodes = 300\n",
    "\n",
    "    # episode_rewards = []\n",
    "    # for episode in range(episodes):\n",
    "    #     obs = env.reset()\n",
    "    #     done = False\n",
    "    #     episode_reward = 0\n",
    "    #     while not done:\n",
    "    #         action, _states = model.predict(obs)\n",
    "    #         obs, reward, done, info = env.step(action)\n",
    "    #         episode_reward += reward\n",
    "    #     episode_rewards.append(episode_reward)\n",
    "\n",
    "    # # Calcola reward media e varianza\n",
    "    # mean_reward = np.mean(episode_rewards)\n",
    "    # reward_std = np.std(episode_rewards)\n",
    "\n",
    "    # # Definisce l'obiettivo: massimizzare la reward media penalizzando la varianza\n",
    "    # score = mean_reward - std_penalty_weight * reward_std\n",
    "\n",
    "    # print(f'Mean is: {mean_reward}, Std is: {reward_std}\\n')\n",
    "\n",
    "\n",
    "\n",
    "    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=episodes)\n",
    "\n",
    "    # Chiudi e rilascia le risorse\n",
    "    env.close()\n",
    "    del model, env\n",
    "    gc.collect()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "# Crea uno studio Optuna e ottimizza l'obiettivo\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=300)\n",
    "\n",
    "# Stampa i migliori iperparametri trovati\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
